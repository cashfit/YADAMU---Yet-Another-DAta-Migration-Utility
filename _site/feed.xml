<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-06-22T08:44:02-07:00</updated><id>http://localhost:4000/</id><title type="html">JSON Exchange</title><subtitle>A simple utility for exporting and importing data using JSON</subtitle><entry><title type="html"></title><link href="http://localhost:4000/2018/06/22/2018-06-22-Cloning-Sample-Schemas.html" rel="alternate" type="text/html" title="" /><published>2018-06-22T08:44:02-07:00</published><updated>2018-06-22T08:44:02-07:00</updated><id>http://localhost:4000/2018/06/22/2018-06-22-Cloning%20Sample%20Schemas</id><content type="html" xml:base="http://localhost:4000/2018/06/22/2018-06-22-Cloning-Sample-Schemas.html">&lt;h2 id=&quot;cloning-oracles-sample-schemas&quot;&gt;Cloning Oracle’s Sample Schemas&lt;/h2&gt;

&lt;p&gt;he previous post covered using DBMS_METADATA to include DDL operations the files created by EXPORT_SCHEMA and how t these DDL operations can be applied to the target schema before importing the data.This post will cover the testing performed on EXPORT_SCHEMA and IMPORT_JSON. The  success criteria established for the project is to successfully export and import the six Oracle supplied sample schemas. Testing will be performed by using the EXPORT_SCHEMA script to generate an export file for each of schemas and then using IMPORT_SCHEMA script to import each of the files into a newly created schema.  The process should complete without any FATAL errors being recorded in the log records generated by the import process.&lt;/p&gt;

&lt;p&gt;The results of running the tests with each of the sample schemas is shown in the following table:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Schema&lt;/th&gt;
      &lt;th&gt;EXPORT_SCHEMA&lt;/th&gt;
      &lt;th&gt;IMPORT_JSON (DDL)&lt;/th&gt;
      &lt;th&gt;IMPORT_JSON (DML)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;HR&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️ (Duplicates)&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SH&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️ (Duplicates)&lt;/td&gt;
      &lt;td&gt;❌ (Materialized Views)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;OE&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️ (Duplicates,References)&lt;/td&gt;
      &lt;td&gt;❌(Objects, Mutating Table)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PM&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️ (Duplicates,References)&lt;/td&gt;
      &lt;td&gt;❌(Objects)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;IX&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️ (Duplicates,References,AQ)&lt;/td&gt;
      &lt;td&gt;❌(Objects)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BI&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️(No data to import)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;At this point the EXPORT_SCHEMA and IMPORT_JSCHEMA scripts can be used to clone the HR and BI schemas. However further modifications are required to handle the issues that arise when attempting to clone the SH, OE, PM and IX schemas&lt;/p&gt;

&lt;p&gt;Cloning the SH Schema fails when it attempts an “INSERT as SELECT” operation on a materialized view. The cause of the problem is EXPORT_SCHEMA included the content of the materialized view in the export file. Materialized views are included because they appear in the view ALL_ALL_TABLES, which is referenced in the SQL used by GENERATE_STATEMENETS to determine which objects to export. The solution to this problem is to simply exclude materialized views from the export file.&lt;/p&gt;

&lt;p&gt;Materialized views are excluded from the export file by adding a left outer join with ALL_MVIEWS to  the SQL used by GENERATE_STATEMENTS and applying a filter that removes rows where MVIEW_NAME is not null. The code for this is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;    from ALL_ALL_TABLES aat
         inner join ALL_TAB_COLS atc
                 on atc.OWNER = aat.OWNER
                and atc.TABLE_NAME = aat.TABLE_NAME
    left outer join ALL_TYPES at
                 on at.TYPE_NAME = atc.DATA_TYPE
                and at.OWNER = atc.DATA_TYPE_OWNER
    left outer join ALL_MVIEWS amv
		         on amv.OWNER = aat.OWNER
		        and amv.MVIEW_NAME = aat.TABLE_NAME
   where aat.STATUS = 'VALID'
     and aat.DROPPED = 'NO'
     and aat.TEMPORARY = 'N'
     and aat.EXTERNAL = 'NO'
     and aat.NESTED = 'NO'
     and aat.SECONDARY = 'N'
     and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT')
     and (
           ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = 'NO'))
         or 
           (
               (TABLE_TYPE is not NULL) 
               and 
               (COLUMN_NAME in ('SYS_NC_ROWINFO$','SYS_NC_OID$','ACLOID','OWNERID'))
           )
         )        
	 and amv.MVIEW_NAME is NULL
     and aat.OWNER = P_SCHEMA
   group by aat.OWNER, aat.TABLE_NAME;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cloning the OE schema fails when importing the ORDER_ITEMS table due to a  “mutating table” error. The error occurs because there is a trigger on the table that prevents bulk operations, such as INSERT as SELECT from executing. This issue is solved by modifying IMPORT_JSON to catch the mutating table exception, and re-attempt the import operation using a PL/SQL cursor that inserts the rows one at a time.&lt;/p&gt;

&lt;p&gt;Cloning OE, PM and IX schemas also fails due to issues related to Oracle object types. EXPORT_SCHEMA required custom PL/SQL code to serialize objects, and custom PL/SQL is required to parse the serialized representation. Oracle does not expose a generic parser for Oracle objects, fortunately there is a relatively simple solution, all that is required to parse the object is to wrap the serialized representation of the object in a “select … from dual” operation and execute the code using EXECUTE IMMEDIATE. This causes Oracle to parse the object and return an instance of the object type. A separate function is required for each type of object, as the function definition must specify the type of object returned. The required set of functions needs to be added to a WITH clause that is added to the SQL used to ingest the JSON.&lt;/p&gt;

&lt;p&gt;The test folder contains two scripts that can be used to validate whether or not the export / import process successfully cloned the source schema. VALIDATE_STRUCTURE compares the entries in view ALL_OBJECTS for the source and target schema, and generates a report showing any inconsistencies  that occur after the IMPORT _JSON operation is complete. VALIDATE_CONTENT uses the SQL minus operator to compare the content of each table in the source and target schema, and generates a summary report showing the number of mismatched rows.&lt;/p&gt;</content><author><name></name></author></entry><entry><title type="html">Schema DDL operations</title><link href="http://localhost:4000/json/import/export/oracle/2018/06/21/DDL-Operations.html" rel="alternate" type="text/html" title="Schema DDL operations" /><published>2018-06-21T18:00:00-07:00</published><updated>2018-06-21T18:00:00-07:00</updated><id>http://localhost:4000/json/import/export/oracle/2018/06/21/DDL%20Operations</id><content type="html" xml:base="http://localhost:4000/json/import/export/oracle/2018/06/21/DDL-Operations.html">&lt;h2 id=&quot;adding-ddl-operations-in-the-export-file&quot;&gt;Adding DDL operations in the export file.&lt;/h2&gt;

&lt;p&gt;The previous post described a package for importing data from the files generated by JSON_EXPORT. It also described a method for testing this package which relied on using Oracle’s classing export (exp) and import(imp) utilities to  create an empty clone of a database schema. The method works for the HR schema, Unfortunately when it applied to the  other sample schemas, a number of errors are reported during the import operation, including illegal identifiers and unsupported data types. Clearly another solution to duplicating a schema’s structure is required.&lt;/p&gt;

&lt;p&gt;This post introduces the PL/SQL package JSON_EXPORT_DDL. This package provides two functions. The first, FETCH_DDL_STATEMENTS, returns the set of DDL statements used to create all of the objects in the schema. The second, APPLY_DDL_STATEMENTS replays the DDL captured by FETCH_DLL_STATEMENTS in the target schema, effective turning the target schema into a structural clone of the source schema.&lt;/p&gt;

&lt;p&gt;The FETCH_DDL_STATEMENTS function uses the Oracle supplied package DBMS_METADATA  to obtain the set of DLL operations used to create the source schema. The order in which DDL Operations are exported is as follows&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;XML Schemas, generated using DBMS_METADATA. XML Schemas must be registered in the target schema before attempting to perform DDL operations.&lt;/li&gt;
  &lt;li&gt;DLL Operations, generated using DBMS_METADATA. DBMS_METADATA is configured to omit storage clauses making it much easier to replay the DLL operations in the target environment.&lt;/li&gt;
  &lt;li&gt;DMS_XDBZ.enableHierarchy() operations. This re-establishes any relationship between an XMLType table and the XML DB repository.&lt;/li&gt;
  &lt;li&gt;Index naming operations. This ensure that system generated index names from the source schema are preserved in the target schema.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The code used to access the DDL operations is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;  begin
    V_HDL_OPEN := DBMS_METADATA.OPEN('SCHEMA_EXPORT');
    DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'SCHEMA',P_SCHEMA);

    V_HDL_TRANSFORM := DBMS_METADATA.ADD_TRANSFORM(V_HDL_OPEN,'DDL');

    -- Suppress Segement information for TABLES, INDEXES and CONSTRAINTS

    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'TABLE');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'INDEX');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'CONSTRAINT');

    -- Return constraints as 'ALTER TABLE' operations

    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'CONSTRAINTS_AS_ALTER',true,'TABLE');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'REF_CONSTRAINTS',false,'TABLE');

    -- Exclude XML Schema Info. XML Schemas need to come first and 
    -- are handled in a seperate section

    DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'EXCLUDE_PATH_EXPR','=''XMLSCHEMA''');

    loop
      -- Get the next batch of DDL_STATEMENTS. Each batch may contain zero or more statements.
      V_DDL_STATEMENTS := DBMS_METADATA.FETCH_DDL(V_HDL_OPEN);
	  EXIT WHEN V_DDL_STATEMENTS IS NULL;

      for i in 1 .. V_DDL_STATEMENTS.count loop

  	    V_DDL_STATEMENT := V_DDL_STATEMENTS(i).DDLTEXT;

  	    -- Strip leading and trailing white space from DDL statement
	    V_DDL_STATEMENT := TRIM(BOTH C_NEWLINE FROM V_DDL_STATEMENT);
        V_DDL_STATEMENT := TRIM(BOTH C_CARRIAGE_RETURN FROM V_DDL_STATEMENT);
        V_DDL_STATEMENT := TRIM(V_DDL_STATEMENT);

        PIPE ROW (TRIM(V_DDL_STATEMENT));
      end loop;
    end loop;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The EXPORT_SCHEMA function inserts a “ddl” object into the export file immediately after the “systemInformation” object. The “ddl” object is created based on the DDL statements returned by FETCH_DLL_STATEMENTS. The “ddl” object is an array, each item in the array contains a single ddl operation.&lt;/p&gt;

&lt;p&gt;The IMPORT_JSON function passes the export file to APPLY_DDL_STATEMENTS before attempting to import data. This function uses JSON_TABLE to extract each of the ddl statements from the dump and executes them in order.Before replaying a DDL operation all references to the source schema are replaced with references to the target schema. This enables cloning from schema A to schema B.&lt;/p&gt;

&lt;p&gt;Exceptions may be raised while replaying the DDL generated by DBMS_METADATA.  In most cases these can be safely ignored. Duplicate name/key/index/constraint exceptions occur because the DDL generated by DBMS_METADATA results in some objects being created more than once.  Exceptions also  arise when re-creating constraints if the required permissions have not been granted to the target schema . Exceptions are raised if the source schema uses Oracle Advanced Queuing (AQ). When AQ is present in the source DBMS_METADATA generates operations that cannot be  replayed in the target schemas, even by a  DBA.  Any exceptions that arise while replaying the DDL statements are caught, classified as IGNOREABLE, DUPLICATE, REFERENCE  WARNING, AQ_RELATED, or FATAL and logged.&lt;/p&gt;

&lt;p&gt;The log of DDL operations can be queried by applying a table operator to the IMPORT_DDL_LOG function.&lt;/p&gt;

&lt;p&gt;EXPORT_SCHEMA and IMPORT_JSON are now theoretically capable of cloning any database schema. The next post will examine what occurs when attempting to clone each of the six Oracle supplied sample schemas.&lt;/p&gt;</content><author><name></name></author><summary type="html">Adding DDL operations in the export file. The previous post described a package for importing data from the files generated by JSON_EXPORT. It also described a method for testing this package which relied on using Oracle’s classing export (exp) and import(imp) utilities to create an empty clone of a database schema. The method works for the HR schema, Unfortunately when it applied to the other sample schemas, a number of errors are reported during the import operation, including illegal identifiers and unsupported data types. Clearly another solution to duplicating a schema’s structure is required. This post introduces the PL/SQL package JSON_EXPORT_DDL. This package provides two functions. The first, FETCH_DDL_STATEMENTS, returns the set of DDL statements used to create all of the objects in the schema. The second, APPLY_DDL_STATEMENTS replays the DDL captured by FETCH_DLL_STATEMENTS in the target schema, effective turning the target schema into a structural clone of the source schema. The FETCH_DDL_STATEMENTS function uses the Oracle supplied package DBMS_METADATA to obtain the set of DLL operations used to create the source schema. The order in which DDL Operations are exported is as follows XML Schemas, generated using DBMS_METADATA. XML Schemas must be registered in the target schema before attempting to perform DDL operations. DLL Operations, generated using DBMS_METADATA. DBMS_METADATA is configured to omit storage clauses making it much easier to replay the DLL operations in the target environment. DMS_XDBZ.enableHierarchy() operations. This re-establishes any relationship between an XMLType table and the XML DB repository. Index naming operations. This ensure that system generated index names from the source schema are preserved in the target schema. The code used to access the DDL operations is shown below begin V_HDL_OPEN := DBMS_METADATA.OPEN('SCHEMA_EXPORT'); DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'SCHEMA',P_SCHEMA); V_HDL_TRANSFORM := DBMS_METADATA.ADD_TRANSFORM(V_HDL_OPEN,'DDL'); -- Suppress Segement information for TABLES, INDEXES and CONSTRAINTS DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'TABLE'); DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'INDEX'); DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'CONSTRAINT'); -- Return constraints as 'ALTER TABLE' operations DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'CONSTRAINTS_AS_ALTER',true,'TABLE'); DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'REF_CONSTRAINTS',false,'TABLE'); -- Exclude XML Schema Info. XML Schemas need to come first and -- are handled in a seperate section DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'EXCLUDE_PATH_EXPR','=''XMLSCHEMA'''); loop -- Get the next batch of DDL_STATEMENTS. Each batch may contain zero or more statements. V_DDL_STATEMENTS := DBMS_METADATA.FETCH_DDL(V_HDL_OPEN); EXIT WHEN V_DDL_STATEMENTS IS NULL; for i in 1 .. V_DDL_STATEMENTS.count loop V_DDL_STATEMENT := V_DDL_STATEMENTS(i).DDLTEXT; -- Strip leading and trailing white space from DDL statement V_DDL_STATEMENT := TRIM(BOTH C_NEWLINE FROM V_DDL_STATEMENT); V_DDL_STATEMENT := TRIM(BOTH C_CARRIAGE_RETURN FROM V_DDL_STATEMENT); V_DDL_STATEMENT := TRIM(V_DDL_STATEMENT); PIPE ROW (TRIM(V_DDL_STATEMENT)); end loop; end loop; The EXPORT_SCHEMA function inserts a “ddl” object into the export file immediately after the “systemInformation” object. The “ddl” object is created based on the DDL statements returned by FETCH_DLL_STATEMENTS. The “ddl” object is an array, each item in the array contains a single ddl operation. The IMPORT_JSON function passes the export file to APPLY_DDL_STATEMENTS before attempting to import data. This function uses JSON_TABLE to extract each of the ddl statements from the dump and executes them in order.Before replaying a DDL operation all references to the source schema are replaced with references to the target schema. This enables cloning from schema A to schema B. Exceptions may be raised while replaying the DDL generated by DBMS_METADATA. In most cases these can be safely ignored. Duplicate name/key/index/constraint exceptions occur because the DDL generated by DBMS_METADATA results in some objects being created more than once. Exceptions also arise when re-creating constraints if the required permissions have not been granted to the target schema . Exceptions are raised if the source schema uses Oracle Advanced Queuing (AQ). When AQ is present in the source DBMS_METADATA generates operations that cannot be replayed in the target schemas, even by a DBA. Any exceptions that arise while replaying the DDL statements are caught, classified as IGNOREABLE, DUPLICATE, REFERENCE WARNING, AQ_RELATED, or FATAL and logged. The log of DDL operations can be queried by applying a table operator to the IMPORT_DDL_LOG function. EXPORT_SCHEMA and IMPORT_JSON are now theoretically capable of cloning any database schema. The next post will examine what occurs when attempting to clone each of the six Oracle supplied sample schemas.</summary></entry><entry><title type="html">Introduction to JSON_IMPORT</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html" rel="alternate" type="text/html" title="Introduction to JSON_IMPORT" /><published>2018-06-20T18:00:00-07:00</published><updated>2018-06-20T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html">&lt;h2 id=&quot;json_import-the-yin-to-json_exports-yang&quot;&gt;JSON_IMPORT: The Yin to JSON_EXPORT’s Yang&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html&quot;&gt;previous&lt;/a&gt; posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format.&lt;/p&gt;

&lt;p&gt;The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{&quot;data&quot; : {
    &quot;REGIONS&quot; : [
        [1,&quot;Europe&quot;],
        [2,&quot;Americas&quot;],
        [3,&quot;Asia&quot;],
        [4,&quot;Middle East and Africa&quot;]
    ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use JSON_TABLE to convert it back in a relational format&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select ji.*
  from MY_EXPORT_FILE,
       JSON_TABLE(
          JSON_EXPORT_DATA,
          '$.data.&quot;REGIONS&quot;[*]'
          columns (
            REGION_ID     NUMBER PATH '$[0]'
           ,REGION_NAME VARCHAR2 PATH '$[1]'
		  )
		) JI
		
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt; REGION_ID REGION_NAME
---------- --------------------------------
         1 Europe
         2 Americas
         3 Asia
         4 Middle East and Africa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To perform this transformation it  is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the  JSON_IMPORT utility.&lt;/p&gt;

&lt;p&gt;In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Key&lt;/th&gt;
      &lt;th&gt;Content&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;date&lt;/td&gt;
      &lt;td&gt;Date the file was generated, ISO 8601 format, Zulu time&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;schema&lt;/td&gt;
      &lt;td&gt;The database schema that was the source for the export&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;exportVersion&lt;/td&gt;
      &lt;td&gt;The version of JSON_EXPORT used. Currently V1.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;jsonFeatures&lt;/td&gt;
      &lt;td&gt;The current settings in package JSON_FEATURE_DETECTION&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;sessionUser&lt;/td&gt;
      &lt;td&gt;The session user who performed the EXPORT_SCHEMA operation&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dbName&lt;/td&gt;
      &lt;td&gt;The name of the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;serverHostName&lt;/td&gt;
      &lt;td&gt;The logical name of the server hosting the database&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;databaseVersion&lt;/td&gt;
      &lt;td&gt;The release of the database the data came from&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;nlsInformation&lt;/td&gt;
      &lt;td&gt;An object containing the NLS parameters of the source database.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In a 12.2 database the SQL used to generate the systemInformation object is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;with 
function DATABASE_RELEASE return NUMBER deterministic
as
begin
  return DBMS_DB_VERSION.VERSION || '.' || DBMS_DB_VERSION.RELEASE;
end;
--
function JSON_FEATURES return VARCHAR2 deterministic
as
begin
  return JSON_OBJECT(
           'treatAsJSON'     value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED
       	  ,'CLOB'            value JSON_FEATURE_DETECTION.CLOB_SUPPORTED
          ,'extendedString'  value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED
		);
end;
function EXPORT_VERSION return NUMBER
as
begin
  return 1.0;
end;
select JSON_OBJECT(
           'systemInformation'
		   value JSON_OBJECT(
	              'date'            value SYS_EXTRACT_UTC(SYSTIMESTAMP)
	             ,'schema'          value 'HR'
		         ,'exportVersion'   value EXPORT_VERSION()
		         ,'jsonFeatures'    value JSON_QUERY(JSON_FEATURES(),'$')
	             ,'sessionUser'     value SYS_CONTEXT('USERENV','SESSION_USER')
		         ,'dbName'          value SYS_CONTEXT('USERENV','DB_NAME')
		         ,'serverHostName'  value SYS_CONTEXT('USERENV','SERVER_HOST')
		         ,'databaseVersion' value DATABASE_RELEASE()
		         ,'nlsInformation'  value JSON_OBJECTAGG(parameter, value)
	             )
		 ) SYSTEM_INFORMATION
    from NLS_DATABASE_PARAMETERS;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;An abridged version of systemInformation object is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{
	&quot;systemInformation&quot;: {
		&quot;date&quot;: &quot;2018-06-21T04:42:44.523029&quot;,
		&quot;schema&quot;: &quot;HR&quot;,
		&quot;exportVersion&quot;: 1,
		&quot;jsonFeatures&quot;: {
			&quot;treatAsJSON&quot;: false,
			&quot;CLOB&quot;: false,
			&quot;extendedString&quot;: true
		},
		&quot;sessionUser&quot;: &quot;SYSTEM&quot;,
		&quot;dbName&quot;: &quot;ORCL&quot;,
		&quot;serverHostName&quot;: &quot;localhost&quot;,
		&quot;databaseVersion&quot;: 12.2,
		&quot;nlsInformation&quot;: {
			&quot;NLS_RDBMS_VERSION&quot;: &quot;12.2.0.1.0&quot;,
			...
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation.&lt;/p&gt;

&lt;p&gt;A metadata object is added to the export file. This object provides the following information for each table included in the export file.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Key&lt;/th&gt;
      &lt;th&gt;Contents&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;owner&lt;/td&gt;
      &lt;td&gt;The database schema containing the table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tableName&lt;/td&gt;
      &lt;td&gt;The name of the table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;columnList&lt;/td&gt;
      &lt;td&gt;The columns in the table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dataTypeList&lt;/td&gt;
      &lt;td&gt;The data types of the columns in the table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;exportSelectList&lt;/td&gt;
      &lt;td&gt;The select list used to export the data in the table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;columnPatternList&lt;/td&gt;
      &lt;td&gt;The column patterns needed to convert the JSON back into relational form&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object  is added to the GENERATE_STATEMENT module.&lt;/p&gt;

&lt;p&gt;The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;&quot;metadata&quot;: {
  &quot;REGIONS&quot;: {
     &quot;owner&quot;: &quot;HR&quot;,
     &quot;tableName&quot;: &quot;REGIONS&quot;,
	 &quot;columns&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;,
	 &quot;dataTypes&quot;: &quot;\&quot;NUMBER\&quot;,\&quot;VARCHAR2\&quot;&quot;,
	 &quot;exportSelectList&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;,
	 &quot;columnPatterns&quot;: &quot;\&quot;REGION_ID\&quot; NUMBER PATH '$[0]',\&quot;REGION_NAME\&quot; VARCHAR2 PATH '$[1]'&quot;
  },
  &quot;JOBS&quot; : {...
  },
  ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases.&lt;/p&gt;

&lt;p&gt;In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB)
return T_SQL_OPERATIONS_TAB
as
  V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB;
begin
  select OWNER
        ,TABLE_NAME
        ,'insert into &quot;' || TABLE_NAME ||'&quot;(' || EXPORT_SELECT_LIST || ')' || C_NEWLINE ||
         'select ' || SELECT_LIST || C_NEWLINE ||
         '  from JSON_TABLE(' || C_NEWLINE ||
         '         :JSON,' || C_NEWLINE ||
         '         ''$.data.&quot;' || TABLE_NAME || '&quot;[*]''' || C_NEWLINE ||
         '         COLUMNS(' || C_NEWLINE ||  COLUMN_PATTERNS || C_NEWLINE || '))' 
        ,NULL
        ,NULL
        ,NULL
   	bulk collect into V_SQL_OPERATIONS
   	from JSON_TABLE(
            P_JSON_DUMP_FILE,
		   '$.metadata.*' ERROR ON ERROR
            COLUMNS (
               OWNER       VARCHAR2(128) PATH '$.owner'
  			, TABLE_NAME  VARCHAR2(128) PATH '$.tableName'
               $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN   
   			, EXPORT_SELECT_LIST              CLOB PATH '$.columns'
             , DATA_TYPES                      CLOB PATH '$.dataTypes'
             , COLUMN_PATTERNS                 CLOB PATH '$.columnPatterns'
		    , SELECT_LIST                     CLOB PATH '$.importSelectList'
             $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
			, EXPORT_SELECT_LIST  VARCHAR2(32767) PATH '$.columns'
			, DATA_TYPES          VARCHAR2(32767) PATH '$.dataTypes'
			, COLUMN_PATTERNS     VARCHAR2(32767) PATH '$.columnPatterns'
			, SELECT_LIST         VARCHAR2(32767) PATH '$.importSelectList'
			$ELSE
			, EXPORT_SELECT_LIST   VARCHAR2(4000) PATH '$.columns'
			, DATA_TYPES           VARCHAR2(4000) PATH '$.dataTypes'
			, COLUMN_PATTERNS      VARCHAR2(4000) PATH '$.columnPatterns'
			, SELECT_LIST          VARCHAR2(4000) PATH '$.importSelectList'
			$END
		  )
		);
  return V_SQL_OPERATIONS;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB,
                      P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT('USERENV','CURRENT_SCHEMA'))
as
  V_CURRENT_SCHEMA           CONSTANT VARCHAR2(128) := SYS_CONTEXT('USERENV','CURRENT_SCHEMA');
  V_START_TIME TIMESTAMP(6);
  V_END_TIME   TIMESTAMP(6);
begin
  SET_CURRENT_SCHEMA(P_TARGET_SCHEMA);
  
  SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE);
							
  for i in 1 .. SQL_OPERATIONS_TABLE.count loop
    begin
      V_START_TIME := SYSTIMESTAMP;
      execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE;  
 	  SQL_OPERATIONS_TABLE(i).RESULT := 'Operation completed succecssfully at ' 
 	                                 || SYS_EXTRACT_UTC(SYSTIMESTAMP) 
 	                                 || '. Processed ' || TO_CHAR(SQL%ROWCOUNT) 
 	                                 || ' rows. Elapsed time: ' 
 	                                 || (V_END_TIME - V_START_TIME) || '.';
	  commit;
	  SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS;
    exception
      when others then
	    SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack;
		SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR;
	end;
  end loop;
  SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
exception
  when OTHERS then
    SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
	RAISE;
end;
--
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SH&quot;&gt;exp userid=system@ORCL owner=HR file=HR.dmp rows=N
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A new database schema was created using the following command in SQL*PLUS&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;grant connect, resource, unlimited tablespace to HR2 identified by ********
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SH&quot;&gt;imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set echo on
spool logs/IMPORT_FROM_FILE.log
--
def JSON_DIR = &amp;amp;1
def FILENAME = &amp;amp;2
def SCHEMA = &amp;amp;3
--
VAR JSON CLOB
--
create or replace directory JSON_DIR as '&amp;amp;JSON_DIR'
/
DECLARE
  V_DEST_OFFSET NUMBER := 1;
  V_SRC_OFFSET  NUMBER := 1;
  V_CONTEXT     NUMBER := 0;
  V_WARNINGS    NUMBER := 0;
  V_BFILE	     BFILE := BFILENAME('JSON_DIR','&amp;amp;FILENAME');
begin
  DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION);
  DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY);
  DBMS_LOB.LOADCLOBFROMFILE (
             :JSON,V_BFILE
            ,DBMS_LOB.LOBMAXSIZE
            ,V_DEST_OFFSET
            ,V_SRC_OFFSET
            ,NLS_CHARSET_ID('AL32UTF8')
            ,V_CONTEXT
            ,V_WARNINGS);
  DBMS_LOB.FILECLOSE(V_BFILE);
end;
/
select 1
	from DUAL
  where :JSON IS JSON
/
begin
  JSON_IMPORT.IMPORT_JSON(:JSON,'&amp;amp;SCHEMA');
end;
/
set pages 50 lines 256 trimspool on long 1000000
--
column TABLE_NAME format A30
column SQL_STATEMENT format A80
column STATUS format A12
column RESULT format A32
-- 
select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT 
  from table(JSON_IMPORT.SQL_OPERATIONS)
/
exit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function.&lt;/p&gt;

&lt;p&gt;The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to  the transient inconsistencies that are inherent in a table by table data load process.&lt;/p&gt;

&lt;p&gt;Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported.&lt;/p&gt;

&lt;p&gt;The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully.&lt;/p&gt;</content><author><name></name></author><summary type="html">JSON_IMPORT: The Yin to JSON_EXPORT’s Yang The previous posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format. The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this {&quot;data&quot; : { &quot;REGIONS&quot; : [ [1,&quot;Europe&quot;], [2,&quot;Americas&quot;], [3,&quot;Asia&quot;], [4,&quot;Middle East and Africa&quot;] ] } We can use JSON_TABLE to convert it back in a relational format select ji.* from MY_EXPORT_FILE, JSON_TABLE( JSON_EXPORT_DATA, '$.data.&quot;REGIONS&quot;[*]' columns ( REGION_ID NUMBER PATH '$[0]' ,REGION_NAME VARCHAR2 PATH '$[1]' ) ) JI REGION_ID REGION_NAME ---------- -------------------------------- 1 Europe 2 Americas 3 Asia 4 Middle East and Africa To perform this transformation it is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the JSON_IMPORT utility. In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information Key Content date Date the file was generated, ISO 8601 format, Zulu time schema The database schema that was the source for the export exportVersion The version of JSON_EXPORT used. Currently V1.0 jsonFeatures The current settings in package JSON_FEATURE_DETECTION sessionUser The session user who performed the EXPORT_SCHEMA operation dbName The name of the database serverHostName The logical name of the server hosting the database databaseVersion The release of the database the data came from nlsInformation An object containing the NLS parameters of the source database. In a 12.2 database the SQL used to generate the systemInformation object is shown below with function DATABASE_RELEASE return NUMBER deterministic as begin return DBMS_DB_VERSION.VERSION || '.' || DBMS_DB_VERSION.RELEASE; end; -- function JSON_FEATURES return VARCHAR2 deterministic as begin return JSON_OBJECT( 'treatAsJSON' value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED ,'CLOB' value JSON_FEATURE_DETECTION.CLOB_SUPPORTED ,'extendedString' value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED ); end; function EXPORT_VERSION return NUMBER as begin return 1.0; end; select JSON_OBJECT( 'systemInformation' value JSON_OBJECT( 'date' value SYS_EXTRACT_UTC(SYSTIMESTAMP) ,'schema' value 'HR' ,'exportVersion' value EXPORT_VERSION() ,'jsonFeatures' value JSON_QUERY(JSON_FEATURES(),'$') ,'sessionUser' value SYS_CONTEXT('USERENV','SESSION_USER') ,'dbName' value SYS_CONTEXT('USERENV','DB_NAME') ,'serverHostName' value SYS_CONTEXT('USERENV','SERVER_HOST') ,'databaseVersion' value DATABASE_RELEASE() ,'nlsInformation' value JSON_OBJECTAGG(parameter, value) ) ) SYSTEM_INFORMATION from NLS_DATABASE_PARAMETERS; An abridged version of systemInformation object is shown below { &quot;systemInformation&quot;: { &quot;date&quot;: &quot;2018-06-21T04:42:44.523029&quot;, &quot;schema&quot;: &quot;HR&quot;, &quot;exportVersion&quot;: 1, &quot;jsonFeatures&quot;: { &quot;treatAsJSON&quot;: false, &quot;CLOB&quot;: false, &quot;extendedString&quot;: true }, &quot;sessionUser&quot;: &quot;SYSTEM&quot;, &quot;dbName&quot;: &quot;ORCL&quot;, &quot;serverHostName&quot;: &quot;localhost&quot;, &quot;databaseVersion&quot;: 12.2, &quot;nlsInformation&quot;: { &quot;NLS_RDBMS_VERSION&quot;: &quot;12.2.0.1.0&quot;, ... } } } In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. A metadata object is added to the export file. This object provides the following information for each table included in the export file. Key Contents owner The database schema containing the table tableName The name of the table columnList The columns in the table dataTypeList The data types of the columns in the table exportSelectList The select list used to export the data in the table columnPatternList The column patterns needed to convert the JSON back into relational form Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object is added to the GENERATE_STATEMENT module. The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below &quot;metadata&quot;: { &quot;REGIONS&quot;: { &quot;owner&quot;: &quot;HR&quot;, &quot;tableName&quot;: &quot;REGIONS&quot;, &quot;columns&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;dataTypes&quot;: &quot;\&quot;NUMBER\&quot;,\&quot;VARCHAR2\&quot;&quot;, &quot;exportSelectList&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;columnPatterns&quot;: &quot;\&quot;REGION_ID\&quot; NUMBER PATH '$[0]',\&quot;REGION_NAME\&quot; VARCHAR2 PATH '$[1]'&quot; }, &quot;JOBS&quot; : {... }, ... } The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB) return T_SQL_OPERATIONS_TAB as V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB; begin select OWNER ,TABLE_NAME ,'insert into &quot;' || TABLE_NAME ||'&quot;(' || EXPORT_SELECT_LIST || ')' || C_NEWLINE || 'select ' || SELECT_LIST || C_NEWLINE || ' from JSON_TABLE(' || C_NEWLINE || ' :JSON,' || C_NEWLINE || ' ''$.data.&quot;' || TABLE_NAME || '&quot;[*]''' || C_NEWLINE || ' COLUMNS(' || C_NEWLINE || COLUMN_PATTERNS || C_NEWLINE || '))' ,NULL ,NULL ,NULL bulk collect into V_SQL_OPERATIONS from JSON_TABLE( P_JSON_DUMP_FILE, '$.metadata.*' ERROR ON ERROR COLUMNS ( OWNER VARCHAR2(128) PATH '$.owner' , TABLE_NAME VARCHAR2(128) PATH '$.tableName' $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN , EXPORT_SELECT_LIST CLOB PATH '$.columns' , DATA_TYPES CLOB PATH '$.dataTypes' , COLUMN_PATTERNS CLOB PATH '$.columnPatterns' , SELECT_LIST CLOB PATH '$.importSelectList' $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN , EXPORT_SELECT_LIST VARCHAR2(32767) PATH '$.columns' , DATA_TYPES VARCHAR2(32767) PATH '$.dataTypes' , COLUMN_PATTERNS VARCHAR2(32767) PATH '$.columnPatterns' , SELECT_LIST VARCHAR2(32767) PATH '$.importSelectList' $ELSE , EXPORT_SELECT_LIST VARCHAR2(4000) PATH '$.columns' , DATA_TYPES VARCHAR2(4000) PATH '$.dataTypes' , COLUMN_PATTERNS VARCHAR2(4000) PATH '$.columnPatterns' , SELECT_LIST VARCHAR2(4000) PATH '$.importSelectList' $END ) ); return V_SQL_OPERATIONS; end; In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB, P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT('USERENV','CURRENT_SCHEMA')) as V_CURRENT_SCHEMA CONSTANT VARCHAR2(128) := SYS_CONTEXT('USERENV','CURRENT_SCHEMA'); V_START_TIME TIMESTAMP(6); V_END_TIME TIMESTAMP(6); begin SET_CURRENT_SCHEMA(P_TARGET_SCHEMA); SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE); for i in 1 .. SQL_OPERATIONS_TABLE.count loop begin V_START_TIME := SYSTIMESTAMP; execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE; SQL_OPERATIONS_TABLE(i).RESULT := 'Operation completed succecssfully at ' || SYS_EXTRACT_UTC(SYSTIMESTAMP) || '. Processed ' || TO_CHAR(SQL%ROWCOUNT) || ' rows. Elapsed time: ' || (V_END_TIME - V_START_TIME) || '.'; commit; SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS; exception when others then SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack; SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR; end; end loop; SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); exception when OTHERS then SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); RAISE; end; -- To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command: exp userid=system@ORCL owner=HR file=HR.dmp rows=N A new database schema was created using the following command in SQL*PLUS grant connect, resource, unlimited tablespace to HR2 identified by ******** And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below set echo on spool logs/IMPORT_FROM_FILE.log -- def JSON_DIR = &amp;amp;1 def FILENAME = &amp;amp;2 def SCHEMA = &amp;amp;3 -- VAR JSON CLOB -- create or replace directory JSON_DIR as '&amp;amp;JSON_DIR' / DECLARE V_DEST_OFFSET NUMBER := 1; V_SRC_OFFSET NUMBER := 1; V_CONTEXT NUMBER := 0; V_WARNINGS NUMBER := 0; V_BFILE BFILE := BFILENAME('JSON_DIR','&amp;amp;FILENAME'); begin DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION); DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY); DBMS_LOB.LOADCLOBFROMFILE ( :JSON,V_BFILE ,DBMS_LOB.LOBMAXSIZE ,V_DEST_OFFSET ,V_SRC_OFFSET ,NLS_CHARSET_ID('AL32UTF8') ,V_CONTEXT ,V_WARNINGS); DBMS_LOB.FILECLOSE(V_BFILE); end; / select 1 from DUAL where :JSON IS JSON / begin JSON_IMPORT.IMPORT_JSON(:JSON,'&amp;amp;SCHEMA'); end; / set pages 50 lines 256 trimspool on long 1000000 -- column TABLE_NAME format A30 column SQL_STATEMENT format A80 column STATUS format A12 column RESULT format A32 -- select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT from table(JSON_IMPORT.SQL_OPERATIONS) / exit After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to the transient inconsistencies that are inherent in a table by table data load process. Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported. The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully.</summary></entry><entry><title type="html">Object Tables and Wide Tables</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html" rel="alternate" type="text/html" title="Object Tables and Wide Tables" /><published>2018-06-19T18:00:00-07:00</published><updated>2018-06-19T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/19/Object%20and%20Wide%20Tables</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html">&lt;h2 id=&quot;supporting-object-tables-and-wide-tables&quot;&gt;Supporting Object Tables and Wide Tables&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/18/Objects-and-ANYDATA.html&quot;&gt;previous&lt;/a&gt; post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how  support for object serialization can be applied to object tables.&lt;/p&gt;

&lt;p&gt;In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$.  XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID.&lt;/p&gt;

&lt;p&gt;The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from  SYS_NC_OID$.&lt;/p&gt;

&lt;p&gt;Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; from ALL_ALL_TABLES aat
          inner join ALL_TAB_COLS atc
		         on atc.OWNER = aat.OWNER
		        and atc.TABLE_NAME = aat.TABLE_NAME
	 left outer join ALL_TYPES at
                  on at.TYPE_NAME = atc.DATA_TYPE
                 and at.OWNER = atc.DATA_TYPE_OWNER
   where aat.STATUS = 'VALID'
     and aat.DROPPED = 'NO'
	 and aat.TEMPORARY = 'N'
     and aat.EXTERNAL = 'NO'
	 and aat.NESTED = 'NO'
	 and aat.SECONDARY = 'N'
	 and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT')
	 and (
	       ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = 'NO'))
		   or 
		   (
		     (TABLE_TYPE is not NULL) 
		     and 
		     (COLUMN_NAME in ('SYS_NC_ROWINFO$','SYS_NC_OID$','ACLOID','OWNERID'))
		   )
         )		           							
	 and aat.OWNER = P_SCHEMA
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file.&lt;/p&gt;

&lt;p&gt;Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;ORA-40478: output value too large (maximum: 32767)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY.&lt;/p&gt;

&lt;p&gt;The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON.  With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2  or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value.&lt;/p&gt;

&lt;p&gt;Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual  JSON array.&lt;/p&gt;

&lt;p&gt;The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a  column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases.&lt;/p&gt;

&lt;p&gt;The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB)
as
   C_SELECT_LIST_START    CONSTANT VARCHAR2(32) := 'select JSON_ARRAY(';
   C_SELECT_LIST_END      CONSTANT VARCHAR2(32) := ' NULL on NULL';
   V_SQL_STATEMENT        CLOB;
   V_SELECT_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_SELECT_LIST_ITEM     VARCHAR2(4000);
   V_SELECT_LIST_START    PLS_INTEGER;
   V_SELECT_LIST_END      PLS_INTEGER;
   V_FROM_CLAUSE_START    PLS_INTEGER;
   V_CURRENT_OFFSET       PLS_INTEGER;
   V_COLUMN_OFFSET        PLS_INTEGER;
   
   V_FROM_WHERE_CLAUSE    VARCHAR2(32767);
   
   V_COLUMN_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_COLUMN_NAME          VARCHAR2(132);
   V_COLUMN_NAME_START    PLS_INTEGER;
   V_COLUMN_NAME_END      PLS_INTEGER;

   V_CURSOR               SYS_REFCURSOR;
   V_CURSOR_ID            NUMBER := DBMS_SQL.OPEN_CURSOR;
   V_COLUMN_DESCRIPTIONS  DBMS_SQL.DESC_TAB2;
   V_COLUMN_COUNT         NUMBER;
   V_COLUMN_VALUE         VARCHAR2(32767);
   V_FIRST_ROW            BOOLEAN := TRUE;
   V_FIRST_COLUMN         BOOLEAN := TRUE;

   V_INDEX                PLS_INTEGER;
begin
   V_SQL_STATEMENT      := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_SELECT_LIST_START  := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START)
                         + LENGTH(C_SELECT_LIST_START);
   V_SELECT_LIST_END    := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1;
   V_CURRENT_OFFSET := V_SELECT_LIST_START;
   loop
     V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,',',V_CURRENT_OFFSET); 
     exit when ((V_COLUMN_OFFSET &amp;lt; 1) or (V_COLUMN_OFFSET &amp;gt; V_SELECT_LIST_END));
	 V_SELECT_LIST.extend;
	 V_COLUMN_LIST.extend;
	 V_INDEX := V_SELECT_LIST.count;
	 V_SELECT_LIST_ITEM  := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET 
                          - V_CURRENT_OFFSET,V_CURRENT_OFFSET);

	 V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,'&quot;');
     V_COLUMN_NAME_END   := instr(V_SELECT_LIST_ITEM,'&quot;',V_COLUMN_NAME_START+1)+1;
     V_COLUMN_NAME       := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END
                           -V_COLUMN_NAME_START);

	 V_SELECT_LIST_ITEM  := 'JSON_ARRAY(' 
	                     || V_SELECT_LIST_ITEM 
	                     || ' NULL ON NULL RETURNING VARCHAR2(' 
	                     || C_MAX_OUTPUT_SIZE || ')) ' 
	                     || V_COLUMN_NAME;
						
	 V_SELECT_LIST(V_INDEX) :=  V_SELECT_LIST_ITEM;
	 v_COLUMN_LIST(V_INDEX) :=  V_COLUMN_NAME;
	 V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1;
   end loop;

   V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,' from ',V_SELECT_LIST_END);
   V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START);
   DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12);
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST));
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE));
   EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT;
   V_FIRST_ROW := TRUE;
   open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR);
   DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS);
   V_COLUMN_NAME_START := 2;
   for i in 1..V_COLUMN_COUNT loop
     DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767);
   end loop;
   while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &amp;gt; 0) loop
   	 if (not V_FIRST_ROW) then
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,',');
	 end if;
 	 V_FIRST_ROW := FALSE;
	 V_FIRST_COLUMN := TRUE;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,'[');
     for i in 1..V_COLUMN_COUNT loop
	   DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE);
	   V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2);
       if (not V_FIRST_COLUMN) then
         V_COLUMN_VALUE := ',' || V_COLUMN_VALUE;
  	   end if;
       V_FIRST_COLUMN := FALSE;
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE);
     end loop;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,']');
   end loop;
   DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID);
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/20/JSON_IMPORT.html&quot;&gt;next&lt;/a&gt; post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data.&lt;/p&gt;</content><author><name></name></author><summary type="html">Supporting Object Tables and Wide Tables The previous post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how support for object serialization can be applied to object tables. In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$. XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID. The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from SYS_NC_OID$. Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below from ALL_ALL_TABLES aat inner join ALL_TAB_COLS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME left outer join ALL_TYPES at on at.TYPE_NAME = atc.DATA_TYPE and at.OWNER = atc.DATA_TYPE_OWNER where aat.STATUS = 'VALID' and aat.DROPPED = 'NO' and aat.TEMPORARY = 'N' and aat.EXTERNAL = 'NO' and aat.NESTED = 'NO' and aat.SECONDARY = 'N' and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT') and ( ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = 'NO')) or ( (TABLE_TYPE is not NULL) and (COLUMN_NAME in ('SYS_NC_ROWINFO$','SYS_NC_OID$','ACLOID','OWNERID')) ) ) and aat.OWNER = P_SCHEMA With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file. Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in ORA-40478: output value too large (maximum: 32767) The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY. The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON. With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2 or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value. Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual JSON array. The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases. The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB) as C_SELECT_LIST_START CONSTANT VARCHAR2(32) := 'select JSON_ARRAY('; C_SELECT_LIST_END CONSTANT VARCHAR2(32) := ' NULL on NULL'; V_SQL_STATEMENT CLOB; V_SELECT_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_SELECT_LIST_ITEM VARCHAR2(4000); V_SELECT_LIST_START PLS_INTEGER; V_SELECT_LIST_END PLS_INTEGER; V_FROM_CLAUSE_START PLS_INTEGER; V_CURRENT_OFFSET PLS_INTEGER; V_COLUMN_OFFSET PLS_INTEGER; V_FROM_WHERE_CLAUSE VARCHAR2(32767); V_COLUMN_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_COLUMN_NAME VARCHAR2(132); V_COLUMN_NAME_START PLS_INTEGER; V_COLUMN_NAME_END PLS_INTEGER; V_CURSOR SYS_REFCURSOR; V_CURSOR_ID NUMBER := DBMS_SQL.OPEN_CURSOR; V_COLUMN_DESCRIPTIONS DBMS_SQL.DESC_TAB2; V_COLUMN_COUNT NUMBER; V_COLUMN_VALUE VARCHAR2(32767); V_FIRST_ROW BOOLEAN := TRUE; V_FIRST_COLUMN BOOLEAN := TRUE; V_INDEX PLS_INTEGER; begin V_SQL_STATEMENT := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_SELECT_LIST_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START) + LENGTH(C_SELECT_LIST_START); V_SELECT_LIST_END := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1; V_CURRENT_OFFSET := V_SELECT_LIST_START; loop V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,',',V_CURRENT_OFFSET); exit when ((V_COLUMN_OFFSET &amp;lt; 1) or (V_COLUMN_OFFSET &amp;gt; V_SELECT_LIST_END)); V_SELECT_LIST.extend; V_COLUMN_LIST.extend; V_INDEX := V_SELECT_LIST.count; V_SELECT_LIST_ITEM := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET - V_CURRENT_OFFSET,V_CURRENT_OFFSET); V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,'&quot;'); V_COLUMN_NAME_END := instr(V_SELECT_LIST_ITEM,'&quot;',V_COLUMN_NAME_START+1)+1; V_COLUMN_NAME := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END -V_COLUMN_NAME_START); V_SELECT_LIST_ITEM := 'JSON_ARRAY(' || V_SELECT_LIST_ITEM || ' NULL ON NULL RETURNING VARCHAR2(' || C_MAX_OUTPUT_SIZE || ')) ' || V_COLUMN_NAME; V_SELECT_LIST(V_INDEX) := V_SELECT_LIST_ITEM; v_COLUMN_LIST(V_INDEX) := V_COLUMN_NAME; V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1; end loop; V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,' from ',V_SELECT_LIST_END); V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START); DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12); DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST)); DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE)); EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT; V_FIRST_ROW := TRUE; open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR); DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS); V_COLUMN_NAME_START := 2; for i in 1..V_COLUMN_COUNT loop DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767); end loop; while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &amp;gt; 0) loop if (not V_FIRST_ROW) then DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,','); end if; V_FIRST_ROW := FALSE; V_FIRST_COLUMN := TRUE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,'['); for i in 1..V_COLUMN_COUNT loop DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE); V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2); if (not V_FIRST_COLUMN) then V_COLUMN_VALUE := ',' || V_COLUMN_VALUE; end if; V_FIRST_COLUMN := FALSE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE); end loop; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,']'); end loop; DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID); end; With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas. The next post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data.</summary></entry><entry><title type="html">Unsupported Data Types (Part 3)</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/18/Objects-and-ANYDATA.html" rel="alternate" type="text/html" title="Unsupported Data Types (Part 3)" /><published>2018-06-18T18:00:00-07:00</published><updated>2018-06-18T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/18/Objects%20and%20ANYDATA</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/18/Objects-and-ANYDATA.html">&lt;h2 id=&quot;adding-support-for-objects-collections-and-anydata&quot;&gt;Adding Support for Objects, Collections and ANYDATA.&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/17/BFILE-and-BLOB.html&quot;&gt;previous&lt;/a&gt; post showed how to use simple In-line PL/SQL procedures to support the BFILE and BLOB data types. This post will expand on that idea to add support for Object Types, Collection Types and the ANYDATA data type.&lt;/p&gt;

&lt;p&gt;At first glance supporting Objects and Collections appeared to be simple, but as was mentioned earlier the Devil’s in the details. The first approach considered was to leverage Oracle’s ability to generate XML from an Object and to then covert that XML back into objects. Unfortunately as the following simple example shows that approach appears to be a non-starter  since it appears that NULL objects are transformed into empty objects&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SQL&amp;gt; set pages 0
SQL&amp;gt; set lines 132
SQL&amp;gt; with
  2  function XML_2_SDO_GEOMETRY(P_XMLTYPE XMLTYPE) return SDO_GEOMETRY
  3  as
  4    V_SDO_GEOMETRY SDO_GEOMETRY;
  5  begin
  6    P_XMLTYPE.TOOBJECT(V_SDO_GEOMETRY);
  7    return V_SDO_GEOMETRY;
  8  end;
  9  select WH_GEO_LOCATION, XML_2_SDO_GEOMETRY(XMLTYPE(WH_GEO_LOCATION))
 10    from OE.WAREHOUSES
 11   where WH_GEO_LOCATION is not NULL;
 12  /
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SDO_GEOMETRY(2001, 8307, SDO_POINT_TYPE(-103.00195, 36.500374, NULL), NULL, NULL)
SDO_GEOMETRY(2001, 8307, SDO_POINT_TYPE(-103.00195, 36.500374, NULL), SDO_ELEM_INFO_ARRAY(), SDO_ORDINATE_ARRAY())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second approach considered was to try to handle Objects and Collections using Oracle’s ANYDATA and ANYTYPE feature. This appeared quite promising at first, but the approach failed when attempting to deal with objects that contained non-scalar attributes. If ANYDATA provided a method to retrieve a non-scalar attribute as an instance of ANYDATA, the approach might have proved to be workable. Unfortunately the only was to fetch the value of non-scalar attribute from an ANYDATA is using code which knows ipso-facto what the object type of the attribute being fetched is. This makes it impossible to write generic ANYDATA based code that can traverse a nested hierarchy of objects without requiring baked-in knowledge of the structure of the objects. Attempts to work around this limitation  using execute immediate also proved fruitless, as the context of the piecewise operations used to traverse the attributes of an object appear to get lost when the ANYDATA object is passed as a bind variable. The final nail in the ANYDATA coffin was the discovery that ANYDATA does not support all of the scalar data types support by Oracle’s object types. An example of this is the ORDSYS family of data types which declare attributes of data type INTEGER. These value of these attributes cannot be accessed using the methods provided by the ANYDATA object.&lt;/p&gt;

&lt;p&gt;As can be seen above, when you retrieve an Oracle object in SQLPLUS it is serialized as a set of nested constructors. This representation makes quite a bit of sense, since the serialized representation is, in effect, the code required to recreate the object using SQL. Unfortunately while SQL*PLUS appears to contain the code required to serialize an object in this format, it does not appear that this functionality is exposed in SQL. One would have hoped that the TO_CHAR or TO_CLOB operators provided this functionality, or that Oracle could have provided a SERIALIZE operator, similar to XMLSERIALIZE, but it appears that this is not the case.&lt;/p&gt;

&lt;p&gt;The missing functionality was addressed by creating an OBJECT_SERIALIZATION package. This package takes an object, or set of objects, and generates a PL/SQL code that outputs a CLOB containing the serialized representation of an instance of that object. The code generated by the object will handle simple object types, complex types, e.g. types that contain non-scalar attributes, and collection types. It will also handle objects that are part of a type hierarchy. Creating this package was non-trivial but that is the subject of another post, maybe.&lt;/p&gt;

&lt;p&gt;The package exposes two methods. The first, GENERATE_TABLE_TYPES is used when processing a schema on a table by table basis, while the second, GENERATE_SCHEMA_TYPES, is used when processing an entire schema in a single operation. Both methods return a PL/SQL procedure and a PL/SQL function called SERIALIZE_OBJECT. The function and procedure both expect the object to be serialized to be passed as an instance of the ANYDATA data type.&lt;/p&gt;

&lt;p&gt;The function is simply  a wrapper for the procedure that creates the CLOB that contains the serialized representation of the object and invokes the procedure. When the SERIALIZE_OBJECT procedure completes the function returns the serialized representation of the object to the caller. This structure makes it possible to use a single CLOB to manage the object serialization process and invoke it directly from SQL.&lt;/p&gt;

&lt;p&gt;The procedure consists of a PL/SQL block specific to each of object types that can be encountered while processing the target table or schema. Each block creates a serialized representation of a specific object type by printing it’s attributes into a CLOB.  If an objects contains an attribute that is based on an object type or collection type, that attribute is serialized by converting it into an ANYDATA and making a recursive call to SERIALIZE_OBJECT.&lt;/p&gt;

&lt;p&gt;Since objects can contain attributes of type BFILE and BLOB the generated code makes use of the BFILE2CHAR and BLOB2HEXBINARY functions that were described in the previous post, and these functions are included in the PL/SQL code returned by GENERATE_TABLE_TYPES and GENERATE_SCHEMA_TYPES..&lt;/p&gt;

&lt;p&gt;The other possibility for managing Object and Collection types is to generate a JSON representation of the object. Again, as of release 18, this functionality is not present in Oracle. Adding this functionality is one of a long list of enhancements under consideration future versions of the JSON_EXPORT package. However, the JSON serialization of an object would probably be much larger than the ‘native’ serialization, and so would not add value if we are simply interested in exporting data  from one Oracle database and importing into another.&lt;/p&gt;

&lt;p&gt;Support for the ANYDATA data type was added by creating a function, SERIALIZE_ANYDATA, that serializes ANYDATA objects using logic similar to the logic used for serializing objects. However this function is limited in scope. Due to the issues outlined earlier in this post it does not handle ANYDATAs based on objects that have attributes whose data types that are not supported by ANYDATA, or ANYDATAs based on objects that contain attributes based on object or collection types.&lt;/p&gt;

&lt;p&gt;Once the OBJECT_SERIALIZATION package was created it is relatively easy to incorporate the provided  functionality into the JSON_EXPORT package. The case statement in GENERATE_STATEMENT is modified to convert the contents of Object or Collection type columns into an ANYDATA instance and invoke the SERIALIZE_OBJECT function&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;/*
**
** Support ANYDATA, OBJECT and COLLECTION types
**
*/
when DATA_TYPE = 'ANYDATA'  -- Can be owned by SYS or PUBLIC
   then 'case when &quot;' ||  COLUMN_NAME || '&quot; is NULL then NULL 
              else SERIALIZE_ANYDATA(&quot;' ||  COLUMN_NAME || '&quot;) 
         end'
when TYPECODE = 'COLLECTION'
  then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL 
             else SERIALIZE_OBJECT(ANYDATA.convertCollection(&quot;' || COLUMN_NAME || '&quot;)) 
        end'
when TYPECODE = 'OBJECT'
  then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL 
             else SERIALIZE_OBJECT(ANYDATA.convertObject(&quot;' || COLUMN_NAME || '&quot;)) 
        end'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A left outer join with ALL_OBJECTS is required to allow the TYPECODE column to be used to differentiate Object and Collection types.&lt;/p&gt;

&lt;p&gt;The GENERATE_WITH_CLAUSE method is modified to invoke the GENERATE_TABLE_TYPES or GENERATE_SCHEMA_TYPES functions as required and to add support for including the SERIALIZE_ANYDATA function into the WITH clause where necessary&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;$IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN
procedure GENERATE_WITH_CLAUSE(P_OWNER VARCHAR2, P_TABLE_NAME_LIST T_VC4000_TABLE, 
                               P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, 
                               P_ANYDATA_COUNT NUMBER, P_SQL_STATEMENT IN OUT CLOB)
as
  V_OBJECT_SERIALIZER CLOB;
begin
  V_OBJECT_SERIALIZER := OBJECT_SERIALIZATION.SERIALIZE_TABLE_TYPES(P_OWNER,P_TABLE_NAME_LIST);
$ELSE
procedure GENERATE_WITH_CLAUSE(P_OWNER VARCHAR2, P_TABLE_NAME VARCHAR2, 
                               P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, 
                               P_ANYDATA_COUNT NUMBER, P_SQL_STATEMENT IN OUT CLOB)
as
  V_OBJECT_SERIALIZER CLOB;
begin
  V_OBJECT_SERIALIZER :=  OBJECT_SERIALIZATION.SERIALIZE_TABLE_TYPES(P_OWNER,P_TABLE_NAME);	
$END
  if ((P_BFILE_COUNT + P_BLOB_COUNT + P_ANYDATA_COUNT=0) AND (V_OBJECT_SERIALIZER is NULL)) then
    return;
  end if;

  DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB('WITH' || C_NEWLINE));
  if (V_OBJECT_SERIALIZER is not null) then
    DBMS_LOB.APPEND(P_SQL_STATEMENT,V_OBJECT_SERIALIZER);
  else
    if ((P_BFILE_COUNT &amp;gt; 0) or (P_ANYDATA_COUNT &amp;gt; 0)) then
      DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_BFILE2CHAR));
    end if;
    if ((P_BLOB_COUNT &amp;gt; 0) or (P_ANYDATA_COUNT &amp;gt; 0)) then
      DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_BLOB2HEXBINARY));
    end if;
	if (P_ANYDATA_COUNT &amp;gt; 0) then
      DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_SERIALIZE_ANYDATA));
	end if;
 end if;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once the necessary modifications have been made, EXPORT_SCHEMA successfully includes the content of the  WAREHOUSE and CUSTOMERS tables in the export file. However the Object tables CATEOGORIES_TAB and PURCHASEORDER are still excluded. The &lt;a href=&quot;/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html&quot;&gt;next&lt;/a&gt; post will examine what is required to add support for object tables to  the EXPORT_SCHEMA function.&lt;/p&gt;</content><author><name></name></author><summary type="html">Adding Support for Objects, Collections and ANYDATA. The previous post showed how to use simple In-line PL/SQL procedures to support the BFILE and BLOB data types. This post will expand on that idea to add support for Object Types, Collection Types and the ANYDATA data type. At first glance supporting Objects and Collections appeared to be simple, but as was mentioned earlier the Devil’s in the details. The first approach considered was to leverage Oracle’s ability to generate XML from an Object and to then covert that XML back into objects. Unfortunately as the following simple example shows that approach appears to be a non-starter since it appears that NULL objects are transformed into empty objects SQL&amp;gt; set pages 0 SQL&amp;gt; set lines 132 SQL&amp;gt; with 2 function XML_2_SDO_GEOMETRY(P_XMLTYPE XMLTYPE) return SDO_GEOMETRY 3 as 4 V_SDO_GEOMETRY SDO_GEOMETRY; 5 begin 6 P_XMLTYPE.TOOBJECT(V_SDO_GEOMETRY); 7 return V_SDO_GEOMETRY; 8 end; 9 select WH_GEO_LOCATION, XML_2_SDO_GEOMETRY(XMLTYPE(WH_GEO_LOCATION)) 10 from OE.WAREHOUSES 11 where WH_GEO_LOCATION is not NULL; 12 / SDO_GEOMETRY(2001, 8307, SDO_POINT_TYPE(-103.00195, 36.500374, NULL), NULL, NULL) SDO_GEOMETRY(2001, 8307, SDO_POINT_TYPE(-103.00195, 36.500374, NULL), SDO_ELEM_INFO_ARRAY(), SDO_ORDINATE_ARRAY()) The second approach considered was to try to handle Objects and Collections using Oracle’s ANYDATA and ANYTYPE feature. This appeared quite promising at first, but the approach failed when attempting to deal with objects that contained non-scalar attributes. If ANYDATA provided a method to retrieve a non-scalar attribute as an instance of ANYDATA, the approach might have proved to be workable. Unfortunately the only was to fetch the value of non-scalar attribute from an ANYDATA is using code which knows ipso-facto what the object type of the attribute being fetched is. This makes it impossible to write generic ANYDATA based code that can traverse a nested hierarchy of objects without requiring baked-in knowledge of the structure of the objects. Attempts to work around this limitation using execute immediate also proved fruitless, as the context of the piecewise operations used to traverse the attributes of an object appear to get lost when the ANYDATA object is passed as a bind variable. The final nail in the ANYDATA coffin was the discovery that ANYDATA does not support all of the scalar data types support by Oracle’s object types. An example of this is the ORDSYS family of data types which declare attributes of data type INTEGER. These value of these attributes cannot be accessed using the methods provided by the ANYDATA object. As can be seen above, when you retrieve an Oracle object in SQLPLUS it is serialized as a set of nested constructors. This representation makes quite a bit of sense, since the serialized representation is, in effect, the code required to recreate the object using SQL. Unfortunately while SQL*PLUS appears to contain the code required to serialize an object in this format, it does not appear that this functionality is exposed in SQL. One would have hoped that the TO_CHAR or TO_CLOB operators provided this functionality, or that Oracle could have provided a SERIALIZE operator, similar to XMLSERIALIZE, but it appears that this is not the case. The missing functionality was addressed by creating an OBJECT_SERIALIZATION package. This package takes an object, or set of objects, and generates a PL/SQL code that outputs a CLOB containing the serialized representation of an instance of that object. The code generated by the object will handle simple object types, complex types, e.g. types that contain non-scalar attributes, and collection types. It will also handle objects that are part of a type hierarchy. Creating this package was non-trivial but that is the subject of another post, maybe. The package exposes two methods. The first, GENERATE_TABLE_TYPES is used when processing a schema on a table by table basis, while the second, GENERATE_SCHEMA_TYPES, is used when processing an entire schema in a single operation. Both methods return a PL/SQL procedure and a PL/SQL function called SERIALIZE_OBJECT. The function and procedure both expect the object to be serialized to be passed as an instance of the ANYDATA data type. The function is simply a wrapper for the procedure that creates the CLOB that contains the serialized representation of the object and invokes the procedure. When the SERIALIZE_OBJECT procedure completes the function returns the serialized representation of the object to the caller. This structure makes it possible to use a single CLOB to manage the object serialization process and invoke it directly from SQL. The procedure consists of a PL/SQL block specific to each of object types that can be encountered while processing the target table or schema. Each block creates a serialized representation of a specific object type by printing it’s attributes into a CLOB. If an objects contains an attribute that is based on an object type or collection type, that attribute is serialized by converting it into an ANYDATA and making a recursive call to SERIALIZE_OBJECT. Since objects can contain attributes of type BFILE and BLOB the generated code makes use of the BFILE2CHAR and BLOB2HEXBINARY functions that were described in the previous post, and these functions are included in the PL/SQL code returned by GENERATE_TABLE_TYPES and GENERATE_SCHEMA_TYPES.. The other possibility for managing Object and Collection types is to generate a JSON representation of the object. Again, as of release 18, this functionality is not present in Oracle. Adding this functionality is one of a long list of enhancements under consideration future versions of the JSON_EXPORT package. However, the JSON serialization of an object would probably be much larger than the ‘native’ serialization, and so would not add value if we are simply interested in exporting data from one Oracle database and importing into another. Support for the ANYDATA data type was added by creating a function, SERIALIZE_ANYDATA, that serializes ANYDATA objects using logic similar to the logic used for serializing objects. However this function is limited in scope. Due to the issues outlined earlier in this post it does not handle ANYDATAs based on objects that have attributes whose data types that are not supported by ANYDATA, or ANYDATAs based on objects that contain attributes based on object or collection types. Once the OBJECT_SERIALIZATION package was created it is relatively easy to incorporate the provided functionality into the JSON_EXPORT package. The case statement in GENERATE_STATEMENT is modified to convert the contents of Object or Collection type columns into an ANYDATA instance and invoke the SERIALIZE_OBJECT function /* ** ** Support ANYDATA, OBJECT and COLLECTION types ** */ when DATA_TYPE = 'ANYDATA' -- Can be owned by SYS or PUBLIC then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL else SERIALIZE_ANYDATA(&quot;' || COLUMN_NAME || '&quot;) end' when TYPECODE = 'COLLECTION' then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL else SERIALIZE_OBJECT(ANYDATA.convertCollection(&quot;' || COLUMN_NAME || '&quot;)) end' when TYPECODE = 'OBJECT' then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL else SERIALIZE_OBJECT(ANYDATA.convertObject(&quot;' || COLUMN_NAME || '&quot;)) end' A left outer join with ALL_OBJECTS is required to allow the TYPECODE column to be used to differentiate Object and Collection types. The GENERATE_WITH_CLAUSE method is modified to invoke the GENERATE_TABLE_TYPES or GENERATE_SCHEMA_TYPES functions as required and to add support for including the SERIALIZE_ANYDATA function into the WITH clause where necessary $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN procedure GENERATE_WITH_CLAUSE(P_OWNER VARCHAR2, P_TABLE_NAME_LIST T_VC4000_TABLE, P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, P_ANYDATA_COUNT NUMBER, P_SQL_STATEMENT IN OUT CLOB) as V_OBJECT_SERIALIZER CLOB; begin V_OBJECT_SERIALIZER := OBJECT_SERIALIZATION.SERIALIZE_TABLE_TYPES(P_OWNER,P_TABLE_NAME_LIST); $ELSE procedure GENERATE_WITH_CLAUSE(P_OWNER VARCHAR2, P_TABLE_NAME VARCHAR2, P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, P_ANYDATA_COUNT NUMBER, P_SQL_STATEMENT IN OUT CLOB) as V_OBJECT_SERIALIZER CLOB; begin V_OBJECT_SERIALIZER := OBJECT_SERIALIZATION.SERIALIZE_TABLE_TYPES(P_OWNER,P_TABLE_NAME); $END if ((P_BFILE_COUNT + P_BLOB_COUNT + P_ANYDATA_COUNT=0) AND (V_OBJECT_SERIALIZER is NULL)) then return; end if; DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB('WITH' || C_NEWLINE)); if (V_OBJECT_SERIALIZER is not null) then DBMS_LOB.APPEND(P_SQL_STATEMENT,V_OBJECT_SERIALIZER); else if ((P_BFILE_COUNT &amp;gt; 0) or (P_ANYDATA_COUNT &amp;gt; 0)) then DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_BFILE2CHAR)); end if; if ((P_BLOB_COUNT &amp;gt; 0) or (P_ANYDATA_COUNT &amp;gt; 0)) then DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_BLOB2HEXBINARY)); end if; if (P_ANYDATA_COUNT &amp;gt; 0) then DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(OBJECT_SERIALIZATION.CODE_SERIALIZE_ANYDATA)); end if; end if; end; Once the necessary modifications have been made, EXPORT_SCHEMA successfully includes the content of the WAREHOUSE and CUSTOMERS tables in the export file. However the Object tables CATEOGORIES_TAB and PURCHASEORDER are still excluded. The next post will examine what is required to add support for object tables to the EXPORT_SCHEMA function.</summary></entry><entry><title type="html">Unsupported Data Types (Part 2)</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/17/BFILE-and-BLOB.html" rel="alternate" type="text/html" title="Unsupported Data Types (Part 2)" /><published>2018-06-17T18:00:00-07:00</published><updated>2018-06-17T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/17/BFILE%20and%20BLOB</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/17/BFILE-and-BLOB.html">&lt;h2 id=&quot;using-in-line-plsql-to-support-bfile-and-blob&quot;&gt;Using In-line PL/SQL to support BFILE and BLOB&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/16/Unsupported-Scalar-Types.html&quot;&gt;previous&lt;/a&gt; post showed how to use SQL conversions to extend the set of data types that can be used with JSON_ARRAY. This post will build on that concept showing how in-line PL/SQL can be used in places where pure SQL is not sufficient.&lt;/p&gt;

&lt;p&gt;JSON_ARRAY does not support the BFILE data type in any current release of the Oracle Database. A BFILE is basically a pointer to a file stored outside the database. The pointer consists of two components, a SQL Directory name and a path to the file, relative to the folder associated with the directory. There are 2 approaches to supporting the BFILE datatype.  The first is to inline the content of the file directly into the export, the second is to serialize the BFILE as a directory name and a relative path. In the current implementation of EXPORT_SCHEMA the second option is used.&lt;/p&gt;

&lt;p&gt;It is not possible to extract this information directly a from BFILE column using SQL, so PL/SQL needs to be used.  The PL/SQL is extremely simple, given a BFILE column the method DBMS_LOB.FILEGETNAME will return the directory name and filename. However, since the FILEGETNAME method uses OUT parameters to return the required information it cannot be called directly from SQL, so a PL/SQL wrapper is required to allow FILEGETNAME to be invoked as part of the JSON_ARRAY operation.&lt;/p&gt;

&lt;p&gt;The wrapper function is show below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;function BFILE2CHAR(P_BFILE BFILE) return VARCHAR2
as
  V_SINGLE_QUOTE     CONSTANT CHAR(1) := CHR(39);
  V_DIRECTORY_ALIAS  VARCHAR2(128 CHAR);
  V_PATH2FILE        VARCHAR2(2000 CHAR);
begin
  DBMS_LOB.FILEGETNAME(P_BFILE,V_DIRECTORY_ALIAS,V_PATH2FILE);
  return 'BFILENAME(' || V_SINGLE_QUOTE || V_DIRECTORY_ALIAS || V_SINGLE_QUOTE || ',' 
                      || V_SINGLE_QUOTE || V_PATH2FILE || V_SINGLE_QUOTE || ')';
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It returns the directory and file names as the SQL function that was used when the BFILE was created.&lt;/p&gt;

&lt;p&gt;A similar approach is taken in Oracle 12 when dealing with BLOB datatypes. Oracle 18 correctly outputs BLOB content as a HEXBINARY string. This behavior is mimicked in Oracle 12 by supplying  a PL/SQL wrapper that returns a CLOB containing the HEXBINARY representation of the BLOB.  The maximum size that can be handled using this approach is limited based on the return type specified for the JSON_ARRAY operation. Consequently, an error will be returned if the HEXBINARY representation of the input document is larger than the maximum size supported by the JSON_ARRAY operator. The size check assumes that the BLOB column is the only input to JSON_ARRAY, and it’s primary use is to avoid the overhead of serializing large documents in cases where the generated output cannot be processed further.&lt;/p&gt;

&lt;p&gt;The following changes were made to the case statement in GENERATE_SCHEMA. For handling BLOBS in release 12&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;/*
** 18.1 compatible handling of BLOB
*/
when DATA_TYPE = 'BLOB'
  then 'BLOB2HEXBINARY(&quot;' || COLUMN_NAME || '&quot;)' 	
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and for handling BFILES in all releases&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;/*
** Fix for BFILENAME
*/
when DATA_TYPE = 'BFILE'
  then 'BFILE2CHAR(&quot;' || COLUMN_NAME || '&quot;)'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In order to enable the use of these functions from EXPORT_SCHEMA a WITH clause containing the PL/SQL code is added the SQL statement(s) created by GENERATE_SCHEMA. To ensure that the WITH block is only added when necessary, the query in GENERATE_SCHEMA is modified to count the number of BLOB and BFILE columns for each table as shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select aat.owner
        ,aat.table_name
  	    ,SUM(CASE WHEN DATA_TYPE = 'BLOB'  THEN 1 ELSE 0 END) BLOB_COUNT
  	    ,SUM(CASE WHEN DATA_TYPE = 'BFILE' THEN 1 ELSE 0 END) BFILE_COUNT
	    ,cast(collect...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A new method, GENERATE_WITH_CLAUSE is added to generate the WITH clause&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;procedure GENERATE_WITH_CLAUSE(P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, 
                               P_SQL_STATEMENT IN OUT CLOB)
as
begin
  if ((P_BFILE_COUNT = 0) and (P_BLOB_COUNT = 0)) then
    return;
  end if;
  DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB('WITH' || C_NEWLINE));
  if (P_BFILE_COUNT &amp;gt; 0) then
    DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(CODE_BFILE2CHAR));
  end if;
  if (P_BLOB_COUNT &amp;gt; 0) then
    DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(CODE_BLOB2HEXBINARY));
  end if;
end;

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This method is invoked in GENERATE schema prior to printing the ‘select’ into the LOB that contains the generated SQL statement. The &lt;a href=&quot;/json/export/import/oracle/2018/06/18/Objects-and-ANYDATA.html&quot;&gt;next&lt;/a&gt; post will discuss how a similar technique can be used to support Oracle OBJECT types and the ANYDATA type.&lt;/p&gt;</content><author><name></name></author><summary type="html">Using In-line PL/SQL to support BFILE and BLOB The previous post showed how to use SQL conversions to extend the set of data types that can be used with JSON_ARRAY. This post will build on that concept showing how in-line PL/SQL can be used in places where pure SQL is not sufficient. JSON_ARRAY does not support the BFILE data type in any current release of the Oracle Database. A BFILE is basically a pointer to a file stored outside the database. The pointer consists of two components, a SQL Directory name and a path to the file, relative to the folder associated with the directory. There are 2 approaches to supporting the BFILE datatype. The first is to inline the content of the file directly into the export, the second is to serialize the BFILE as a directory name and a relative path. In the current implementation of EXPORT_SCHEMA the second option is used. It is not possible to extract this information directly a from BFILE column using SQL, so PL/SQL needs to be used. The PL/SQL is extremely simple, given a BFILE column the method DBMS_LOB.FILEGETNAME will return the directory name and filename. However, since the FILEGETNAME method uses OUT parameters to return the required information it cannot be called directly from SQL, so a PL/SQL wrapper is required to allow FILEGETNAME to be invoked as part of the JSON_ARRAY operation. The wrapper function is show below function BFILE2CHAR(P_BFILE BFILE) return VARCHAR2 as V_SINGLE_QUOTE CONSTANT CHAR(1) := CHR(39); V_DIRECTORY_ALIAS VARCHAR2(128 CHAR); V_PATH2FILE VARCHAR2(2000 CHAR); begin DBMS_LOB.FILEGETNAME(P_BFILE,V_DIRECTORY_ALIAS,V_PATH2FILE); return 'BFILENAME(' || V_SINGLE_QUOTE || V_DIRECTORY_ALIAS || V_SINGLE_QUOTE || ',' || V_SINGLE_QUOTE || V_PATH2FILE || V_SINGLE_QUOTE || ')'; end; It returns the directory and file names as the SQL function that was used when the BFILE was created. A similar approach is taken in Oracle 12 when dealing with BLOB datatypes. Oracle 18 correctly outputs BLOB content as a HEXBINARY string. This behavior is mimicked in Oracle 12 by supplying a PL/SQL wrapper that returns a CLOB containing the HEXBINARY representation of the BLOB. The maximum size that can be handled using this approach is limited based on the return type specified for the JSON_ARRAY operation. Consequently, an error will be returned if the HEXBINARY representation of the input document is larger than the maximum size supported by the JSON_ARRAY operator. The size check assumes that the BLOB column is the only input to JSON_ARRAY, and it’s primary use is to avoid the overhead of serializing large documents in cases where the generated output cannot be processed further. The following changes were made to the case statement in GENERATE_SCHEMA. For handling BLOBS in release 12 /* ** 18.1 compatible handling of BLOB */ when DATA_TYPE = 'BLOB' then 'BLOB2HEXBINARY(&quot;' || COLUMN_NAME || '&quot;)' and for handling BFILES in all releases /* ** Fix for BFILENAME */ when DATA_TYPE = 'BFILE' then 'BFILE2CHAR(&quot;' || COLUMN_NAME || '&quot;)' In order to enable the use of these functions from EXPORT_SCHEMA a WITH clause containing the PL/SQL code is added the SQL statement(s) created by GENERATE_SCHEMA. To ensure that the WITH block is only added when necessary, the query in GENERATE_SCHEMA is modified to count the number of BLOB and BFILE columns for each table as shown below select aat.owner ,aat.table_name ,SUM(CASE WHEN DATA_TYPE = 'BLOB' THEN 1 ELSE 0 END) BLOB_COUNT ,SUM(CASE WHEN DATA_TYPE = 'BFILE' THEN 1 ELSE 0 END) BFILE_COUNT ,cast(collect... A new method, GENERATE_WITH_CLAUSE is added to generate the WITH clause procedure GENERATE_WITH_CLAUSE(P_BFILE_COUNT NUMBER, P_BLOB_COUNT NUMBER, P_SQL_STATEMENT IN OUT CLOB) as begin if ((P_BFILE_COUNT = 0) and (P_BLOB_COUNT = 0)) then return; end if; DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB('WITH' || C_NEWLINE)); if (P_BFILE_COUNT &amp;gt; 0) then DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(CODE_BFILE2CHAR)); end if; if (P_BLOB_COUNT &amp;gt; 0) then DBMS_LOB.APPEND(P_SQL_STATEMENT,TO_CLOB(CODE_BLOB2HEXBINARY)); end if; end; This method is invoked in GENERATE schema prior to printing the ‘select’ into the LOB that contains the generated SQL statement. The next post will discuss how a similar technique can be used to support Oracle OBJECT types and the ANYDATA type.</summary></entry><entry><title type="html">Unsupported Data Types (Part 1)</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/16/Unsupported-Scalar-Types.html" rel="alternate" type="text/html" title="Unsupported Data Types (Part 1)" /><published>2018-06-16T18:00:00-07:00</published><updated>2018-06-16T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/16/Unsupported%20Scalar%20Types</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/16/Unsupported-Scalar-Types.html">&lt;h2 id=&quot;the-devils-in-the-details-unsupported-scalar-data-types&quot;&gt;The Devil’s in the Details: Unsupported scalar data types&lt;/h2&gt;

&lt;p&gt;In the &lt;a href=&quot;/json/export/import/oracle/2018/06/15/JSON_EXPORT.html&quot;&gt;previous&lt;/a&gt; post, the first attempt to run EXPORT_SCHEMA on the SH schema failed due to fact one of the tables contained a BLOB column. Fortunately, in that case of the SH schema, the BLOB column was in a table that needed to be excluded from the export operation, and all remaining tables in the SH schema use data types that are directly supported by the JSON_ARRAY operator However that incident was simply a precursor for the problems that are encountered when attempting to process the OE schema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('OE') from dual;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;ERROR:
ORA-40654: Input to JSON generation function has unsupported data type.
ORA-06512: at &quot;SYSTEM.JSON_EXPORT&quot;, line 197
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In order to determine the set of data types supported by JSON_ARRAY in each database release  a simple script was developed. The script creates a table containing one column for each possible scalar data type, insert 1 row of data into the tables and then attempts to perform a JSON_ARRAY operation on each column. The script, DATA_TYPE_TEST.sql can be found on GitHub in the project’s test directory. The results of running this script on 12.2 and 18.1 are documented in the following table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Data Type&lt;/th&gt;
      &lt;th&gt;12cR2&lt;/th&gt;
      &lt;th&gt;18S&lt;/th&gt;
      &lt;th&gt;SQL Workaround&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CHAR (BYTE semantics)``&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CHAR (CHAR semantics&lt;/td&gt;
      &lt;td&gt;:heavy_check_mark:&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VARCHAR2 (Byte Semantics)&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;VARCHAR2 (Char Semantics)&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NCHAR&lt;/td&gt;
      &lt;td&gt;❗️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NVARCHAR&lt;/td&gt;
      &lt;td&gt;❗️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NUMBER&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NUMBER (Precision)&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NUMBER (Precision,Scale)&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BINARY FLOAT&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;TO_CHAR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BINARY DECIMAL&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;TO_CHAR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DATE&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TIMESTAMP&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TIMESTAMP WITH TIME ZONE&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;TIMESTAMP WITH LOCAL TIME ZONE&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;SYS_EXTRACT_UTC&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;INTERVAL YEAR TO MONTH&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;Custom SQL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;INTERVAL DAY TO SECOND&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;Custom SQL&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;RAW&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ROWID&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;ROWIDTOCHAR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;UROWID&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;ROWIDTOCHAR&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BLOB&lt;/td&gt;
      &lt;td&gt;❗️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLOB&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;NCLOB&lt;/td&gt;
      &lt;td&gt;❗️&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BFILE&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;XMLTYPE&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;XMLSEQUENCE&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LONG&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LONG RAW&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The script also demonstrates potential solutions for some of the data types that are not supported natively.&lt;/p&gt;

&lt;p&gt;❗️ The JSON_ARRAY documentation for Oracle 12.2 clearly states that BLOB is not a supported data type. However, as is shown by the DATA_TYPE_TESTS.sql script, if a BLOB is passed to the JSON_ARRAY operator it does not raise the expected “ORA-40654: Input to JSON generation function has unsupported data type” exception. Rather it mistakenly assumes that the BLOB column  contains textual data and attempts to process the content as text. If the BLOB contains binary data this will typically result in the exception “ORA-40474: invalid UTF-8 byte sequence in JSON data” being raised by JSON_ARRAY. In Oracle 18c the BLOB data type is supported by JSON_ARRAY. When a  BLOB is passed to JSON_ARRAY a HEXBINARY encoded representation of the BLOB’s content is returned by the JSON_ARRAY operator.&lt;/p&gt;

&lt;p&gt;❗️There is a similar issue with the way in which JSON_ARRAY handles of NCHAR, NVARCHAR2 and NCLOB in database 12.2 which can also result in exception  “ORA-40474: invalid UTF-8 byte sequence in JSON data”  being raised. A possible workaround for this, at least in databases configured to use ‘AL32UTF8’ as the database character set, is to apply the TO_CHAR or TO_CLOB operator to the column before passing it to the JSON_ARRAY operator. Note that it is likely that this workaround will fail in non AL32UTF8 environments.&lt;/p&gt;

&lt;p&gt;Running the DATA_TYPE_TESTS.sql script in Oracle 18 conforms native support for Interval data types, and the value of an Interval column is output in a format compliant with the ISO 8601 standard. In order to support Interval data types in Oracle12.2 it is necessary to use the SQL extract function to decompose the interval value into discrete components and then construct a string that mimics the native support provided by Oracle 18.&lt;/p&gt;

&lt;p&gt;Implementing the workarounds requires modifying the column list generated by GENERATE_STATEMENT to incorporate the appropriate workarounds. A case statement, based on the DATA_TYPE column,  is used to apply each workaround when generating the list of columns names. Conditional compilation is used to determine which workarounds are required in which database version. The result of implementing this logic is the query is now generates the “select list” required to export the table’s content as JSON, rather than a list of the columns in the table.&lt;/p&gt;

&lt;p&gt;The case statement required to generate the select list is shown below&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;case&lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;-- For some reason RAW columns have DATA_TYPE_OWNER set to the current schema and &lt;/span&gt;
   &lt;span class=&quot;c1&quot;&gt;-- the condition DATA_TYPE_OWNER is not NULL is requried to identify OBJECT types&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'RAW'&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;'&lt;/span&gt;
  &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IF&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NOT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;JSON_FEATURE_DETECTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLOB_SUPPORTED&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;THEN&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/*
  ** Pre 18.1 Some Scalar Data Types are not natively supported by JSON_ARRAY()
  */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'BINARY_DOUBLE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'BINARY_FLOAT'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TO_CHAR(&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;)'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LIKE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TIMESTAMP%WITH LOCAL TIME ZONE'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TO_CHAR(SYS_EXTRACT_UTC(&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;),&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;IYYY-MM-DD&quot;T&quot;HH24:MI:SS.FF9&quot;Z&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;)'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LIKE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'INTERVAL DAY% TO SECOND%'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; || extract(DAY FROM &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) || &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; ||
          &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; || case 
                     when extract(HOUR FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) &amp;lt;&amp;gt; 0 
                       then extract(HOUR FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) || &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;H&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; 
                   end 
                || case 
                     when extract(MINUTE FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) &amp;lt;&amp;gt; 0 
                     then extract(MINUTE FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) || &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; 
                   end
	            || case 
                     when extract(SECOND FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) &amp;lt;&amp;gt; 0 
                     then extract(SECOND FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) ||  &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;S&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;
                   end'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LIKE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'INTERVAL YEAR% TO MONTH%'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; || extract(YEAR FROM &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) || 
          &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt; || case 
                     when extract(MONTH FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) &amp;lt;&amp;gt; 0 
                       then extract(MONTH FROM  &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;) || &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;M&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;
                   end'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'NCHAR'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'NVARCHAR2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TO_CHAR(&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;)'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'NCLOB'&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'TO_CLOB(&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;)'&lt;/span&gt;
  &lt;span class=&quot;err&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;END&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/*
  ** Quick Fixes for datatypes not natively supported
  */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'XMLTYPE'&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;-- Can be owned by SYS or PUBLIC&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'case 
            when &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot; is NULL 
              then NULL 
              else XMLSERIALIZE(CONTENT &quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt;  &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot; as CLOB) 
            end'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ROWID'&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'UROWID'&lt;/span&gt;
     &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ROWIDTOCHAR(&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;)'&lt;/span&gt;
  &lt;span class=&quot;cm&quot;&gt;/*
  ** Comment outunsupported scalar data types and Object types
  */&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'LONG'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'LONG RAW'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'BFILE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'BLOB'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;. Unsupported data type [&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;when&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE_OWNER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;then&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;. Unsupported object type [&quot;'&lt;/span&gt; 
               &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE_OWNER&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;.&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DATA_TYPE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;]&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;
    &lt;span class=&quot;s1&quot;&gt;'&quot;'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;COLUMN_NAME&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;'&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above implementation assumes that applying the patch to enable CLOB support to Database 12.2 will also extend the list of data types supported natively by JSON_ARRAY in 12.2. If this is not the case further ‘DUCK TYPING’ may be necessary to determine which data types are supported natively in which database release in order to generate a correctly optimized column list.&lt;/p&gt;

&lt;p&gt;After implementing these changes  an EXPORT_SCHEMA operation on the OE schema succeeds. However a closer examination of the JSON document generated shows that the export failed to export all of the data in the schema The following table shows the results of the export operation&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Table Name&lt;/th&gt;
      &lt;th&gt;Results&lt;/th&gt;
      &lt;th&gt;Cause&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;INVENTORIES&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ORDERS&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ORDER_ITEMS&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PROMOTIONS&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PROCUCT_INFORMATION&lt;/td&gt;
      &lt;td&gt;✔️&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CUSTOMERS&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;Unsupported OBJECT Type Column&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;WAREHOUSES&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;Unsupported OBJECT Type Column&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CATEOGORIES&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;Skipped OBJECT Table&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;PURCHASEORDER&lt;/td&gt;
      &lt;td&gt;❌&lt;/td&gt;
      &lt;td&gt;Skipped OBJECT (XMLTYPE) Table&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;At this point EXPORT_SCHEMA is capable of handling most of the common scalar data types supported by the Oracle Database. The &lt;a href=&quot;/json/export/import/oracle/2018/06/17/BFILE-and-BLOB.html&quot;&gt;next&lt;/a&gt; post will explain how in-line PL/SQL procedures can be used to enable support for BIFLE and BLOB data types.&lt;/p&gt;</content><author><name></name></author><summary type="html">The Devil’s in the Details: Unsupported scalar data types In the previous post, the first attempt to run EXPORT_SCHEMA on the SH schema failed due to fact one of the tables contained a BLOB column. Fortunately, in that case of the SH schema, the BLOB column was in a table that needed to be excluded from the export operation, and all remaining tables in the SH schema use data types that are directly supported by the JSON_ARRAY operator However that incident was simply a precursor for the problems that are encountered when attempting to process the OE schema. SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('OE') from dual; ERROR: ORA-40654: Input to JSON generation function has unsupported data type. ORA-06512: at &quot;SYSTEM.JSON_EXPORT&quot;, line 197 In order to determine the set of data types supported by JSON_ARRAY in each database release a simple script was developed. The script creates a table containing one column for each possible scalar data type, insert 1 row of data into the tables and then attempts to perform a JSON_ARRAY operation on each column. The script, DATA_TYPE_TEST.sql can be found on GitHub in the project’s test directory. The results of running this script on 12.2 and 18.1 are documented in the following table. Data Type 12cR2 18S SQL Workaround CHAR (BYTE semantics)`` ✔️ ✔️   CHAR (CHAR semantics :heavy_check_mark: ✔️   VARCHAR2 (Byte Semantics) ✔️ ✔️   VARCHAR2 (Char Semantics) ✔️ ✔️   NCHAR ❗️ ✔️   NVARCHAR ❗️ ✔️   NUMBER ✔️ ✔️   NUMBER (Precision) ✔️ ✔️   NUMBER (Precision,Scale) ✔️ ✔️   BINARY FLOAT ❌ ✔️ TO_CHAR BINARY DECIMAL ❌ ✔️ TO_CHAR DATE ✔️ ✔️   TIMESTAMP ✔️ ✔️   TIMESTAMP WITH TIME ZONE ✔️ ✔️   TIMESTAMP WITH LOCAL TIME ZONE ❌ ✔️ SYS_EXTRACT_UTC INTERVAL YEAR TO MONTH ❌ ✔️ Custom SQL INTERVAL DAY TO SECOND ❌ ✔️ Custom SQL RAW ✔️ ✔️   ROWID ❌ ❌ ROWIDTOCHAR UROWID ❌ ❌ ROWIDTOCHAR BLOB ❗️ ✔️   CLOB ✔️ ✔️   NCLOB ❗️ ✔️   BFILE ❌ ❌   XMLTYPE ❌ ❌ XMLSEQUENCE LONG ❌ ❌   LONG RAW ❌ ❌   The script also demonstrates potential solutions for some of the data types that are not supported natively. ❗️ The JSON_ARRAY documentation for Oracle 12.2 clearly states that BLOB is not a supported data type. However, as is shown by the DATA_TYPE_TESTS.sql script, if a BLOB is passed to the JSON_ARRAY operator it does not raise the expected “ORA-40654: Input to JSON generation function has unsupported data type” exception. Rather it mistakenly assumes that the BLOB column contains textual data and attempts to process the content as text. If the BLOB contains binary data this will typically result in the exception “ORA-40474: invalid UTF-8 byte sequence in JSON data” being raised by JSON_ARRAY. In Oracle 18c the BLOB data type is supported by JSON_ARRAY. When a BLOB is passed to JSON_ARRAY a HEXBINARY encoded representation of the BLOB’s content is returned by the JSON_ARRAY operator. ❗️There is a similar issue with the way in which JSON_ARRAY handles of NCHAR, NVARCHAR2 and NCLOB in database 12.2 which can also result in exception “ORA-40474: invalid UTF-8 byte sequence in JSON data” being raised. A possible workaround for this, at least in databases configured to use ‘AL32UTF8’ as the database character set, is to apply the TO_CHAR or TO_CLOB operator to the column before passing it to the JSON_ARRAY operator. Note that it is likely that this workaround will fail in non AL32UTF8 environments. Running the DATA_TYPE_TESTS.sql script in Oracle 18 conforms native support for Interval data types, and the value of an Interval column is output in a format compliant with the ISO 8601 standard. In order to support Interval data types in Oracle12.2 it is necessary to use the SQL extract function to decompose the interval value into discrete components and then construct a string that mimics the native support provided by Oracle 18. Implementing the workarounds requires modifying the column list generated by GENERATE_STATEMENT to incorporate the appropriate workarounds. A case statement, based on the DATA_TYPE column, is used to apply each workaround when generating the list of columns names. Conditional compilation is used to determine which workarounds are required in which database version. The result of implementing this logic is the query is now generates the “select list” required to export the table’s content as JSON, rather than a list of the columns in the table. The case statement required to generate the select list is shown below case -- For some reason RAW columns have DATA_TYPE_OWNER set to the current schema and -- the condition DATA_TYPE_OWNER is not NULL is requried to identify OBJECT types when DATA_TYPE = 'RAW' then '&quot;' || COLUMN_NAME || '&quot;' $IF NOT JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN /* ** Pre 18.1 Some Scalar Data Types are not natively supported by JSON_ARRAY() */ when DATA_TYPE in ('BINARY_DOUBLE','BINARY_FLOAT') then 'TO_CHAR(&quot;' || COLUMN_NAME || '&quot;)' when DATA_TYPE LIKE 'TIMESTAMP%WITH LOCAL TIME ZONE' then 'TO_CHAR(SYS_EXTRACT_UTC(&quot;' || COLUMN_NAME || '&quot;),''IYYY-MM-DD&quot;T&quot;HH24:MI:SS.FF9&quot;Z&quot;'')' when DATA_TYPE LIKE 'INTERVAL DAY% TO SECOND%' then '''P'' || extract(DAY FROM &quot;' || COLUMN_NAME || '&quot;) || ''D'' || ''T'' || case when extract(HOUR FROM &quot;' || COLUMN_NAME || '&quot;) &amp;lt;&amp;gt; 0 then extract(HOUR FROM &quot;' || COLUMN_NAME || '&quot;) || ''H'' end || case when extract(MINUTE FROM &quot;' || COLUMN_NAME || '&quot;) &amp;lt;&amp;gt; 0 then extract(MINUTE FROM &quot;' || COLUMN_NAME || '&quot;) || ''M'' end || case when extract(SECOND FROM &quot;' || COLUMN_NAME || '&quot;) &amp;lt;&amp;gt; 0 then extract(SECOND FROM &quot;' || COLUMN_NAME || '&quot;) || ''S'' end' when DATA_TYPE LIKE 'INTERVAL YEAR% TO MONTH%' then '''P'' || extract(YEAR FROM &quot;' || COLUMN_NAME || '&quot;) || ''Y'' || case when extract(MONTH FROM &quot;' || COLUMN_NAME || '&quot;) &amp;lt;&amp;gt; 0 then extract(MONTH FROM &quot;' || COLUMN_NAME || '&quot;) || ''M'' end' when DATA_TYPE in ('NCHAR','NVARCHAR2') then 'TO_CHAR(&quot;' || COLUMN_NAME || '&quot;)' when DATA_TYPE = 'NCLOB' then 'TO_CLOB(&quot;' || COLUMN_NAME || '&quot;)' $END /* ** Quick Fixes for datatypes not natively supported */ when DATA_TYPE = 'XMLTYPE' -- Can be owned by SYS or PUBLIC then 'case when &quot;' || COLUMN_NAME || '&quot; is NULL then NULL else XMLSERIALIZE(CONTENT &quot;' || COLUMN_NAME || '&quot; as CLOB) end' when DATA_TYPE = 'ROWID' or DATA_TYPE = 'UROWID' then 'ROWIDTOCHAR(&quot;' || COLUMN_NAME || '&quot;)' /* ** Comment outunsupported scalar data types and Object types */ when DATA_TYPE in ('LONG','LONG RAW','BFILE','BLOB') then '''&quot;' || COLUMN_NAME || '&quot;. Unsupported data type [&quot;' || DATA_TYPE || '&quot;]''' when DATA_TYPE_OWNER is not NULL then '''&quot;' || COLUMN_NAME || '&quot;. Unsupported object type [&quot;' || DATA_TYPE_OWNER || '&quot;.&quot;' || DATA_TYPE || '&quot;]''' else '&quot;' || COLUMN_NAME || '&quot;' The above implementation assumes that applying the patch to enable CLOB support to Database 12.2 will also extend the list of data types supported natively by JSON_ARRAY in 12.2. If this is not the case further ‘DUCK TYPING’ may be necessary to determine which data types are supported natively in which database release in order to generate a correctly optimized column list. After implementing these changes an EXPORT_SCHEMA operation on the OE schema succeeds. However a closer examination of the JSON document generated shows that the export failed to export all of the data in the schema The following table shows the results of the export operation Table Name Results Cause INVENTORIES ✔️   ORDERS ✔️   ORDER_ITEMS ✔️   PROMOTIONS ✔️   PROCUCT_INFORMATION ✔️   CUSTOMERS ❌ Unsupported OBJECT Type Column WAREHOUSES ❌ Unsupported OBJECT Type Column CATEOGORIES ❌ Skipped OBJECT Table PURCHASEORDER ❌ Skipped OBJECT (XMLTYPE) Table At this point EXPORT_SCHEMA is capable of handling most of the common scalar data types supported by the Oracle Database. The next post will explain how in-line PL/SQL procedures can be used to enable support for BIFLE and BLOB data types.</summary></entry><entry><title type="html">JSON_EXPORT</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/15/JSON_EXPORT.html" rel="alternate" type="text/html" title="JSON_EXPORT" /><published>2018-06-15T18:00:00-07:00</published><updated>2018-06-15T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/15/JSON_EXPORT</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/15/JSON_EXPORT.html">&lt;h2 id=&quot;a-plsql-package-for-exporting-an-oracle-schema-as-json&quot;&gt;A PL/SQL package for exporting an Oracle Schema as JSON.&lt;/h2&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/14/Introduction.html&quot;&gt;previous&lt;/a&gt; post in this blog showed how to generate a JSON representation of the contents of an Oracle database schema. This post outlines the basics of a PL/SQL package that can automate the process of exporting an entire database schema, or a subset of a database schema as a JSON document.&lt;/p&gt;

&lt;p&gt;The previous post mentioned that in (an unpatched) 12cR2 database, JSON operators cannot  generate documents larger that  32K, and that in Oracle 18 this limit no longer applies. In an attempt to avoid the 32K limit on JSON generation in  Oracle 12.2 , the PL/SQL package will use PL/SQL conditional compilation to bypass the use of JSON_QUERY and JSON_ARRAYAGG in databases where CLOB is not a supported return type.&lt;/p&gt;

&lt;p&gt;Conditional compilation requires a PL/SQL package is required that provides static constants that accurately describe the state of the database. In this project the package JSON_FEATURE_DETECTION exposes constants TREAT_AS_JSON_SUPPORTED, CLOB_SUPPORED and EXTENDED_STRING_SUPPORTED. TREAT_AS_JSON_SUPPORTED is set TRUE if the database supports the TREAT( AS JSON) feature. Treat as JSON is supported starting with Oracle 18. CLOB_SUPPORTED will be set TRUE in all 18c databases as well as patched 12.2 databases.  EXTENDED_STRING_SUPPORTED will be set true in database where extended data type support has been enabled&lt;/p&gt;

&lt;p&gt;The JSON_FEATURE_DETECTION  package is generated dynamically by a PL/SQL block that uses ‘Duck Typing’ to detect which features are supported. Duck Typing, for this not familiar with the term, is based on the assumption that if it “walks like a duck”, “swims like a duck” and “quacks like a duck” that is is “probably a member of Anatidae family”, eg a DUCK. In this case the lack of each feature is detected by executing a SQL statement that makes use of the feature and catching the exception thrown if the feature in question is not supported by the database.&lt;/p&gt;

&lt;p&gt;At first glance creating a PL/SQL package that will generate the required SQL seems quite simple. All that is needed is a list of the tables, along with the set of columns for each table. This appears to be a perfect fit for the LISTAGG operator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select TABLE_NAME, LISTAGG(COLUMN_NAME,',') WITHIN GROUP (ORDER BY COLUMN_ID) COLUMN_LIST
  from ALL_TAB_COLUMNS
 where OWNER = 'HR'
 group by TABLE_NAME
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;TABLE_NAME&lt;/th&gt;
      &lt;th&gt;COLUMN_LIST&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;COUNTRIES&lt;/td&gt;
      &lt;td&gt;COUNTRY_ID,COUNTRY_NAME,REGION_ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DEPARTMENTS&lt;/td&gt;
      &lt;td&gt;DEPARTMENT_ID,DEPARTMENT_NAME,MANAGER_ID,LOCATION_ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;EMPLOYEES&lt;/td&gt;
      &lt;td&gt;EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;EMP_DETAILS_VIEW&lt;/td&gt;
      &lt;td&gt;EMPLOYEE_ID,JOB_ID,MANAGER_ID,DEPARTMENT_ID,LOCATION_ID,COUNTRY_ID,FIRST_NAME,LAST_NAME,SALARY,COMMISSION_PCT,DEPARTMENT_NAME,JOB_TITLE,CITY,STATE_PROVINCE,COUNTRY_NAME,REGION_NAME&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;JOBS&lt;/td&gt;
      &lt;td&gt;JOB_ID,JOB_TITLE,MIN_SALARY,MAX_SALARY&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;JOB_HISTORY&lt;/td&gt;
      &lt;td&gt;EMPLOYEE_ID,START_DATE,END_DATE,JOB_ID,DEPARTMENT_ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LOCATIONS&lt;/td&gt;
      &lt;td&gt;LOCATION_ID,STREET_ADDRESS,POSTAL_CODE,CITY,STATE_PROVINCE,COUNTRY_ID&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;REGIONS&lt;/td&gt;
      &lt;td&gt;REGION_ID,REGION_NAME&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Unfortunately if we do the math, in a a database that supports 128 character identifiers, the results of this query could easily exceed the maximum size supported by LISTAGG. Consequently the column list is captured as a collection, using a CAST(COLLECT &lt;em&gt;COLUMN_NAME&lt;/em&gt;) AS &lt;em&gt;TABLE_TYPE_OBECT&lt;/em&gt;) operation. The table is then converted into a list using a PL/SQL function that returns a CLOB. The type T_VC4000_TABLE is defined for this purpose.&lt;/p&gt;

&lt;p&gt;The JSON_EXPORT package is split into two main areas of functionality. The first, encapsulated by the function GENERATE_STATEMENT, generates the SQL statement that will be used to export the contents of the schema as JSON. The second, encapsulated in the function EXECUTE_STAETMENT executes the generated statement and returns the resulting JSON document.&lt;/p&gt;

&lt;p&gt;The initial version of the GENERATE_STATEMENT procedure is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;procedure GENERATE_STATEMENT(P_SCHEMA VARCHAR2)
as
  V_SQL_FRAGMENT  VARCHAR2(32767);

  $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN
	V_RETURN_TYPE VARCHAR2(32) := 'CLOB';
  $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
	V_RETURN_TYPE VARCHAR2(32):= 'VARCHAR2(32767)';
  $ELSE
    V_RETURN_TYPE VARCHAR2(32):= 'VARCHAR2(4000)';
  $END  

  cursor getTableMetadata
  is
  select aat.owner
        ,aat.table_name
		,cast(collect('&quot;' || COLUMN_NAME || '&quot;' ORDER BY COLUMN_ID) 
              as T_VC4000_TABLE) COLUMN_LIST
    from ALL_ALL_TABLES aat
	     inner join ALL_TAB_COLUMNS atc
		         on atc.OWNER = aat.OWNER
		        and atc.TABLE_NAME = aat.TABLE_NAME
   where aat.OWNER = P_SCHEMA
   group by aat.OWNER, aat.TABLE_NAME;
    
  V_FIRST_ROW BOOLEAN := TRUE;
begin

  DBMS_LOB.CREATETEMPORARY(SQL_STATEMENT,TRUE,DBMS_LOB.CALL);
  V_SQL_FRAGMENT := 'select JSON_OBJECT(''data'' value JSON_OBJECT (' || C_NEWLINE;
  DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);

  for t in getTableMetadata loop  
	V_SQL_FRAGMENT := C_SINGLE_QUOTE || t.TABLE_NAME || C_SINGLE_QUOTE 
	               || ' value ( select JSON_ARRAYAGG(JSON_ARRAY(';
    if (NOT V_FIRST_ROW) then
      V_SQL_FRAGMENT := ',' || V_SQL_FRAGMENT;
	end if;
	V_FIRST_ROW := FALSE;
	DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);
	DBMS_LOB.APPEND(SQL_STATEMENT,TABLE_TO_LIST(t.COLUMN_LIST));
    V_SQL_FRAGMENT := ' NULL ON NULL returning ' || V_RETURN_TYPE || ') returning '
                   || V_RETURN_TYPE || ') FROM &quot;' 
                   || t.OWNER || '&quot;.&quot;' || t.TABLE_NAME || '&quot;)' || C_NEWLINE;
	DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);
  end loop;

  V_SQL_FRAGMENT := '             returning ' || V_RETURN_TYPE || C_NEWLINE
                 || '           )' || C_NEWLINE
                 || '         returning ' || V_RETURN_TYPE || C_NEWLINE
                 || '       )' || C_NEWLINE
                 || '  from DUAL';
  DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It basically queries the data dictionary tables ALL_TAB_COLUMNS and ALL_ALL_TABLES to generate the list of columns for each table in the schema and then iterates over the result set constructing a JSON_ARRAYAGG sub-query for each table. Note how it uses conditional compilation to determine what return type will be used with the SQL operators. The generated SQL is written into the CLOB variable SQL_STATEMENT.&lt;/p&gt;

&lt;p&gt;The initial version of the EXPORT_SCHEMA function is shown below&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;function EXPORT_SCHEMA(P_SCHEMA VARCHAR2)
return CLOB
as
  V_JSON_DOCUMENT CLOB;
  V_CURSOR        SYS_REFCURSOR;
begin
  GENERATE_STATEMENT(P_SCHEMA);
  OPEN V_CURSOR FOR SQL_STATEMENT;
  FETCH V_CURSOR INTO V_JSON_DOCUMENT;
  CLOSE V_CURSOR;
  return V_JSON_DOCUMENT;
end;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This function takes the output of GENERATE_STATEMENT, and executes it using a cursor. Since the generated statement only returns a single row the function  simply opens the cursor, fetches the row  and closes the cursor.&lt;/p&gt;

&lt;p&gt;The JSON export file can now be generated by simply calling the EXPORT_SCHEMA function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select JSON_EXPORT.EXPORT_SCHEMA('HR') from dual;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{
   &quot;data&quot; : {
      &quot;JOBS&quot; : [[&quot;AD_PRES&quot;,&quot;President&quot;,20080,40000]
               ,[&quot;AD_VP&quot;,&quot;Administration Vice President&quot;,15000,30000]
               ,....
               ,[&quot;PR_REP&quot;,&quot;Public Relations Representative&quot;,4500,10500]
               ],
      &quot;REGIONS&quot; : [[1,&quot;Europe&quot;],[2,&quot;Americas&quot;],[3,&quot;Asia&quot;],[4,&quot;Middle East and Africa&quot;]],
      &quot;COUNTRIES&quot; : [[...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After invoking the EXPORT_SCHEMA method, the method DUMP_SQL_STATEMENT can be used to retrieve the SQL generated by the GENERTE_STATEMENT method.&lt;/p&gt;

&lt;p&gt;The next challenge is what happens with a larger dataset, such as the ‘SH’ schema. Invoking EXPORT_SCHEMA for the ‘SH’ schema results in the following error&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select JSON_EXPORT.EXPORT_SCHEMA('SH') from dual;
&lt;/code&gt;&lt;/pre&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ORA-40654: Input to JSON generation function has unsupported data type.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Looking at the data types used by the tables in the SH schema, it appears that there are two tables that contain ROWID columns. ROWID is one a number of data types not currently supported by the JSON generation functions, Fortunately in this case these tables are not  schema data tables, but are secondary tables, used by the Oracle Text index and consequently their content should not be included in the export file.&lt;/p&gt;

&lt;p&gt;To prevent unwanted tables from being included in the output file, the SQL used by GENERATE_TABLES is modified to exclude the following types of table: INVALID TABLES, DROPPED TABLES, TEMORARY_TABLES, EXTERNAL_TABLES, SECONDARY_TABLES, NESTED_TABLES and IOT_OVERFLOW tables. OBJECT tables are also excluded for the moment, as they will require special handling. After applying the filter conditions the select statement used by GENERATE_TABLES is as follows&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt; select aat.owner
        ,aat.table_name
		,cast(collect('&quot;' || COLUMN_NAME || '&quot;' ORDER BY COLUMN_ID) as T_VC4000_TABLE) 
		 COLUMN_LIST
    from ALL_ALL_TABLES aat
	     inner join ALL_TAB_COLUMNS atc
		         on atc.OWNER = aat.OWNER
		        and atc.TABLE_NAME = aat.TABLE_NAME
   where aat.STATUS = 'VALID'
     and aat.DROPPED = 'NO'
	 and aat.TEMPORARY = 'N'
     and aat.EXTERNAL = 'NO'
	 and aat.NESTED = 'NO'
	 and aat.SECONDARY = 'N'
	 and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT')
	 and aat.TABLE_TYPE is NULL
	 and aat.OWNER = P_SCHEMA
   group by aat.OWNER, aat.TABLE_NAME;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After modifying the GENERATE_STATEMENT method, invoking EXPORT_SCHEMA on the SH schema in an unpatched 12.2 results in&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ORA-40459: output value too large (actual: 32801, maximum: 32767)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since generating a JSON representation of the SH schema results in a document much larger than 32K. One way of avoiding this problem is to limit the number of rows processed for each table in the schema. This can done by making it possible set a row limit and then using row limit value in a WHERE clause that restricts the number of rows passed to the JSON_ARRAYAGG operations. Limiting the number of rows processed by the EXPORT_SCHEMA function somewhat defeats the purpose of an export utility. Clearly an alternative solution is required in databases where CLOB is not a supported return type for the JSON operators.&lt;/p&gt;

&lt;p&gt;One way of approaching this is to manually print the output the JSON_ARRAY operations directly into a CLOB. In general, using string concatenation to generate JSON is a really bad idea. In particular there are a lot of nuances around correctly handling special characters in text values, and it is very easy to make mistakes that result in the creation of invalid JSON documents.  However in this case, string concatenation is an acceptable approach, since the all string conversions are handled by the JSON_ARRAY operator and the results can safely be combined to form the desired output.&lt;/p&gt;

&lt;p&gt;The 12.2 implementation of GENERATE_STATEMENT method returns a SQL statement for each table, rather than one statement for the entire schema. This allow the results for each table to inserted as a distinct key in the main document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;declare
    V_SQL_STATEMENT CLOB;
begin
  EXPORT_METADATA_CACHE := T_EXPORT_METADATA_TABLE();
  for t in getTableMetadata loop  
    EXPORT_METADATA_CACHE.extend();
    EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).OWNER := t.OWNER;
    EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).TABLE_NAME := t.TABLE_NAME;
    DBMS_LOB.CREATETEMPORARY(V_SQL_STATEMENT,TRUE,DBMS_LOB.CALL);
    V_SQL_FRAGMENT := 'select JSON_ARRAY(';
    DBMS_LOB.WRITEAPPEND(V_SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);
    DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(t.COLUMN_LIST));
    V_SQL_FRAGMENT := ' NULL on NULL returning '|| V_RETURN_TYPE || ') FROM &quot;' 
                   || t.OWNER || '&quot;.&quot;' || t.TABLE_NAME || '&quot; ';
    if (ROW_LIMIT &amp;gt; -1) then
	  V_SQL_FRAGMENT := V_SQL_FRAGMENT || 'WHERE ROWNUM &amp;lt; ' || ROW_LIMIT;
	end if;
    DBMS_LOB.WRITEAPPEND(V_SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT);
    EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).SQL_STATEMENT := V_SQL_STATEMENT;	
  end loop;
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The 12.2 implementation of EXPORT_SCHEMA starts by printing the outer  &lt;em&gt;{ “data” :&lt;/em&gt;  to the CLOB. Next it loops over the set of SQL statements produced by GENERATE_STATEMENT. For each statement it  appends a key based on the table name whose value is an array. It then executes the SQL for that table and appends the rows returned by the query as the members of the array. Conditional compilation, based on CLOB_SUPPORTED, is used in both implementations to determine which path is executed.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;function EXPORT_SCHEMA(P_SCHEMA VARCHAR2)
return CLOB
as
  V_JSON_DOCUMENT CLOB;
  V_CURSOR        SYS_REFCURSOR;

  V_JSON_FRAGMENT VARCHAR2(4000);

  V_FIRST_TABLE   BOOLEAN := TRUE;
  V_FIRST_ITEM    BOOLEAN := TRUE;

  $IF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
  V_JSON_ARRAY VARCHAR2(32767);
  $ELSE
  V_JSON_ARRAY VARCHAR2(4000);
  $END  

begin
  DBMS_LOB.CREATETEMPORARY(V_JSON_DOCUMENT,TRUE,DBMS_LOB.CALL);
  V_JSON_FRAGMENT := '{&quot;data&quot;:{';
  DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,length(V_JSON_FRAGMENT),V_JSON_FRAGMENT);
  GENERATE_STATEMENT(P_SCHEMA);
  for i in 1 .. EXPORT_METADATA_CACHE.count loop
    V_JSON_FRAGMENT := '&quot;' || EXPORT_METADATA_CACHE(i).table_name || '&quot;:[';
	if (not V_FIRST_TABLE) then 
  	  V_JSON_FRAGMENT := ',' || V_JSON_FRAGMENT;
	end if;
	V_FIRST_TABLE := false;
	DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,length(V_JSON_FRAGMENT),V_JSON_FRAGMENT);
    V_FIRST_ITEM := TRUE;
    OPEN V_CURSOR for EXPORT_METADATA_CACHE(i).SQL_STATEMENT;
	loop
	  FETCH V_CURSOR into V_JSON_ARRAY;
	  EXIT WHEN V_CURSOR%notfound;	  
	  if (NOT V_FIRST_ITEM) then
    	DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,',');
	  end if;
 	  V_FIRST_ITEM := FALSE;
      DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,LENGTH(V_JSON_ARRAY),V_JSON_ARRAY);
	end loop;
	CLOSE V_CURSOR;
    DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,']');
  end loop;
  DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,'}');
  DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,'}');
  return V_JSON_DOCUMENT;
end;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The EXPORT_SCHEMA function can now successfully return the full content of the both the HR and SH schemas, even in a 12.2 database where the JSON operators do support CLOB.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('HR') from dual
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{&quot;data&quot;:{&quot;JOBS&quot;:[[&quot;AD_PRES&quot;,&quot;President&quot;,20080,40000],[&quot;AD_VP&quot;,&quot;Administration Vic
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('SH') from dual;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{&quot;data&quot;:{&quot;COSTS&quot;:[],&quot;SALES&quot;:[[13,987,&quot;1998-01-10T00:00:00&quot;,3,999,1,1232.16],[13,1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQl&quot;&gt;SQL&amp;gt; select DBMS_LOB.GETLENGTH(JSON_EXPORT.EXPORT_SCHEMA('SH')) from dual;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;58582866
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As can be seen the size of the document generated when processing the SH schema is almost 60MB.  The IS JSON condition can be used to verify that the generated document is valid JSON.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SQL&amp;gt; select 1 &quot;VALID JSON&quot; from dual where JSON_EXPORT.EXPORT_SCHEMA('SH') is JSON;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;VALID JSON
----------
         1
1 row selected.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note this statement would return no rows if the generated document was not valid JSON.&lt;/p&gt;

&lt;p&gt;A quick and the dirty was to output the generated document to a file is to use a simple SQL*PLUS script which spools the results to a file.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;DEF SCHEMA_NAME = &amp;amp;1
set lines 1024
column JSON format A1024
set feedback off
set heading off
set termout off
set verify off
set long 1000000000
set pages 0
set echo off
spool JSON/&amp;amp;SCHEMA_NAME..json
select JSON_EXPORT.EXPORT_SCHEMA('&amp;amp;SCHEMA_NAME') JSON from dual;
spool off
set echo on
set pages 100
set verify on
set termout on
set heading on
set feedback on
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Unfortunately SQL*PLUS inserts ‘hard’ end-of-line characters into the JSON after each block of 1024 characters, these need to be stripped out before the file can be processed as JSON.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;/json/export/import/oracle/2018/06/16/Unsupported-Scalar-Types.html&quot;&gt;next&lt;/a&gt; post will investigate what happens when we attempt to export the OE schema, which contains a much richer set of Oracle data types, than the HR and SH schemas.&lt;/p&gt;</content><author><name></name></author><summary type="html">A PL/SQL package for exporting an Oracle Schema as JSON. The previous post in this blog showed how to generate a JSON representation of the contents of an Oracle database schema. This post outlines the basics of a PL/SQL package that can automate the process of exporting an entire database schema, or a subset of a database schema as a JSON document. The previous post mentioned that in (an unpatched) 12cR2 database, JSON operators cannot generate documents larger that 32K, and that in Oracle 18 this limit no longer applies. In an attempt to avoid the 32K limit on JSON generation in Oracle 12.2 , the PL/SQL package will use PL/SQL conditional compilation to bypass the use of JSON_QUERY and JSON_ARRAYAGG in databases where CLOB is not a supported return type. Conditional compilation requires a PL/SQL package is required that provides static constants that accurately describe the state of the database. In this project the package JSON_FEATURE_DETECTION exposes constants TREAT_AS_JSON_SUPPORTED, CLOB_SUPPORED and EXTENDED_STRING_SUPPORTED. TREAT_AS_JSON_SUPPORTED is set TRUE if the database supports the TREAT( AS JSON) feature. Treat as JSON is supported starting with Oracle 18. CLOB_SUPPORTED will be set TRUE in all 18c databases as well as patched 12.2 databases. EXTENDED_STRING_SUPPORTED will be set true in database where extended data type support has been enabled The JSON_FEATURE_DETECTION package is generated dynamically by a PL/SQL block that uses ‘Duck Typing’ to detect which features are supported. Duck Typing, for this not familiar with the term, is based on the assumption that if it “walks like a duck”, “swims like a duck” and “quacks like a duck” that is is “probably a member of Anatidae family”, eg a DUCK. In this case the lack of each feature is detected by executing a SQL statement that makes use of the feature and catching the exception thrown if the feature in question is not supported by the database. At first glance creating a PL/SQL package that will generate the required SQL seems quite simple. All that is needed is a list of the tables, along with the set of columns for each table. This appears to be a perfect fit for the LISTAGG operator. select TABLE_NAME, LISTAGG(COLUMN_NAME,',') WITHIN GROUP (ORDER BY COLUMN_ID) COLUMN_LIST from ALL_TAB_COLUMNS where OWNER = 'HR' group by TABLE_NAME TABLE_NAME COLUMN_LIST COUNTRIES COUNTRY_ID,COUNTRY_NAME,REGION_ID DEPARTMENTS DEPARTMENT_ID,DEPARTMENT_NAME,MANAGER_ID,LOCATION_ID EMPLOYEES EMPLOYEE_ID,FIRST_NAME,LAST_NAME,EMAIL,PHONE_NUMBER,HIRE_DATE,JOB_ID,SALARY,COMMISSION_PCT,MANAGER_ID,DEPARTMENT_ID EMP_DETAILS_VIEW EMPLOYEE_ID,JOB_ID,MANAGER_ID,DEPARTMENT_ID,LOCATION_ID,COUNTRY_ID,FIRST_NAME,LAST_NAME,SALARY,COMMISSION_PCT,DEPARTMENT_NAME,JOB_TITLE,CITY,STATE_PROVINCE,COUNTRY_NAME,REGION_NAME JOBS JOB_ID,JOB_TITLE,MIN_SALARY,MAX_SALARY JOB_HISTORY EMPLOYEE_ID,START_DATE,END_DATE,JOB_ID,DEPARTMENT_ID LOCATIONS LOCATION_ID,STREET_ADDRESS,POSTAL_CODE,CITY,STATE_PROVINCE,COUNTRY_ID REGIONS REGION_ID,REGION_NAME Unfortunately if we do the math, in a a database that supports 128 character identifiers, the results of this query could easily exceed the maximum size supported by LISTAGG. Consequently the column list is captured as a collection, using a CAST(COLLECT COLUMN_NAME) AS TABLE_TYPE_OBECT) operation. The table is then converted into a list using a PL/SQL function that returns a CLOB. The type T_VC4000_TABLE is defined for this purpose. The JSON_EXPORT package is split into two main areas of functionality. The first, encapsulated by the function GENERATE_STATEMENT, generates the SQL statement that will be used to export the contents of the schema as JSON. The second, encapsulated in the function EXECUTE_STAETMENT executes the generated statement and returns the resulting JSON document. The initial version of the GENERATE_STATEMENT procedure is shown below procedure GENERATE_STATEMENT(P_SCHEMA VARCHAR2) as V_SQL_FRAGMENT VARCHAR2(32767); $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN V_RETURN_TYPE VARCHAR2(32) := 'CLOB'; $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN V_RETURN_TYPE VARCHAR2(32):= 'VARCHAR2(32767)'; $ELSE V_RETURN_TYPE VARCHAR2(32):= 'VARCHAR2(4000)'; $END cursor getTableMetadata is select aat.owner ,aat.table_name ,cast(collect('&quot;' || COLUMN_NAME || '&quot;' ORDER BY COLUMN_ID) as T_VC4000_TABLE) COLUMN_LIST from ALL_ALL_TABLES aat inner join ALL_TAB_COLUMNS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME where aat.OWNER = P_SCHEMA group by aat.OWNER, aat.TABLE_NAME; V_FIRST_ROW BOOLEAN := TRUE; begin DBMS_LOB.CREATETEMPORARY(SQL_STATEMENT,TRUE,DBMS_LOB.CALL); V_SQL_FRAGMENT := 'select JSON_OBJECT(''data'' value JSON_OBJECT (' || C_NEWLINE; DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); for t in getTableMetadata loop V_SQL_FRAGMENT := C_SINGLE_QUOTE || t.TABLE_NAME || C_SINGLE_QUOTE || ' value ( select JSON_ARRAYAGG(JSON_ARRAY('; if (NOT V_FIRST_ROW) then V_SQL_FRAGMENT := ',' || V_SQL_FRAGMENT; end if; V_FIRST_ROW := FALSE; DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); DBMS_LOB.APPEND(SQL_STATEMENT,TABLE_TO_LIST(t.COLUMN_LIST)); V_SQL_FRAGMENT := ' NULL ON NULL returning ' || V_RETURN_TYPE || ') returning ' || V_RETURN_TYPE || ') FROM &quot;' || t.OWNER || '&quot;.&quot;' || t.TABLE_NAME || '&quot;)' || C_NEWLINE; DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); end loop; V_SQL_FRAGMENT := ' returning ' || V_RETURN_TYPE || C_NEWLINE || ' )' || C_NEWLINE || ' returning ' || V_RETURN_TYPE || C_NEWLINE || ' )' || C_NEWLINE || ' from DUAL'; DBMS_LOB.WRITEAPPEND(SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); end; It basically queries the data dictionary tables ALL_TAB_COLUMNS and ALL_ALL_TABLES to generate the list of columns for each table in the schema and then iterates over the result set constructing a JSON_ARRAYAGG sub-query for each table. Note how it uses conditional compilation to determine what return type will be used with the SQL operators. The generated SQL is written into the CLOB variable SQL_STATEMENT. The initial version of the EXPORT_SCHEMA function is shown below function EXPORT_SCHEMA(P_SCHEMA VARCHAR2) return CLOB as V_JSON_DOCUMENT CLOB; V_CURSOR SYS_REFCURSOR; begin GENERATE_STATEMENT(P_SCHEMA); OPEN V_CURSOR FOR SQL_STATEMENT; FETCH V_CURSOR INTO V_JSON_DOCUMENT; CLOSE V_CURSOR; return V_JSON_DOCUMENT; end; This function takes the output of GENERATE_STATEMENT, and executes it using a cursor. Since the generated statement only returns a single row the function simply opens the cursor, fetches the row and closes the cursor. The JSON export file can now be generated by simply calling the EXPORT_SCHEMA function. select JSON_EXPORT.EXPORT_SCHEMA('HR') from dual; { &quot;data&quot; : { &quot;JOBS&quot; : [[&quot;AD_PRES&quot;,&quot;President&quot;,20080,40000] ,[&quot;AD_VP&quot;,&quot;Administration Vice President&quot;,15000,30000] ,.... ,[&quot;PR_REP&quot;,&quot;Public Relations Representative&quot;,4500,10500] ], &quot;REGIONS&quot; : [[1,&quot;Europe&quot;],[2,&quot;Americas&quot;],[3,&quot;Asia&quot;],[4,&quot;Middle East and Africa&quot;]], &quot;COUNTRIES&quot; : [[... After invoking the EXPORT_SCHEMA method, the method DUMP_SQL_STATEMENT can be used to retrieve the SQL generated by the GENERTE_STATEMENT method. The next challenge is what happens with a larger dataset, such as the ‘SH’ schema. Invoking EXPORT_SCHEMA for the ‘SH’ schema results in the following error select JSON_EXPORT.EXPORT_SCHEMA('SH') from dual; ORA-40654: Input to JSON generation function has unsupported data type. Looking at the data types used by the tables in the SH schema, it appears that there are two tables that contain ROWID columns. ROWID is one a number of data types not currently supported by the JSON generation functions, Fortunately in this case these tables are not schema data tables, but are secondary tables, used by the Oracle Text index and consequently their content should not be included in the export file. To prevent unwanted tables from being included in the output file, the SQL used by GENERATE_TABLES is modified to exclude the following types of table: INVALID TABLES, DROPPED TABLES, TEMORARY_TABLES, EXTERNAL_TABLES, SECONDARY_TABLES, NESTED_TABLES and IOT_OVERFLOW tables. OBJECT tables are also excluded for the moment, as they will require special handling. After applying the filter conditions the select statement used by GENERATE_TABLES is as follows select aat.owner ,aat.table_name ,cast(collect('&quot;' || COLUMN_NAME || '&quot;' ORDER BY COLUMN_ID) as T_VC4000_TABLE) COLUMN_LIST from ALL_ALL_TABLES aat inner join ALL_TAB_COLUMNS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME where aat.STATUS = 'VALID' and aat.DROPPED = 'NO' and aat.TEMPORARY = 'N' and aat.EXTERNAL = 'NO' and aat.NESTED = 'NO' and aat.SECONDARY = 'N' and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT') and aat.TABLE_TYPE is NULL and aat.OWNER = P_SCHEMA group by aat.OWNER, aat.TABLE_NAME; After modifying the GENERATE_STATEMENT method, invoking EXPORT_SCHEMA on the SH schema in an unpatched 12.2 results in ORA-40459: output value too large (actual: 32801, maximum: 32767) Since generating a JSON representation of the SH schema results in a document much larger than 32K. One way of avoiding this problem is to limit the number of rows processed for each table in the schema. This can done by making it possible set a row limit and then using row limit value in a WHERE clause that restricts the number of rows passed to the JSON_ARRAYAGG operations. Limiting the number of rows processed by the EXPORT_SCHEMA function somewhat defeats the purpose of an export utility. Clearly an alternative solution is required in databases where CLOB is not a supported return type for the JSON operators. One way of approaching this is to manually print the output the JSON_ARRAY operations directly into a CLOB. In general, using string concatenation to generate JSON is a really bad idea. In particular there are a lot of nuances around correctly handling special characters in text values, and it is very easy to make mistakes that result in the creation of invalid JSON documents. However in this case, string concatenation is an acceptable approach, since the all string conversions are handled by the JSON_ARRAY operator and the results can safely be combined to form the desired output. The 12.2 implementation of GENERATE_STATEMENT method returns a SQL statement for each table, rather than one statement for the entire schema. This allow the results for each table to inserted as a distinct key in the main document. declare V_SQL_STATEMENT CLOB; begin EXPORT_METADATA_CACHE := T_EXPORT_METADATA_TABLE(); for t in getTableMetadata loop EXPORT_METADATA_CACHE.extend(); EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).OWNER := t.OWNER; EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).TABLE_NAME := t.TABLE_NAME; DBMS_LOB.CREATETEMPORARY(V_SQL_STATEMENT,TRUE,DBMS_LOB.CALL); V_SQL_FRAGMENT := 'select JSON_ARRAY('; DBMS_LOB.WRITEAPPEND(V_SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(t.COLUMN_LIST)); V_SQL_FRAGMENT := ' NULL on NULL returning '|| V_RETURN_TYPE || ') FROM &quot;' || t.OWNER || '&quot;.&quot;' || t.TABLE_NAME || '&quot; '; if (ROW_LIMIT &amp;gt; -1) then V_SQL_FRAGMENT := V_SQL_FRAGMENT || 'WHERE ROWNUM &amp;lt; ' || ROW_LIMIT; end if; DBMS_LOB.WRITEAPPEND(V_SQL_STATEMENT,LENGTH(V_SQL_FRAGMENT),V_SQL_FRAGMENT); EXPORT_METADATA_CACHE(EXPORT_METADATA_CACHE.count).SQL_STATEMENT := V_SQL_STATEMENT; end loop; end; The 12.2 implementation of EXPORT_SCHEMA starts by printing the outer { “data” : to the CLOB. Next it loops over the set of SQL statements produced by GENERATE_STATEMENT. For each statement it appends a key based on the table name whose value is an array. It then executes the SQL for that table and appends the rows returned by the query as the members of the array. Conditional compilation, based on CLOB_SUPPORTED, is used in both implementations to determine which path is executed. function EXPORT_SCHEMA(P_SCHEMA VARCHAR2) return CLOB as V_JSON_DOCUMENT CLOB; V_CURSOR SYS_REFCURSOR; V_JSON_FRAGMENT VARCHAR2(4000); V_FIRST_TABLE BOOLEAN := TRUE; V_FIRST_ITEM BOOLEAN := TRUE; $IF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN V_JSON_ARRAY VARCHAR2(32767); $ELSE V_JSON_ARRAY VARCHAR2(4000); $END begin DBMS_LOB.CREATETEMPORARY(V_JSON_DOCUMENT,TRUE,DBMS_LOB.CALL); V_JSON_FRAGMENT := '{&quot;data&quot;:{'; DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,length(V_JSON_FRAGMENT),V_JSON_FRAGMENT); GENERATE_STATEMENT(P_SCHEMA); for i in 1 .. EXPORT_METADATA_CACHE.count loop V_JSON_FRAGMENT := '&quot;' || EXPORT_METADATA_CACHE(i).table_name || '&quot;:['; if (not V_FIRST_TABLE) then V_JSON_FRAGMENT := ',' || V_JSON_FRAGMENT; end if; V_FIRST_TABLE := false; DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,length(V_JSON_FRAGMENT),V_JSON_FRAGMENT); V_FIRST_ITEM := TRUE; OPEN V_CURSOR for EXPORT_METADATA_CACHE(i).SQL_STATEMENT; loop FETCH V_CURSOR into V_JSON_ARRAY; EXIT WHEN V_CURSOR%notfound; if (NOT V_FIRST_ITEM) then DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,','); end if; V_FIRST_ITEM := FALSE; DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,LENGTH(V_JSON_ARRAY),V_JSON_ARRAY); end loop; CLOSE V_CURSOR; DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,']'); end loop; DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,'}'); DBMS_LOB.WRITEAPPEND(V_JSON_DOCUMENT,1,'}'); return V_JSON_DOCUMENT; end; The EXPORT_SCHEMA function can now successfully return the full content of the both the HR and SH schemas, even in a 12.2 database where the JSON operators do support CLOB. SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('HR') from dual {&quot;data&quot;:{&quot;JOBS&quot;:[[&quot;AD_PRES&quot;,&quot;President&quot;,20080,40000],[&quot;AD_VP&quot;,&quot;Administration Vic SQL&amp;gt; select JSON_EXPORT.EXPORT_SCHEMA('SH') from dual; {&quot;data&quot;:{&quot;COSTS&quot;:[],&quot;SALES&quot;:[[13,987,&quot;1998-01-10T00:00:00&quot;,3,999,1,1232.16],[13,1 SQL&amp;gt; select DBMS_LOB.GETLENGTH(JSON_EXPORT.EXPORT_SCHEMA('SH')) from dual; 58582866 As can be seen the size of the document generated when processing the SH schema is almost 60MB. The IS JSON condition can be used to verify that the generated document is valid JSON. SQL&amp;gt; select 1 &quot;VALID JSON&quot; from dual where JSON_EXPORT.EXPORT_SCHEMA('SH') is JSON; VALID JSON ---------- 1 1 row selected. Note this statement would return no rows if the generated document was not valid JSON. A quick and the dirty was to output the generated document to a file is to use a simple SQL*PLUS script which spools the results to a file. DEF SCHEMA_NAME = &amp;amp;1 set lines 1024 column JSON format A1024 set feedback off set heading off set termout off set verify off set long 1000000000 set pages 0 set echo off spool JSON/&amp;amp;SCHEMA_NAME..json select JSON_EXPORT.EXPORT_SCHEMA('&amp;amp;SCHEMA_NAME') JSON from dual; spool off set echo on set pages 100 set verify on set termout on set heading on set feedback on Unfortunately SQL*PLUS inserts ‘hard’ end-of-line characters into the JSON after each block of 1024 characters, these need to be stripped out before the file can be processed as JSON. The next post will investigate what happens when we attempt to export the OE schema, which contains a much richer set of Oracle data types, than the HR and SH schemas.</summary></entry><entry><title type="html">Introduction</title><link href="http://localhost:4000/json/export/import/oracle/2018/06/14/Introduction.html" rel="alternate" type="text/html" title="Introduction" /><published>2018-06-14T18:00:00-07:00</published><updated>2018-06-14T18:00:00-07:00</updated><id>http://localhost:4000/json/export/import/oracle/2018/06/14/Introduction</id><content type="html" xml:base="http://localhost:4000/json/export/import/oracle/2018/06/14/Introduction.html">&lt;h2 id=&quot;an-introduction-to-generating-json-with-oracle&quot;&gt;An introduction to generating JSON with Oracle.&lt;/h2&gt;

&lt;p&gt;The JSON-Exchange project uses the JSON generation features of Oracle 12.2 and 18 to develop a simple JSON based replacement for the traditional Oracle EXP and IMP utilities. The basic concept is extremely simple, but as is often the case the devil is in the details.&lt;/p&gt;

&lt;p&gt;Oracle 12.2 introduced support for generating JSON directly from SQL. Of particular interest for this project are two operators, JSON_ARRAY and JSON_ARRAYAGG. JSON_ARRAY is used to generate a JSON array from each row returned by a SQL query, and JSON_ARRAYAGG is a aggregation operator that takes the set rows generated by a query and returns a single JSON array that contains one member for each row.&lt;/p&gt;

&lt;p&gt;The following queries  show how this works. You’ll need to have the Oracle sample schemas, available &lt;a href=&quot;https://github.com/oracle/db-sample-schemas&quot;&gt;here&lt;/a&gt;, installed to run this queries locally. Alternatively you can also run them on Oracle’s &lt;a href=&quot;https://livesql.oracle.com/apex/livesql/file/index.html&quot;&gt;LiveSQL&lt;/a&gt; site, assuming you have an Oracle Technology Account.&lt;/p&gt;

&lt;p&gt;Let’s start by looking at the content of the REGIONS table in the HR schema&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select REGION_ID, REGION_NAME
  from HR.REGIONS
/  
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;REGION_ID&lt;/th&gt;
      &lt;th&gt;REGION_NAME&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;Europe&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;Americas&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;Asia&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;Middle East and Africa&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;To obtain a JSON array for each row we simply modify the statement, adding the JSON_ARRAY operator. The  NULL on NULL option ensures that each JSON array contains the number of elements, regardless of whether or not any of the columns in that row are NULL.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select JSON_ARRAY(REGION_ID, REGION_NAME NULL ON NULL)
  from HR.REGIONS
/  
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;JSON_ARRAY(REGION_ID,REGION_NAME)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[1,”Europe”]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[2,”Americas”]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[3,”Asia”]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[4,”Middle East and Africa”]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As can be see each row is now represented as JSON ARRAY. We can can turn the set of rows into a single JSON document by adding the JSON_ARRAYAGG operator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select JSON_ARRAYAGG(JSON_ARRAY(REGION_ID, REGION_NAME))
  from HR.REGIONS
/  
&lt;/code&gt;&lt;/pre&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;JSON_ARRAYAGG(JSON_ARRAY(REGION_ID,REGION_NAME))&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;[[1,”Europe”],[2,”Americas”],[3,”Asia”],[4,”Middle East and Africa”]]&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;As you can see we have now have a relatively simple SQL statement that provides a JSON representation of the entire table.&lt;/p&gt;

&lt;p&gt;In order to export the entire schema we can use something slightly more complex.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;select JSON_OBJECT(
         'data' value 
         JSON_OBJECT(
           'REGIONS' value
           (select JSON_ARRAYAGG(JSON_ARRAY(REGION_ID, ... NULL ON NULL) 
                                 RETURNING VARCHAR2(32767) from HR.REGIONS)
          ,'JOBS' value
           (select JSON_ARRAYAGG(JSON_ARRAY(JOB_ID,JOB_TITLE, ... NULL ON NULL)
                                 RETURNING VARCHAR2(32767)) from HR.JOBS)
          ...
          RETURNING VARCHAR2(32767)
         )
         RETURNING VARCHAR2(32767)    
       )
from dual
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&quot;language-JSON&quot;&gt;{&quot;data&quot; : {
    &quot;REGIONS&quot; : [
        [1,&quot;Europe&quot;],
        [2,&quot;Americas&quot;],
        [3,&quot;Asia&quot;],
        [4,&quot;Middle East and Africa&quot;]
    ]
    ,&quot;JOBS&quot; : [
        [&quot;AD_PRES&quot;,&quot;President&quot;,20080,40000],
        [&quot;AD_VP&quot;,&quot;Administration Vice President&quot;,15000,30000],
        ...
        [&quot;HR_REP&quot;,&quot;Human Resources Representative&quot;,4000,9000],
        [&quot;PR_REP&quot;,&quot;Public Relations Representative&quot;,4500,10500]
    ]
    ....
}}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This statement, shown in abridged form, uses a separate select statement to generate a JSON_ARRAY for each table in the HR schema. The results of each statement are combined inside a “data” object that contains one key for each table in the schema. The key’s name is the table name, and the keys value is a JSON array containing the data for that table. Using this technique we generate a single JSON document for the entire HR schema.&lt;/p&gt;

&lt;p&gt;The default return type used by the JSON operators is VARCHAR2(4000), this puts a fairly significant limitation on the size of the JSON documents that can be generated. While VARCHAR2(4000) maybe  reasonable when representing the contents of a single row as a JSON array, it is of pretty limited usefulness when representing the contents of a table as an array or arrays.&lt;/p&gt;

&lt;p&gt;Fortunately all of tthe JSON operators support a returning clause that can be used to override the operator’s default return type. In Oracle 12.2, assuming that the database option MAX_STRING_SIZE = ‘EXTENDED’ has been enabled, the RETURNING clause can be used to specify that the generated JSON should be returned as a VARCHAR2(32767), making the operators far more useful. Database 18 supports CLOB as a return type, removing most practical limits on the size of the generated documents.  My understanding is that Oracle is currently in the process of preparing a patch for database 12.2 that will enable the use of CLOB a return type.&lt;/p&gt;

&lt;p&gt;Some of the arrays generated from the contents of the HR schema are larger than 4K, so the previous statement has been modified to specify that all the JSON operators should return VARCHAR2(32767). While this works for the “HR” schema, this technique will still fail in 12.2 when working with larger amounts of data, such as can be found in the “SH” schema.&lt;/p&gt;

&lt;p&gt;As can be seen from the above example, the SQL required to export an entire Oracle database schema as JSON is fairly straightforward. However for a schema with a large number of tables and columns the required statement quickly becomes quite large and hand coding such a statement becomes tedious and error prone. The &lt;a href=&quot;/json/export/import/oracle/2018/06/15/JSON_EXPORT.html&quot;&gt;next&lt;/a&gt; post will outline a PL/SQL package that uses Oracle’s Data Dictionary to automate this process.&lt;/p&gt;</content><author><name></name></author><summary type="html">An introduction to generating JSON with Oracle.</summary></entry></feed>