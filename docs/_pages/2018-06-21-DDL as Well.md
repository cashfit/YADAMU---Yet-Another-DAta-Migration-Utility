---
layout: post
title:  "Importing Objects, BLOBs and ANYDATA"
date:   2018-06-21 18:00:00-0700
categories: JSON Import Export Oracle
---

## JSON-IMPORT: Objects and other challenges

The previous post described a package for exporting Oracle database schema's as JSON documents. The next post will examine a package, JSON_IMPORT that is capable of taking output created by JSON_EXPORT and converting it back into relational format.

The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this

```JSON
{"data" : {
    "REGIONS" : [
        [1,"Europe"],
        [2,"Americas"],
        [3,"Asia"],
        [4,"Middle East and Africa"]
    ]
}
```

We can use JSON_TABLE to convert it back in a relational format

```SQL
select ji.*
  from MY_EXPORT_FILE,
       JSON_TABLE(
          JSON_EXPORT_DATA,
          '$.data."REGIONS"[*]'
          columns (
            REGION_ID     NUMBER PATH '$[0]'
           ,REGION_NAME VARCHAR2 PATH '$[1]'
		  )
		) JI
		
```

```SQL
 REGION_ID REGION_NAME
---------- --------------------------------
         1 Europe
         2 Americas
         3 Asia
         4 Middle East and Africa
```

To perform this transformation it  is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the  JSON_IMPORT utility. 

In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a "systemInformation" object will be added to each file. This object will contain the following information 

| Key             | Content                                                      |
| --------------- | ------------------------------------------------------------ |
| date            | Date the file was generated, ISO 8601 format, Zulu time      |
| schema          | The database schema that was the source for the export       |
| exportVersion   | The version of JSON_EXPORT used. Currently V1.0              |
| jsonFeatures    | The current settings in package JSON_FEATURE_DETECTION       |
| sessionUser     | The session user who performed the EXPORT_SCHEMA operation   |
| dbName          | The name of the database                                     |
| serverHostName  | The logical name of the server hosting the database          |
| databaseVersion | The release of the database the data came from               |
| nlsInformation  | An object containing the NLS parameters of the source database. |

In a 12.2 database the SQL used to generate the systemInformation object is shown below

```SQL
with 
function DATABASE_RELEASE return NUMBER deterministic
as
begin
  return DBMS_DB_VERSION.VERSION || '.' || DBMS_DB_VERSION.RELEASE;
end;
--
function JSON_FEATURES return VARCHAR2 deterministic
as
begin
  return JSON_OBJECT(
           'treatAsJSON'     value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED
       	  ,'CLOB'            value JSON_FEATURE_DETECTION.CLOB_SUPPORTED
          ,'extendedString'  value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED
		);
end;
function EXPORT_VERSION return NUMBER
as
begin
  return 1.0;
end;
select JSON_OBJECT(
           'systemInformation'
		   value JSON_OBJECT(
	              'date'            value SYS_EXTRACT_UTC(SYSTIMESTAMP)
	             ,'schema'          value 'HR'
		         ,'exportVersion'   value EXPORT_VERSION()
		         ,'jsonFeatures'    value JSON_QUERY(JSON_FEATURES(),'$')
	             ,'sessionUser'     value SYS_CONTEXT('USERENV','SESSION_USER')
		         ,'dbName'          value SYS_CONTEXT('USERENV','DB_NAME')
		         ,'serverHostName'  value SYS_CONTEXT('USERENV','SERVER_HOST')
		         ,'databaseVersion' value DATABASE_RELEASE()
		         ,'nlsInformation'  value JSON_OBJECTAGG(parameter, value)
	             )
		 ) SYSTEM_INFORMATION
    from NLS_DATABASE_PARAMETERS;
```

An abridged version of systemInformation object is shown below 

```JSON
{
	"systemInformation": {
		"date": "2018-06-21T04:42:44.523029",
		"schema": "HR",
		"exportVersion": 1,
		"jsonFeatures": {
			"treatAsJSON": false,
			"CLOB": false,
			"extendedString": true
		},
		"sessionUser": "SYSTEM",
		"dbName": "ORCL",
		"serverHostName": "localhost",
		"databaseVersion": 12.2,
		"nlsInformation": {
			"NLS_RDBMS_VERSION": "12.2.0.1.0",
			...
		}
	}
}
```

In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. 

A metadata object is added to the export file. This object provides the following information for each table included in the export file. 

| Key               | Contents                                                     |
| ----------------- | ------------------------------------------------------------ |
| owner             | The database schema containing the table                     |
| tableName         | The name of the table                                        |
| columnList        | The columns in the table                                     |
| dataTypeList      | The data types of the columns in the table                   |
| exportSelectList  | The select list used to export the data in the table         |
| columnPatternList | The column patterns needed to convert the JSON back into relational form |

Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object  is added to the GENERATE_STATEMENT module. 

The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below

```JSON
"metadata": {
  "REGIONS": {
     "owner": "HR",
     "tableName": "REGIONS",
	 "columns": "\"REGION_ID\",\"REGION_NAME\"",
	 "dataTypes": "\"NUMBER\",\"VARCHAR2\"",
	 "exportSelectList": "\"REGION_ID\",\"REGION_NAME\"",
	 "columnPatterns": "\"REGION_ID\" NUMBER PATH '$[0]',\"REGION_NAME\" VARCHAR2 PATH '$[1]'"
  },
  "JOBS" : {...
  },
  ...
}
```

The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. 

In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of "insert as select" statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below

```SQL
function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB)
return T_SQL_OPERATIONS_TAB
as
  V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB;
begin
  select OWNER
        ,TABLE_NAME
        ,'insert into "' || TABLE_NAME ||'"(' || EXPORT_SELECT_LIST || ')' || C_NEWLINE ||
         'select ' || SELECT_LIST || C_NEWLINE ||
         '  from JSON_TABLE(' || C_NEWLINE ||
         '         :JSON,' || C_NEWLINE ||
         '         ''$.data."' || TABLE_NAME || '"[*]''' || C_NEWLINE ||
         '         COLUMNS(' || C_NEWLINE ||  COLUMN_PATTERNS || C_NEWLINE || '))' 
        ,NULL
        ,NULL
        ,NULL
   	bulk collect into V_SQL_OPERATIONS
   	from JSON_TABLE(
            P_JSON_DUMP_FILE,
		   '$.metadata.*' ERROR ON ERROR
            COLUMNS (
               OWNER       VARCHAR2(128) PATH '$.owner'
  			, TABLE_NAME  VARCHAR2(128) PATH '$.tableName'
               $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN   
   			, EXPORT_SELECT_LIST              CLOB PATH '$.columns'
             , DATA_TYPES                      CLOB PATH '$.dataTypes'
             , COLUMN_PATTERNS                 CLOB PATH '$.columnPatterns'
		    , SELECT_LIST                     CLOB PATH '$.importSelectList'
             $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
			, EXPORT_SELECT_LIST  VARCHAR2(32767) PATH '$.columns'
			, DATA_TYPES          VARCHAR2(32767) PATH '$.dataTypes'
			, COLUMN_PATTERNS     VARCHAR2(32767) PATH '$.columnPatterns'
			, SELECT_LIST         VARCHAR2(32767) PATH '$.importSelectList'
			$ELSE
			, EXPORT_SELECT_LIST   VARCHAR2(4000) PATH '$.columns'
			, DATA_TYPES           VARCHAR2(4000) PATH '$.dataTypes'
			, COLUMN_PATTERNS      VARCHAR2(4000) PATH '$.columnPatterns'
			, SELECT_LIST          VARCHAR2(4000) PATH '$.importSelectList'
			$END
		  )
		);
  return V_SQL_OPERATIONS;
end;
```

In phase two procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below

```SQL
procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB,
                      P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT('USERENV','CURRENT_SCHEMA'))
as
  V_CURRENT_SCHEMA           CONSTANT VARCHAR2(128) := SYS_CONTEXT('USERENV','CURRENT_SCHEMA');
  V_START_TIME TIMESTAMP(6);
  V_END_TIME   TIMESTAMP(6);
begin
  SET_CURRENT_SCHEMA(P_TARGET_SCHEMA);
  
  SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE);
							
  for i in 1 .. SQL_OPERATIONS_TABLE.count loop
    begin
      V_START_TIME := SYSTIMESTAMP;
      execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE;  
 	  SQL_OPERATIONS_TABLE(i).RESULT := 'Operation completed succecssfully at ' 
 	                                 || SYS_EXTRACT_UTC(SYSTIMESTAMP) 
 	                                 || '. Processed ' || TO_CHAR(SQL%ROWCOUNT) 
 	                                 || ' rows. Elapsed time: ' 
 	                                 || (V_END_TIME - V_START_TIME) || '.';
	  commit;
	  SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS;
    exception
      when others then
	    SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack;
		SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR;
	end;
  end loop;
  SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
exception
  when OTHERS then
    SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
	RAISE;
end;
--
```

After the import has completed the contents of the statement cache can be interrogated to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. 





