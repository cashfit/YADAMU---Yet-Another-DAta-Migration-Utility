<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Introduction to JSON_IMPORT | JSON Exchange</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="Introduction to JSON_IMPORT" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="JSON_IMPORT: The Yin to JSON_EXPORT’s Yang The previous posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format. The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this {&quot;data&quot; : { &quot;REGIONS&quot; : [ [1,&quot;Europe&quot;], [2,&quot;Americas&quot;], [3,&quot;Asia&quot;], [4,&quot;Middle East and Africa&quot;] ] } We can use JSON_TABLE to convert it back in a relational format select ji.* from MY_EXPORT_FILE, JSON_TABLE( JSON_EXPORT_DATA, &#39;$.data.&quot;REGIONS&quot;[*]&#39; columns ( REGION_ID NUMBER PATH &#39;$[0]&#39; ,REGION_NAME VARCHAR2 PATH &#39;$[1]&#39; ) ) JI REGION_ID REGION_NAME ---------- -------------------------------- 1 Europe 2 Americas 3 Asia 4 Middle East and Africa To perform this transformation it is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the JSON_IMPORT utility. In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information Key Content date Date the file was generated, ISO 8601 format, Zulu time schema The database schema that was the source for the export exportVersion The version of JSON_EXPORT used. Currently V1.0 jsonFeatures The current settings in package JSON_FEATURE_DETECTION sessionUser The session user who performed the EXPORT_SCHEMA operation dbName The name of the database serverHostName The logical name of the server hosting the database databaseVersion The release of the database the data came from nlsInformation An object containing the NLS parameters of the source database. In a 12.2 database the SQL used to generate the systemInformation object is shown below with function DATABASE_RELEASE return NUMBER deterministic as begin return DBMS_DB_VERSION.VERSION || &#39;.&#39; || DBMS_DB_VERSION.RELEASE; end; -- function JSON_FEATURES return VARCHAR2 deterministic as begin return JSON_OBJECT( &#39;treatAsJSON&#39; value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED ,&#39;CLOB&#39; value JSON_FEATURE_DETECTION.CLOB_SUPPORTED ,&#39;extendedString&#39; value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED ); end; function EXPORT_VERSION return NUMBER as begin return 1.0; end; select JSON_OBJECT( &#39;systemInformation&#39; value JSON_OBJECT( &#39;date&#39; value SYS_EXTRACT_UTC(SYSTIMESTAMP) ,&#39;schema&#39; value &#39;HR&#39; ,&#39;exportVersion&#39; value EXPORT_VERSION() ,&#39;jsonFeatures&#39; value JSON_QUERY(JSON_FEATURES(),&#39;$&#39;) ,&#39;sessionUser&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SESSION_USER&#39;) ,&#39;dbName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;DB_NAME&#39;) ,&#39;serverHostName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SERVER_HOST&#39;) ,&#39;databaseVersion&#39; value DATABASE_RELEASE() ,&#39;nlsInformation&#39; value JSON_OBJECTAGG(parameter, value) ) ) SYSTEM_INFORMATION from NLS_DATABASE_PARAMETERS; An abridged version of systemInformation object is shown below { &quot;systemInformation&quot;: { &quot;date&quot;: &quot;2018-06-21T04:42:44.523029&quot;, &quot;schema&quot;: &quot;HR&quot;, &quot;exportVersion&quot;: 1, &quot;jsonFeatures&quot;: { &quot;treatAsJSON&quot;: false, &quot;CLOB&quot;: false, &quot;extendedString&quot;: true }, &quot;sessionUser&quot;: &quot;SYSTEM&quot;, &quot;dbName&quot;: &quot;ORCL&quot;, &quot;serverHostName&quot;: &quot;localhost&quot;, &quot;databaseVersion&quot;: 12.2, &quot;nlsInformation&quot;: { &quot;NLS_RDBMS_VERSION&quot;: &quot;12.2.0.1.0&quot;, ... } } } In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. A metadata object is added to the export file. This object provides the following information for each table included in the export file. Key Contents owner The database schema containing the table tableName The name of the table columnList The columns in the table dataTypeList The data types of the columns in the table exportSelectList The select list used to export the data in the table columnPatternList The column patterns needed to convert the JSON back into relational form Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object is added to the GENERATE_STATEMENT module. The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below &quot;metadata&quot;: { &quot;REGIONS&quot;: { &quot;owner&quot;: &quot;HR&quot;, &quot;tableName&quot;: &quot;REGIONS&quot;, &quot;columns&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;dataTypes&quot;: &quot;\&quot;NUMBER\&quot;,\&quot;VARCHAR2\&quot;&quot;, &quot;exportSelectList&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;columnPatterns&quot;: &quot;\&quot;REGION_ID\&quot; NUMBER PATH &#39;$[0]&#39;,\&quot;REGION_NAME\&quot; VARCHAR2 PATH &#39;$[1]&#39;&quot; }, &quot;JOBS&quot; : {... }, ... } The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB) return T_SQL_OPERATIONS_TAB as V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB; begin select OWNER ,TABLE_NAME ,&#39;insert into &quot;&#39; || TABLE_NAME ||&#39;&quot;(&#39; || EXPORT_SELECT_LIST || &#39;)&#39; || C_NEWLINE || &#39;select &#39; || SELECT_LIST || C_NEWLINE || &#39; from JSON_TABLE(&#39; || C_NEWLINE || &#39; :JSON,&#39; || C_NEWLINE || &#39; &#39;&#39;$.data.&quot;&#39; || TABLE_NAME || &#39;&quot;[*]&#39;&#39;&#39; || C_NEWLINE || &#39; COLUMNS(&#39; || C_NEWLINE || COLUMN_PATTERNS || C_NEWLINE || &#39;))&#39; ,NULL ,NULL ,NULL bulk collect into V_SQL_OPERATIONS from JSON_TABLE( P_JSON_DUMP_FILE, &#39;$.metadata.*&#39; ERROR ON ERROR COLUMNS ( OWNER VARCHAR2(128) PATH &#39;$.owner&#39; , TABLE_NAME VARCHAR2(128) PATH &#39;$.tableName&#39; $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN , EXPORT_SELECT_LIST CLOB PATH &#39;$.columns&#39; , DATA_TYPES CLOB PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS CLOB PATH &#39;$.columnPatterns&#39; , SELECT_LIST CLOB PATH &#39;$.importSelectList&#39; $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN , EXPORT_SELECT_LIST VARCHAR2(32767) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(32767) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(32767) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(32767) PATH &#39;$.importSelectList&#39; $ELSE , EXPORT_SELECT_LIST VARCHAR2(4000) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(4000) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(4000) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(4000) PATH &#39;$.importSelectList&#39; $END ) ); return V_SQL_OPERATIONS; end; In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB, P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;)) as V_CURRENT_SCHEMA CONSTANT VARCHAR2(128) := SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;); V_START_TIME TIMESTAMP(6); V_END_TIME TIMESTAMP(6); begin SET_CURRENT_SCHEMA(P_TARGET_SCHEMA); SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE); for i in 1 .. SQL_OPERATIONS_TABLE.count loop begin V_START_TIME := SYSTIMESTAMP; execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE; SQL_OPERATIONS_TABLE(i).RESULT := &#39;Operation completed succecssfully at &#39; || SYS_EXTRACT_UTC(SYSTIMESTAMP) || &#39;. Processed &#39; || TO_CHAR(SQL%ROWCOUNT) || &#39; rows. Elapsed time: &#39; || (V_END_TIME - V_START_TIME) || &#39;.&#39;; commit; SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS; exception when others then SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack; SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR; end; end loop; SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); exception when OTHERS then SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); RAISE; end; -- To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command: exp userid=system@ORCL owner=HR file=HR.dmp rows=N A new database schema was created using the following command in SQL*PLUS grant connect, resource, unlimited tablespace to HR2 identified by ******** And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below set echo on spool logs/IMPORT_FROM_FILE.log -- def JSON_DIR = &amp;1 def FILENAME = &amp;2 def SCHEMA = &amp;3 -- VAR JSON CLOB -- create or replace directory JSON_DIR as &#39;&amp;JSON_DIR&#39; / DECLARE V_DEST_OFFSET NUMBER := 1; V_SRC_OFFSET NUMBER := 1; V_CONTEXT NUMBER := 0; V_WARNINGS NUMBER := 0; V_BFILE BFILE := BFILENAME(&#39;JSON_DIR&#39;,&#39;&amp;FILENAME&#39;); begin DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION); DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY); DBMS_LOB.LOADCLOBFROMFILE ( :JSON,V_BFILE ,DBMS_LOB.LOBMAXSIZE ,V_DEST_OFFSET ,V_SRC_OFFSET ,NLS_CHARSET_ID(&#39;AL32UTF8&#39;) ,V_CONTEXT ,V_WARNINGS); DBMS_LOB.FILECLOSE(V_BFILE); end; / select 1 from DUAL where :JSON IS JSON / begin JSON_IMPORT.IMPORT_JSON(:JSON,&#39;&amp;SCHEMA&#39;); end; / set pages 50 lines 256 trimspool on long 1000000 -- column TABLE_NAME format A30 column SQL_STATEMENT format A80 column STATUS format A12 column RESULT format A32 -- select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT from table(JSON_IMPORT.SQL_OPERATIONS) / exit After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to the transient inconsistencies that are inherent in a table by table data load process. Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported. The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully." />
<meta property="og:description" content="JSON_IMPORT: The Yin to JSON_EXPORT’s Yang The previous posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format. The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this {&quot;data&quot; : { &quot;REGIONS&quot; : [ [1,&quot;Europe&quot;], [2,&quot;Americas&quot;], [3,&quot;Asia&quot;], [4,&quot;Middle East and Africa&quot;] ] } We can use JSON_TABLE to convert it back in a relational format select ji.* from MY_EXPORT_FILE, JSON_TABLE( JSON_EXPORT_DATA, &#39;$.data.&quot;REGIONS&quot;[*]&#39; columns ( REGION_ID NUMBER PATH &#39;$[0]&#39; ,REGION_NAME VARCHAR2 PATH &#39;$[1]&#39; ) ) JI REGION_ID REGION_NAME ---------- -------------------------------- 1 Europe 2 Americas 3 Asia 4 Middle East and Africa To perform this transformation it is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the JSON_IMPORT utility. In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information Key Content date Date the file was generated, ISO 8601 format, Zulu time schema The database schema that was the source for the export exportVersion The version of JSON_EXPORT used. Currently V1.0 jsonFeatures The current settings in package JSON_FEATURE_DETECTION sessionUser The session user who performed the EXPORT_SCHEMA operation dbName The name of the database serverHostName The logical name of the server hosting the database databaseVersion The release of the database the data came from nlsInformation An object containing the NLS parameters of the source database. In a 12.2 database the SQL used to generate the systemInformation object is shown below with function DATABASE_RELEASE return NUMBER deterministic as begin return DBMS_DB_VERSION.VERSION || &#39;.&#39; || DBMS_DB_VERSION.RELEASE; end; -- function JSON_FEATURES return VARCHAR2 deterministic as begin return JSON_OBJECT( &#39;treatAsJSON&#39; value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED ,&#39;CLOB&#39; value JSON_FEATURE_DETECTION.CLOB_SUPPORTED ,&#39;extendedString&#39; value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED ); end; function EXPORT_VERSION return NUMBER as begin return 1.0; end; select JSON_OBJECT( &#39;systemInformation&#39; value JSON_OBJECT( &#39;date&#39; value SYS_EXTRACT_UTC(SYSTIMESTAMP) ,&#39;schema&#39; value &#39;HR&#39; ,&#39;exportVersion&#39; value EXPORT_VERSION() ,&#39;jsonFeatures&#39; value JSON_QUERY(JSON_FEATURES(),&#39;$&#39;) ,&#39;sessionUser&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SESSION_USER&#39;) ,&#39;dbName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;DB_NAME&#39;) ,&#39;serverHostName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SERVER_HOST&#39;) ,&#39;databaseVersion&#39; value DATABASE_RELEASE() ,&#39;nlsInformation&#39; value JSON_OBJECTAGG(parameter, value) ) ) SYSTEM_INFORMATION from NLS_DATABASE_PARAMETERS; An abridged version of systemInformation object is shown below { &quot;systemInformation&quot;: { &quot;date&quot;: &quot;2018-06-21T04:42:44.523029&quot;, &quot;schema&quot;: &quot;HR&quot;, &quot;exportVersion&quot;: 1, &quot;jsonFeatures&quot;: { &quot;treatAsJSON&quot;: false, &quot;CLOB&quot;: false, &quot;extendedString&quot;: true }, &quot;sessionUser&quot;: &quot;SYSTEM&quot;, &quot;dbName&quot;: &quot;ORCL&quot;, &quot;serverHostName&quot;: &quot;localhost&quot;, &quot;databaseVersion&quot;: 12.2, &quot;nlsInformation&quot;: { &quot;NLS_RDBMS_VERSION&quot;: &quot;12.2.0.1.0&quot;, ... } } } In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. A metadata object is added to the export file. This object provides the following information for each table included in the export file. Key Contents owner The database schema containing the table tableName The name of the table columnList The columns in the table dataTypeList The data types of the columns in the table exportSelectList The select list used to export the data in the table columnPatternList The column patterns needed to convert the JSON back into relational form Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object is added to the GENERATE_STATEMENT module. The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below &quot;metadata&quot;: { &quot;REGIONS&quot;: { &quot;owner&quot;: &quot;HR&quot;, &quot;tableName&quot;: &quot;REGIONS&quot;, &quot;columns&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;dataTypes&quot;: &quot;\&quot;NUMBER\&quot;,\&quot;VARCHAR2\&quot;&quot;, &quot;exportSelectList&quot;: &quot;\&quot;REGION_ID\&quot;,\&quot;REGION_NAME\&quot;&quot;, &quot;columnPatterns&quot;: &quot;\&quot;REGION_ID\&quot; NUMBER PATH &#39;$[0]&#39;,\&quot;REGION_NAME\&quot; VARCHAR2 PATH &#39;$[1]&#39;&quot; }, &quot;JOBS&quot; : {... }, ... } The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB) return T_SQL_OPERATIONS_TAB as V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB; begin select OWNER ,TABLE_NAME ,&#39;insert into &quot;&#39; || TABLE_NAME ||&#39;&quot;(&#39; || EXPORT_SELECT_LIST || &#39;)&#39; || C_NEWLINE || &#39;select &#39; || SELECT_LIST || C_NEWLINE || &#39; from JSON_TABLE(&#39; || C_NEWLINE || &#39; :JSON,&#39; || C_NEWLINE || &#39; &#39;&#39;$.data.&quot;&#39; || TABLE_NAME || &#39;&quot;[*]&#39;&#39;&#39; || C_NEWLINE || &#39; COLUMNS(&#39; || C_NEWLINE || COLUMN_PATTERNS || C_NEWLINE || &#39;))&#39; ,NULL ,NULL ,NULL bulk collect into V_SQL_OPERATIONS from JSON_TABLE( P_JSON_DUMP_FILE, &#39;$.metadata.*&#39; ERROR ON ERROR COLUMNS ( OWNER VARCHAR2(128) PATH &#39;$.owner&#39; , TABLE_NAME VARCHAR2(128) PATH &#39;$.tableName&#39; $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN , EXPORT_SELECT_LIST CLOB PATH &#39;$.columns&#39; , DATA_TYPES CLOB PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS CLOB PATH &#39;$.columnPatterns&#39; , SELECT_LIST CLOB PATH &#39;$.importSelectList&#39; $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN , EXPORT_SELECT_LIST VARCHAR2(32767) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(32767) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(32767) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(32767) PATH &#39;$.importSelectList&#39; $ELSE , EXPORT_SELECT_LIST VARCHAR2(4000) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(4000) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(4000) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(4000) PATH &#39;$.importSelectList&#39; $END ) ); return V_SQL_OPERATIONS; end; In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB, P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;)) as V_CURRENT_SCHEMA CONSTANT VARCHAR2(128) := SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;); V_START_TIME TIMESTAMP(6); V_END_TIME TIMESTAMP(6); begin SET_CURRENT_SCHEMA(P_TARGET_SCHEMA); SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE); for i in 1 .. SQL_OPERATIONS_TABLE.count loop begin V_START_TIME := SYSTIMESTAMP; execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE; SQL_OPERATIONS_TABLE(i).RESULT := &#39;Operation completed succecssfully at &#39; || SYS_EXTRACT_UTC(SYSTIMESTAMP) || &#39;. Processed &#39; || TO_CHAR(SQL%ROWCOUNT) || &#39; rows. Elapsed time: &#39; || (V_END_TIME - V_START_TIME) || &#39;.&#39;; commit; SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS; exception when others then SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack; SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR; end; end loop; SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); exception when OTHERS then SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); RAISE; end; -- To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command: exp userid=system@ORCL owner=HR file=HR.dmp rows=N A new database schema was created using the following command in SQL*PLUS grant connect, resource, unlimited tablespace to HR2 identified by ******** And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below set echo on spool logs/IMPORT_FROM_FILE.log -- def JSON_DIR = &amp;1 def FILENAME = &amp;2 def SCHEMA = &amp;3 -- VAR JSON CLOB -- create or replace directory JSON_DIR as &#39;&amp;JSON_DIR&#39; / DECLARE V_DEST_OFFSET NUMBER := 1; V_SRC_OFFSET NUMBER := 1; V_CONTEXT NUMBER := 0; V_WARNINGS NUMBER := 0; V_BFILE BFILE := BFILENAME(&#39;JSON_DIR&#39;,&#39;&amp;FILENAME&#39;); begin DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION); DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY); DBMS_LOB.LOADCLOBFROMFILE ( :JSON,V_BFILE ,DBMS_LOB.LOBMAXSIZE ,V_DEST_OFFSET ,V_SRC_OFFSET ,NLS_CHARSET_ID(&#39;AL32UTF8&#39;) ,V_CONTEXT ,V_WARNINGS); DBMS_LOB.FILECLOSE(V_BFILE); end; / select 1 from DUAL where :JSON IS JSON / begin JSON_IMPORT.IMPORT_JSON(:JSON,&#39;&amp;SCHEMA&#39;); end; / set pages 50 lines 256 trimspool on long 1000000 -- column TABLE_NAME format A30 column SQL_STATEMENT format A80 column STATUS format A12 column RESULT format A32 -- select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT from table(JSON_IMPORT.SQL_OPERATIONS) / exit After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to the transient inconsistencies that are inherent in a table by table data load process. Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported. The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully." />
<link rel="canonical" href="http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html" />
<meta property="og:url" content="http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html" />
<meta property="og:site_name" content="JSON Exchange" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-20T18:00:00-07:00" />
<script type="application/ld+json">
{"description":"JSON_IMPORT: The Yin to JSON_EXPORT’s Yang The previous posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format. The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this {&quot;data&quot; : { &quot;REGIONS&quot; : [ [1,&quot;Europe&quot;], [2,&quot;Americas&quot;], [3,&quot;Asia&quot;], [4,&quot;Middle East and Africa&quot;] ] } We can use JSON_TABLE to convert it back in a relational format select ji.* from MY_EXPORT_FILE, JSON_TABLE( JSON_EXPORT_DATA, &#39;$.data.&quot;REGIONS&quot;[*]&#39; columns ( REGION_ID NUMBER PATH &#39;$[0]&#39; ,REGION_NAME VARCHAR2 PATH &#39;$[1]&#39; ) ) JI REGION_ID REGION_NAME ---------- -------------------------------- 1 Europe 2 Americas 3 Asia 4 Middle East and Africa To perform this transformation it is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the JSON_IMPORT utility. In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information Key Content date Date the file was generated, ISO 8601 format, Zulu time schema The database schema that was the source for the export exportVersion The version of JSON_EXPORT used. Currently V1.0 jsonFeatures The current settings in package JSON_FEATURE_DETECTION sessionUser The session user who performed the EXPORT_SCHEMA operation dbName The name of the database serverHostName The logical name of the server hosting the database databaseVersion The release of the database the data came from nlsInformation An object containing the NLS parameters of the source database. In a 12.2 database the SQL used to generate the systemInformation object is shown below with function DATABASE_RELEASE return NUMBER deterministic as begin return DBMS_DB_VERSION.VERSION || &#39;.&#39; || DBMS_DB_VERSION.RELEASE; end; -- function JSON_FEATURES return VARCHAR2 deterministic as begin return JSON_OBJECT( &#39;treatAsJSON&#39; value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED ,&#39;CLOB&#39; value JSON_FEATURE_DETECTION.CLOB_SUPPORTED ,&#39;extendedString&#39; value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED ); end; function EXPORT_VERSION return NUMBER as begin return 1.0; end; select JSON_OBJECT( &#39;systemInformation&#39; value JSON_OBJECT( &#39;date&#39; value SYS_EXTRACT_UTC(SYSTIMESTAMP) ,&#39;schema&#39; value &#39;HR&#39; ,&#39;exportVersion&#39; value EXPORT_VERSION() ,&#39;jsonFeatures&#39; value JSON_QUERY(JSON_FEATURES(),&#39;$&#39;) ,&#39;sessionUser&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SESSION_USER&#39;) ,&#39;dbName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;DB_NAME&#39;) ,&#39;serverHostName&#39; value SYS_CONTEXT(&#39;USERENV&#39;,&#39;SERVER_HOST&#39;) ,&#39;databaseVersion&#39; value DATABASE_RELEASE() ,&#39;nlsInformation&#39; value JSON_OBJECTAGG(parameter, value) ) ) SYSTEM_INFORMATION from NLS_DATABASE_PARAMETERS; An abridged version of systemInformation object is shown below { &quot;systemInformation&quot;: { &quot;date&quot;: &quot;2018-06-21T04:42:44.523029&quot;, &quot;schema&quot;: &quot;HR&quot;, &quot;exportVersion&quot;: 1, &quot;jsonFeatures&quot;: { &quot;treatAsJSON&quot;: false, &quot;CLOB&quot;: false, &quot;extendedString&quot;: true }, &quot;sessionUser&quot;: &quot;SYSTEM&quot;, &quot;dbName&quot;: &quot;ORCL&quot;, &quot;serverHostName&quot;: &quot;localhost&quot;, &quot;databaseVersion&quot;: 12.2, &quot;nlsInformation&quot;: { &quot;NLS_RDBMS_VERSION&quot;: &quot;12.2.0.1.0&quot;, ... } } } In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. A metadata object is added to the export file. This object provides the following information for each table included in the export file. Key Contents owner The database schema containing the table tableName The name of the table columnList The columns in the table dataTypeList The data types of the columns in the table exportSelectList The select list used to export the data in the table columnPatternList The column patterns needed to convert the JSON back into relational form Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object is added to the GENERATE_STATEMENT module. The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below &quot;metadata&quot;: { &quot;REGIONS&quot;: { &quot;owner&quot;: &quot;HR&quot;, &quot;tableName&quot;: &quot;REGIONS&quot;, &quot;columns&quot;: &quot;\\&quot;REGION_ID\\&quot;,\\&quot;REGION_NAME\\&quot;&quot;, &quot;dataTypes&quot;: &quot;\\&quot;NUMBER\\&quot;,\\&quot;VARCHAR2\\&quot;&quot;, &quot;exportSelectList&quot;: &quot;\\&quot;REGION_ID\\&quot;,\\&quot;REGION_NAME\\&quot;&quot;, &quot;columnPatterns&quot;: &quot;\\&quot;REGION_ID\\&quot; NUMBER PATH &#39;$[0]&#39;,\\&quot;REGION_NAME\\&quot; VARCHAR2 PATH &#39;$[1]&#39;&quot; }, &quot;JOBS&quot; : {... }, ... } The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB) return T_SQL_OPERATIONS_TAB as V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB; begin select OWNER ,TABLE_NAME ,&#39;insert into &quot;&#39; || TABLE_NAME ||&#39;&quot;(&#39; || EXPORT_SELECT_LIST || &#39;)&#39; || C_NEWLINE || &#39;select &#39; || SELECT_LIST || C_NEWLINE || &#39; from JSON_TABLE(&#39; || C_NEWLINE || &#39; :JSON,&#39; || C_NEWLINE || &#39; &#39;&#39;$.data.&quot;&#39; || TABLE_NAME || &#39;&quot;[*]&#39;&#39;&#39; || C_NEWLINE || &#39; COLUMNS(&#39; || C_NEWLINE || COLUMN_PATTERNS || C_NEWLINE || &#39;))&#39; ,NULL ,NULL ,NULL bulk collect into V_SQL_OPERATIONS from JSON_TABLE( P_JSON_DUMP_FILE, &#39;$.metadata.*&#39; ERROR ON ERROR COLUMNS ( OWNER VARCHAR2(128) PATH &#39;$.owner&#39; , TABLE_NAME VARCHAR2(128) PATH &#39;$.tableName&#39; $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN , EXPORT_SELECT_LIST CLOB PATH &#39;$.columns&#39; , DATA_TYPES CLOB PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS CLOB PATH &#39;$.columnPatterns&#39; , SELECT_LIST CLOB PATH &#39;$.importSelectList&#39; $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN , EXPORT_SELECT_LIST VARCHAR2(32767) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(32767) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(32767) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(32767) PATH &#39;$.importSelectList&#39; $ELSE , EXPORT_SELECT_LIST VARCHAR2(4000) PATH &#39;$.columns&#39; , DATA_TYPES VARCHAR2(4000) PATH &#39;$.dataTypes&#39; , COLUMN_PATTERNS VARCHAR2(4000) PATH &#39;$.columnPatterns&#39; , SELECT_LIST VARCHAR2(4000) PATH &#39;$.importSelectList&#39; $END ) ); return V_SQL_OPERATIONS; end; In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB, P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;)) as V_CURRENT_SCHEMA CONSTANT VARCHAR2(128) := SYS_CONTEXT(&#39;USERENV&#39;,&#39;CURRENT_SCHEMA&#39;); V_START_TIME TIMESTAMP(6); V_END_TIME TIMESTAMP(6); begin SET_CURRENT_SCHEMA(P_TARGET_SCHEMA); SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE); for i in 1 .. SQL_OPERATIONS_TABLE.count loop begin V_START_TIME := SYSTIMESTAMP; execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE; SQL_OPERATIONS_TABLE(i).RESULT := &#39;Operation completed succecssfully at &#39; || SYS_EXTRACT_UTC(SYSTIMESTAMP) || &#39;. Processed &#39; || TO_CHAR(SQL%ROWCOUNT) || &#39; rows. Elapsed time: &#39; || (V_END_TIME - V_START_TIME) || &#39;.&#39;; commit; SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS; exception when others then SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack; SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR; end; end loop; SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); exception when OTHERS then SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA); RAISE; end; -- To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command: exp userid=system@ORCL owner=HR file=HR.dmp rows=N A new database schema was created using the following command in SQL*PLUS grant connect, resource, unlimited tablespace to HR2 identified by ******** And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below set echo on spool logs/IMPORT_FROM_FILE.log -- def JSON_DIR = &amp;1 def FILENAME = &amp;2 def SCHEMA = &amp;3 -- VAR JSON CLOB -- create or replace directory JSON_DIR as &#39;&amp;JSON_DIR&#39; / DECLARE V_DEST_OFFSET NUMBER := 1; V_SRC_OFFSET NUMBER := 1; V_CONTEXT NUMBER := 0; V_WARNINGS NUMBER := 0; V_BFILE BFILE := BFILENAME(&#39;JSON_DIR&#39;,&#39;&amp;FILENAME&#39;); begin DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION); DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY); DBMS_LOB.LOADCLOBFROMFILE ( :JSON,V_BFILE ,DBMS_LOB.LOBMAXSIZE ,V_DEST_OFFSET ,V_SRC_OFFSET ,NLS_CHARSET_ID(&#39;AL32UTF8&#39;) ,V_CONTEXT ,V_WARNINGS); DBMS_LOB.FILECLOSE(V_BFILE); end; / select 1 from DUAL where :JSON IS JSON / begin JSON_IMPORT.IMPORT_JSON(:JSON,&#39;&amp;SCHEMA&#39;); end; / set pages 50 lines 256 trimspool on long 1000000 -- column TABLE_NAME format A30 column SQL_STATEMENT format A80 column STATUS format A12 column RESULT format A32 -- select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT from table(JSON_IMPORT.SQL_OPERATIONS) / exit After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to the transient inconsistencies that are inherent in a table by table data load process. Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported. The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully.","@type":"BlogPosting","url":"http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html","headline":"Introduction to JSON_IMPORT","dateModified":"2018-06-20T18:00:00-07:00","datePublished":"2018-06-20T18:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/json/export/import/oracle/2018/06/20/JSON_IMPORT.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="JSON Exchange" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">JSON Exchange</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to JSON_IMPORT</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-06-20T18:00:00-07:00" itemprop="datePublished">Jun 20, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="json_import-the-yin-to-json_exports-yang">JSON_IMPORT: The Yin to JSON_EXPORT’s Yang</h2>

<p>The <a href="/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html">previous</a> posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema’s as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format.</p>

<p>The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this</p>

<pre><code class="language-JSON">{"data" : {
    "REGIONS" : [
        [1,"Europe"],
        [2,"Americas"],
        [3,"Asia"],
        [4,"Middle East and Africa"]
    ]
}
</code></pre>

<p>We can use JSON_TABLE to convert it back in a relational format</p>

<pre><code class="language-SQL">select ji.*
  from MY_EXPORT_FILE,
       JSON_TABLE(
          JSON_EXPORT_DATA,
          '$.data."REGIONS"[*]'
          columns (
            REGION_ID     NUMBER PATH '$[0]'
           ,REGION_NAME VARCHAR2 PATH '$[1]'
		  )
		) JI
		
</code></pre>

<pre><code class="language-SQL"> REGION_ID REGION_NAME
---------- --------------------------------
         1 Europe
         2 Americas
         3 Asia
         4 Middle East and Africa
</code></pre>

<p>To perform this transformation it  is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the  JSON_IMPORT utility.</p>

<p>In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a “systemInformation” object will be added to each file. This object will contain the following information</p>

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Content</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>date</td>
      <td>Date the file was generated, ISO 8601 format, Zulu time</td>
    </tr>
    <tr>
      <td>schema</td>
      <td>The database schema that was the source for the export</td>
    </tr>
    <tr>
      <td>exportVersion</td>
      <td>The version of JSON_EXPORT used. Currently V1.0</td>
    </tr>
    <tr>
      <td>jsonFeatures</td>
      <td>The current settings in package JSON_FEATURE_DETECTION</td>
    </tr>
    <tr>
      <td>sessionUser</td>
      <td>The session user who performed the EXPORT_SCHEMA operation</td>
    </tr>
    <tr>
      <td>dbName</td>
      <td>The name of the database</td>
    </tr>
    <tr>
      <td>serverHostName</td>
      <td>The logical name of the server hosting the database</td>
    </tr>
    <tr>
      <td>databaseVersion</td>
      <td>The release of the database the data came from</td>
    </tr>
    <tr>
      <td>nlsInformation</td>
      <td>An object containing the NLS parameters of the source database.</td>
    </tr>
  </tbody>
</table>

<p>In a 12.2 database the SQL used to generate the systemInformation object is shown below</p>

<pre><code class="language-SQL">with 
function DATABASE_RELEASE return NUMBER deterministic
as
begin
  return DBMS_DB_VERSION.VERSION || '.' || DBMS_DB_VERSION.RELEASE;
end;
--
function JSON_FEATURES return VARCHAR2 deterministic
as
begin
  return JSON_OBJECT(
           'treatAsJSON'     value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED
       	  ,'CLOB'            value JSON_FEATURE_DETECTION.CLOB_SUPPORTED
          ,'extendedString'  value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED
		);
end;
function EXPORT_VERSION return NUMBER
as
begin
  return 1.0;
end;
select JSON_OBJECT(
           'systemInformation'
		   value JSON_OBJECT(
	              'date'            value SYS_EXTRACT_UTC(SYSTIMESTAMP)
	             ,'schema'          value 'HR'
		         ,'exportVersion'   value EXPORT_VERSION()
		         ,'jsonFeatures'    value JSON_QUERY(JSON_FEATURES(),'$')
	             ,'sessionUser'     value SYS_CONTEXT('USERENV','SESSION_USER')
		         ,'dbName'          value SYS_CONTEXT('USERENV','DB_NAME')
		         ,'serverHostName'  value SYS_CONTEXT('USERENV','SERVER_HOST')
		         ,'databaseVersion' value DATABASE_RELEASE()
		         ,'nlsInformation'  value JSON_OBJECTAGG(parameter, value)
	             )
		 ) SYSTEM_INFORMATION
    from NLS_DATABASE_PARAMETERS;
</code></pre>

<p>An abridged version of systemInformation object is shown below</p>

<pre><code class="language-JSON">{
	"systemInformation": {
		"date": "2018-06-21T04:42:44.523029",
		"schema": "HR",
		"exportVersion": 1,
		"jsonFeatures": {
			"treatAsJSON": false,
			"CLOB": false,
			"extendedString": true
		},
		"sessionUser": "SYSTEM",
		"dbName": "ORCL",
		"serverHostName": "localhost",
		"databaseVersion": 12.2,
		"nlsInformation": {
			"NLS_RDBMS_VERSION": "12.2.0.1.0",
			...
		}
	}
}
</code></pre>

<p>In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation.</p>

<p>A metadata object is added to the export file. This object provides the following information for each table included in the export file.</p>

<table>
  <thead>
    <tr>
      <th>Key</th>
      <th>Contents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>owner</td>
      <td>The database schema containing the table</td>
    </tr>
    <tr>
      <td>tableName</td>
      <td>The name of the table</td>
    </tr>
    <tr>
      <td>columnList</td>
      <td>The columns in the table</td>
    </tr>
    <tr>
      <td>dataTypeList</td>
      <td>The data types of the columns in the table</td>
    </tr>
    <tr>
      <td>exportSelectList</td>
      <td>The select list used to export the data in the table</td>
    </tr>
    <tr>
      <td>columnPatternList</td>
      <td>The column patterns needed to convert the JSON back into relational form</td>
    </tr>
  </tbody>
</table>

<p>Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object  is added to the GENERATE_STATEMENT module.</p>

<p>The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below</p>

<pre><code class="language-JSON">"metadata": {
  "REGIONS": {
     "owner": "HR",
     "tableName": "REGIONS",
	 "columns": "\"REGION_ID\",\"REGION_NAME\"",
	 "dataTypes": "\"NUMBER\",\"VARCHAR2\"",
	 "exportSelectList": "\"REGION_ID\",\"REGION_NAME\"",
	 "columnPatterns": "\"REGION_ID\" NUMBER PATH '$[0]',\"REGION_NAME\" VARCHAR2 PATH '$[1]'"
  },
  "JOBS" : {...
  },
  ...
}
</code></pre>

<p>The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases.</p>

<p>In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of “insert as select” statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below</p>

<pre><code class="language-SQL">function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB)
return T_SQL_OPERATIONS_TAB
as
  V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB;
begin
  select OWNER
        ,TABLE_NAME
        ,'insert into "' || TABLE_NAME ||'"(' || EXPORT_SELECT_LIST || ')' || C_NEWLINE ||
         'select ' || SELECT_LIST || C_NEWLINE ||
         '  from JSON_TABLE(' || C_NEWLINE ||
         '         :JSON,' || C_NEWLINE ||
         '         ''$.data."' || TABLE_NAME || '"[*]''' || C_NEWLINE ||
         '         COLUMNS(' || C_NEWLINE ||  COLUMN_PATTERNS || C_NEWLINE || '))' 
        ,NULL
        ,NULL
        ,NULL
   	bulk collect into V_SQL_OPERATIONS
   	from JSON_TABLE(
            P_JSON_DUMP_FILE,
		   '$.metadata.*' ERROR ON ERROR
            COLUMNS (
               OWNER       VARCHAR2(128) PATH '$.owner'
  			, TABLE_NAME  VARCHAR2(128) PATH '$.tableName'
               $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN   
   			, EXPORT_SELECT_LIST              CLOB PATH '$.columns'
             , DATA_TYPES                      CLOB PATH '$.dataTypes'
             , COLUMN_PATTERNS                 CLOB PATH '$.columnPatterns'
		    , SELECT_LIST                     CLOB PATH '$.importSelectList'
             $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
			, EXPORT_SELECT_LIST  VARCHAR2(32767) PATH '$.columns'
			, DATA_TYPES          VARCHAR2(32767) PATH '$.dataTypes'
			, COLUMN_PATTERNS     VARCHAR2(32767) PATH '$.columnPatterns'
			, SELECT_LIST         VARCHAR2(32767) PATH '$.importSelectList'
			$ELSE
			, EXPORT_SELECT_LIST   VARCHAR2(4000) PATH '$.columns'
			, DATA_TYPES           VARCHAR2(4000) PATH '$.dataTypes'
			, COLUMN_PATTERNS      VARCHAR2(4000) PATH '$.columnPatterns'
			, SELECT_LIST          VARCHAR2(4000) PATH '$.importSelectList'
			$END
		  )
		);
  return V_SQL_OPERATIONS;
end;
</code></pre>

<p>In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below</p>

<pre><code class="language-SQL">procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB,
                      P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT('USERENV','CURRENT_SCHEMA'))
as
  V_CURRENT_SCHEMA           CONSTANT VARCHAR2(128) := SYS_CONTEXT('USERENV','CURRENT_SCHEMA');
  V_START_TIME TIMESTAMP(6);
  V_END_TIME   TIMESTAMP(6);
begin
  SET_CURRENT_SCHEMA(P_TARGET_SCHEMA);
  
  SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE);
							
  for i in 1 .. SQL_OPERATIONS_TABLE.count loop
    begin
      V_START_TIME := SYSTIMESTAMP;
      execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE;  
 	  SQL_OPERATIONS_TABLE(i).RESULT := 'Operation completed succecssfully at ' 
 	                                 || SYS_EXTRACT_UTC(SYSTIMESTAMP) 
 	                                 || '. Processed ' || TO_CHAR(SQL%ROWCOUNT) 
 	                                 || ' rows. Elapsed time: ' 
 	                                 || (V_END_TIME - V_START_TIME) || '.';
	  commit;
	  SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS;
    exception
      when others then
	    SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack;
		SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR;
	end;
  end loop;
  SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
exception
  when OTHERS then
    SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
	RAISE;
end;
--
</code></pre>

<p>To test JSON_IMPORT a clone of the HR Schema was made using Oracle’s classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command:</p>

<pre><code class="language-SH">exp userid=system@ORCL owner=HR file=HR.dmp rows=N
</code></pre>

<p>A new database schema was created using the following command in SQL*PLUS</p>

<pre><code class="language-SQL">grant connect, resource, unlimited tablespace to HR2 identified by ********
</code></pre>

<p>And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command</p>

<pre><code class="language-SH">imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp 
</code></pre>

<p>The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set echo on
spool logs/IMPORT_FROM_FILE.log
--
def JSON_DIR = &amp;1
def FILENAME = &amp;2
def SCHEMA = &amp;3
--
VAR JSON CLOB
--
create or replace directory JSON_DIR as '&amp;JSON_DIR'
/
DECLARE
  V_DEST_OFFSET NUMBER := 1;
  V_SRC_OFFSET  NUMBER := 1;
  V_CONTEXT     NUMBER := 0;
  V_WARNINGS    NUMBER := 0;
  V_BFILE	     BFILE := BFILENAME('JSON_DIR','&amp;FILENAME');
begin
  DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION);
  DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY);
  DBMS_LOB.LOADCLOBFROMFILE (
             :JSON,V_BFILE
            ,DBMS_LOB.LOBMAXSIZE
            ,V_DEST_OFFSET
            ,V_SRC_OFFSET
            ,NLS_CHARSET_ID('AL32UTF8')
            ,V_CONTEXT
            ,V_WARNINGS);
  DBMS_LOB.FILECLOSE(V_BFILE);
end;
/
select 1
	from DUAL
  where :JSON IS JSON
/
begin
  JSON_IMPORT.IMPORT_JSON(:JSON,'&amp;SCHEMA');
end;
/
set pages 50 lines 256 trimspool on long 1000000
--
column TABLE_NAME format A30
column SQL_STATEMENT format A80
column STATUS format A12
column RESULT format A32
-- 
select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT 
  from table(JSON_IMPORT.SQL_OPERATIONS)
/
exit
</code></pre></div></div>

<p>After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function.</p>

<p>The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to  the transient inconsistencies that are inherent in a table by table data load process.</p>

<p>Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported.</p>

<p>The next post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully.</p>


  </div><a class="u-url" href="/json/export/import/oracle/2018/06/20/JSON_IMPORT.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">JSON Exchange</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">JSON Exchange</li><li><a class="u-email" href="mailto:mdd@appdev4db.com">mdd@appdev4db.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/markddrake"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">markddrake</span></a></li><li><a href="https://www.twitter.com/markddrake"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">markddrake</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A simple utility for exporting and importing data using JSON</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
