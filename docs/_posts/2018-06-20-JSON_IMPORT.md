---
layout: post
title:  "Part VII: Package JSON_IMPORT"
date:   2018-06-20 18:00:00-0700
categories: JSON Export Import Oracle
---

## The Yin to JSON_EXPORT's Yang

The [previous]({{ site.baseurl }}{% link _posts/2018-06-19-Object and Wide Tables.md%}) posts describe a package, JSON_EXPORT, that can export the contents of Oracle database schema's as a JSON document. The next post will examine a package, JSON_IMPORT, that is capable of taking output generated using JSON_EXPORT and converting it back into relational format.

The idea behind JSON_IMPORT is quite simple, take the output of JSON_EXPORT and covert it back into relational format. The JSON_TABLE operator makes this a relatively simple process. Assuming we have a simple JSON document like this

```JSON
{"data" : {
    "REGIONS" : [
        [1,"Europe"],
        [2,"Americas"],
        [3,"Asia"],
        [4,"Middle East and Africa"]
    ]
}
```

We can use JSON_TABLE to convert it back in a relational format

```SQL
select ji.*
  from MY_EXPORT_FILE,
       JSON_TABLE(
          JSON_EXPORT_DATA,
          '$.data."REGIONS"[*]'
          columns (
            REGION_ID     NUMBER PATH '$[0]'
           ,REGION_NAME VARCHAR2 PATH '$[1]'
		  )
		) JI
		
```

```SQL
 REGION_ID REGION_NAME
---------- --------------------------------
         1 Europe
         2 Americas
         3 Asia
         4 Middle East and Africa
```

To perform this transformation it  is necessary to know the column names, and data types that are needed to create the column patterns clause of the JSON_TABLE statement. This information was present in the source database, so JSON_EXPORT will be modified so that it is included it in export file in a format that can easily be consumed by the  JSON_IMPORT utility. 

In order to identify what the source of the JSON generated by JSON_EXPORT was and which version of JSON_EXPORT generated the file a "systemInformation" object will be added to each file. This object will contain the following information 

| Key             | Content                                                      |
| --------------- | ------------------------------------------------------------ |
| date            | Date the file was generated, ISO 8601 format, Zulu time      |
| schema          | The database schema that was the source for the export       |
| exportVersion   | The version of JSON_EXPORT used. Currently V1.0              |
| jsonFeatures    | The current settings in package JSON_FEATURE_DETECTION       |
| sessionUser     | The session user who performed the EXPORT_SCHEMA operation   |
| dbName          | The name of the database                                     |
| serverHostName  | The logical name of the server hosting the database          |
| databaseVersion | The release of the database the data came from               |
| nlsInformation  | An object containing the NLS parameters of the source database. |

In a 12.2 database the SQL used to generate the systemInformation object is shown below

```SQL
with 
function DATABASE_RELEASE return NUMBER deterministic
as
begin
  return DBMS_DB_VERSION.VERSION || '.' || DBMS_DB_VERSION.RELEASE;
end;
--
function JSON_FEATURES return VARCHAR2 deterministic
as
begin
  return JSON_OBJECT(
           'treatAsJSON'     value JSON_FEATURE_DETECTION.TREAT_AS_JSON_SUPPORTED
       	  ,'CLOB'            value JSON_FEATURE_DETECTION.CLOB_SUPPORTED
          ,'extendedString'  value JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED
		);
end;
function EXPORT_VERSION return NUMBER
as
begin
  return 1.0;
end;
select JSON_OBJECT(
           'systemInformation'
		   value JSON_OBJECT(
	              'date'            value SYS_EXTRACT_UTC(SYSTIMESTAMP)
	             ,'schema'          value 'HR'
		         ,'exportVersion'   value EXPORT_VERSION()
		         ,'jsonFeatures'    value JSON_QUERY(JSON_FEATURES(),'$')
	             ,'sessionUser'     value SYS_CONTEXT('USERENV','SESSION_USER')
		         ,'dbName'          value SYS_CONTEXT('USERENV','DB_NAME')
		         ,'serverHostName'  value SYS_CONTEXT('USERENV','SERVER_HOST')
		         ,'databaseVersion' value DATABASE_RELEASE()
		         ,'nlsInformation'  value JSON_OBJECTAGG(parameter, value)
	             )
		 ) SYSTEM_INFORMATION
    from NLS_DATABASE_PARAMETERS;
```

An abridged version of systemInformation object is shown below 

```JSON
{
	"systemInformation": {
		"date": "2018-06-21T04:42:44.523029",
		"schema": "HR",
		"exportVersion": 1,
		"jsonFeatures": {
			"treatAsJSON": false,
			"CLOB": false,
			"extendedString": true
		},
		"sessionUser": "SYSTEM",
		"dbName": "ORCL",
		"serverHostName": "localhost",
		"databaseVersion": 12.2,
		"nlsInformation": {
			"NLS_RDBMS_VERSION": "12.2.0.1.0",
			...
		}
	}
}
```

In a 12.2 database JSON_FEATURES() has to be wrapped with a JSON_QUERY operator so that JSON_OBJECT recognizes the result as JSON. In database 18, the JSON_QUERY operator is replaced with the more efficient TREAT( JSON_FEATURES() as JSON) operation. 

A metadata object is added to the export file. This object provides the following information for each table included in the export file. 

| Key               | Contents                                                     |
| ----------------- | ------------------------------------------------------------ |
| owner             | The database schema containing the table                     |
| tableName         | The name of the table                                        |
| columnList        | The columns in the table                                     |
| dataTypeList      | The data types of the columns in the table                   |
| exportSelectList  | The select list used to export the data in the table         |
| columnPatternList | The column patterns needed to convert the JSON back into relational form |

Not all data types supported by JSON_ARRAY are supported by JSON_TABLE. The code to generate the column pattern list performs a mapping between unsupported and supported data types. The code to generate the columnList, dataTypeList and columnPatternList and construct the metadata object  is added to the GENERATE_STATEMENT module. 

The code to include the systemInformation and metadata objects is added the EXPORT_SCHEMA function. An abridged version of the metadata object for the HR schema is show below

```JSON
"metadata": {
  "REGIONS": {
     "owner": "HR",
     "tableName": "REGIONS",
	 "columns": "\"REGION_ID\",\"REGION_NAME\"",
	 "dataTypes": "\"NUMBER\",\"VARCHAR2\"",
	 "exportSelectList": "\"REGION_ID\",\"REGION_NAME\"",
	 "columnPatterns": "\"REGION_ID\" NUMBER PATH '$[0]',\"REGION_NAME\" VARCHAR2 PATH '$[1]'"
  },
  "JOBS" : {...
  },
  ...
}
```

The JSON_IMPORT package uses the metadata object to construct the JSON_TABLE operators required to convert the contents of the export file back into relational tables. The import operation is performed in two phases. 

In phase one function GENERATE_DML_STATEMENTS uses a JSON_TABLE query to transform the contents of the metadata object into a set of "insert as select" statements. One statement is generated for each table in the export file. The queries are cached in a PL/SQL table. The code for this function is shown below

```SQL
function GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB)
return T_SQL_OPERATIONS_TAB
as
  V_SQL_OPERATIONS T_SQL_OPERATIONS_TAB;
begin
  select OWNER
        ,TABLE_NAME
        ,'insert into "' || TABLE_NAME ||'"(' || EXPORT_SELECT_LIST || ')' || C_NEWLINE ||
         'select ' || SELECT_LIST || C_NEWLINE ||
         '  from JSON_TABLE(' || C_NEWLINE ||
         '         :JSON,' || C_NEWLINE ||
         '         ''$.data."' || TABLE_NAME || '"[*]''' || C_NEWLINE ||
         '         COLUMNS(' || C_NEWLINE ||  COLUMN_PATTERNS || C_NEWLINE || '))' 
        ,NULL
        ,NULL
        ,NULL
   	bulk collect into V_SQL_OPERATIONS
   	from JSON_TABLE(
            P_JSON_DUMP_FILE,
		   '$.metadata.*' ERROR ON ERROR
            COLUMNS (
               OWNER       VARCHAR2(128) PATH '$.owner'
  			, TABLE_NAME  VARCHAR2(128) PATH '$.tableName'
               $IF JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN   
   			, EXPORT_SELECT_LIST              CLOB PATH '$.columns'
             , DATA_TYPES                      CLOB PATH '$.dataTypes'
             , COLUMN_PATTERNS                 CLOB PATH '$.columnPatterns'
		    , SELECT_LIST                     CLOB PATH '$.importSelectList'
             $ELSIF JSON_FEATURE_DETECTION.EXTENDED_STRING_SUPPORTED $THEN
			, EXPORT_SELECT_LIST  VARCHAR2(32767) PATH '$.columns'
			, DATA_TYPES          VARCHAR2(32767) PATH '$.dataTypes'
			, COLUMN_PATTERNS     VARCHAR2(32767) PATH '$.columnPatterns'
			, SELECT_LIST         VARCHAR2(32767) PATH '$.importSelectList'
			$ELSE
			, EXPORT_SELECT_LIST   VARCHAR2(4000) PATH '$.columns'
			, DATA_TYPES           VARCHAR2(4000) PATH '$.dataTypes'
			, COLUMN_PATTERNS      VARCHAR2(4000) PATH '$.columnPatterns'
			, SELECT_LIST          VARCHAR2(4000) PATH '$.importSelectList'
			$END
		  )
		);
  return V_SQL_OPERATIONS;
end;
```

In phase two, the procedure IMPORT_JSON iterates over the cached statements, executing them one-by-one to import the data. The results of each operation are recorded in the statement cache. The code for the IMPORT_JSON function is shown below

```SQL
procedure IMPORT_JSON(P_JSON_DUMP_FILE IN OUT NOCOPY CLOB,
                      P_TARGET_SCHEMA VARCHAR2 DEFAULT SYS_CONTEXT('USERENV','CURRENT_SCHEMA'))
as
  V_CURRENT_SCHEMA           CONSTANT VARCHAR2(128) := SYS_CONTEXT('USERENV','CURRENT_SCHEMA');
  V_START_TIME TIMESTAMP(6);
  V_END_TIME   TIMESTAMP(6);
begin
  SET_CURRENT_SCHEMA(P_TARGET_SCHEMA);
  
  SQL_OPERATIONS_TABLE := GENERATE_DML_STATEMENTS(P_JSON_DUMP_FILE);
							
  for i in 1 .. SQL_OPERATIONS_TABLE.count loop
    begin
      V_START_TIME := SYSTIMESTAMP;
      execute immediate SQL_OPERATIONS_TABLE(i).SQL_STATEMENT using P_JSON_DUMP_FILE;  
 	  SQL_OPERATIONS_TABLE(i).RESULT := 'Operation completed succecssfully at ' 
 	                                 || SYS_EXTRACT_UTC(SYSTIMESTAMP) 
 	                                 || '. Processed ' || TO_CHAR(SQL%ROWCOUNT) 
 	                                 || ' rows. Elapsed time: ' 
 	                                 || (V_END_TIME - V_START_TIME) || '.';
	  commit;
	  SQL_OPERATIONS_TABLE(i).STATUS := C_SUCCESS;
    exception
      when others then
	    SQL_OPERATIONS_TABLE(i).RESULT := DBMS_UTILITY.format_error_stack;
		SQL_OPERATIONS_TABLE(i).STATUS := C_FATAL_ERROR;
	end;
  end loop;
  SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
exception
  when OTHERS then
    SET_CURRENT_SCHEMA(V_CURRENT_SCHEMA);
	RAISE;
end;
--
```

To test JSON_IMPORT a clone of the HR Schema was made using Oracle's classic export and import tools (exp/imp). An no data export of the HR schema was made using the following command:

```SH
exp userid=system@ORCL owner=HR file=HR.dmp rows=N
```

A new database schema was created using the following command in SQL*PLUS

```SQL
grant connect, resource, unlimited tablespace to HR2 identified by ********
```

And then a clone of the structure of the HR schema was cloned into the HR2 schema using the following command

```SH
imp userid=system@ORCL fromuser=HR touser=HR2 file=HR.dmp 
```

The IMPORT_FROM_FILE script was used to test populating the tables in the HR2 schema using the data that had been exported from the HR schema. The code for this script is shown below

```
set echo on
spool logs/IMPORT_FROM_FILE.log
--
def JSON_DIR = &1
def FILENAME = &2
def SCHEMA = &3
--
VAR JSON CLOB
--
create or replace directory JSON_DIR as '&JSON_DIR'
/
DECLARE
  V_DEST_OFFSET NUMBER := 1;
  V_SRC_OFFSET  NUMBER := 1;
  V_CONTEXT     NUMBER := 0;
  V_WARNINGS    NUMBER := 0;
  V_BFILE	     BFILE := BFILENAME('JSON_DIR','&FILENAME');
begin
  DBMS_LOB.createTemporary(:JSON,TRUE,DBMS_LOB.SESSION);
  DBMS_LOB.FILEOPEN(V_BFILE,DBMS_LOB.FILE_READONLY);
  DBMS_LOB.LOADCLOBFROMFILE (
             :JSON,V_BFILE
            ,DBMS_LOB.LOBMAXSIZE
            ,V_DEST_OFFSET
            ,V_SRC_OFFSET
            ,NLS_CHARSET_ID('AL32UTF8')
            ,V_CONTEXT
            ,V_WARNINGS);
  DBMS_LOB.FILECLOSE(V_BFILE);
end;
/
select 1
	from DUAL
  where :JSON IS JSON
/
begin
  JSON_IMPORT.IMPORT_JSON(:JSON,'&SCHEMA');
end;
/
set pages 50 lines 256 trimspool on long 1000000
--
column TABLE_NAME format A30
column SQL_STATEMENT format A80
column STATUS format A12
column RESULT format A32
-- 
select TABLE_NAME, STATUS, SQL_STATEMENT, RESULT 
  from table(JSON_IMPORT.SQL_OPERATIONS)
/
exit
```

After the import has completes the script interrogates the content of the statement cache to determine whether the operation was successful by applying a table operator to the results of the SQL_OPERATIONS function. 

The log showed that operations on the EMPLOYEES and DEPARTMENTS tables failed due to referential integrity constraint violations. This type of constraint often prevents this style of operation from succeeding due to  the transient inconsistencies that are inherent in a table by table data load process.

Additional code was added to the IMPORT_JSON function to disable referential integrity constraints prior to commencing an import operation and then re-enable them once the operation is complete. At this point importing an export file generated from the HR schema into the HR2 schema completed without any errors being reported.

The [next]({{ site.baseurl }}{% link _posts/2018-06-21-DDL Operations.md%}) post will examine the issues that arise when attempting to use JSON_EXPORT and JSON_IMPORT to clone the SH, OE, PM and IX schemas and to verify that the clone operations have competed successfully.













