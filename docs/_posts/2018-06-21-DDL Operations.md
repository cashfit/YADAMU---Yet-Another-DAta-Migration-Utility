---
layout: post
title:  "Part VIII: Cloning a Schema"
date:   2018-06-21 18:00:00-0700
categories: JSON Import Export Oracle
---

## Adding DDL operations into the mix

The [previous]({{ site.baseurl }}{% link _posts/2018-06-20-JSON_IMPORT.md%}) post described a package for importing data from the files generated by JSON_EXPORT. It also described a method for testing this package which relied on using Oracle's classing export (exp) and import(imp) utilities to  create an empty clone of a database schema. The method works for the HR schema, Unfortunately when it applied to the  other sample schemas, a number of errors are reported during the import operation, including illegal identifiers and unsupported data types. Clearly another solution to duplicating a schema's structure is required. 

This post introduces the PL/SQL package JSON_EXPORT_DDL. This package provides two functions. The first, FETCH_DDL_STATEMENTS, returns the set of DDL statements used to create all of the objects in the schema. The second, APPLY_DDL_STATEMENTS replays the DDL captured by FETCH_DLL_STATEMENTS in the target schema, effective turning the target schema into a structural clone of the source schema.

The FETCH_DDL_STATEMENTS function uses the Oracle supplied package DBMS_METADATA  to obtain the set of DLL operations used to create the source schema. The order in which DDL Operations are exported is as follows

1. XML Schemas, generated using DBMS_METADATA. XML Schemas must be registered in the target schema before attempting to perform DDL operations.
2. DLL Operations, generated using DBMS_METADATA. DBMS_METADATA is configured to omit storage clauses making it much easier to replay the DLL operations in the target environment.
3. DMS_XDBZ.enableHierarchy() operations. This re-establishes any relationship between an XMLType table and the XML DB repository.
4. Index naming operations. This ensure that system generated index names from the source schema are preserved in the target schema.

The code used to access the DDL operations is shown below

```SQL
  begin
    V_HDL_OPEN := DBMS_METADATA.OPEN('SCHEMA_EXPORT');
    DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'SCHEMA',P_SCHEMA);

    V_HDL_TRANSFORM := DBMS_METADATA.ADD_TRANSFORM(V_HDL_OPEN,'DDL');

    -- Suppress Segement information for TABLES, INDEXES and CONSTRAINTS

    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'TABLE');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'INDEX');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'SEGMENT_ATTRIBUTES',false,'CONSTRAINT');

    -- Return constraints as 'ALTER TABLE' operations

    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'CONSTRAINTS_AS_ALTER',true,'TABLE');
    DBMS_METADATA.SET_TRANSFORM_PARAM(V_HDL_TRANSFORM,'REF_CONSTRAINTS',false,'TABLE');

    -- Exclude XML Schema Info. XML Schemas need to come first and 
    -- are handled in a seperate section

    DBMS_METADATA.SET_FILTER(V_HDL_OPEN,'EXCLUDE_PATH_EXPR','=''XMLSCHEMA''');

    loop
      -- Get the next batch of DDL_STATEMENTS. Each batch may contain zero or more statements.
      V_DDL_STATEMENTS := DBMS_METADATA.FETCH_DDL(V_HDL_OPEN);
	  EXIT WHEN V_DDL_STATEMENTS IS NULL;

      for i in 1 .. V_DDL_STATEMENTS.count loop

  	    V_DDL_STATEMENT := V_DDL_STATEMENTS(i).DDLTEXT;

  	    -- Strip leading and trailing white space from DDL statement
	    V_DDL_STATEMENT := TRIM(BOTH C_NEWLINE FROM V_DDL_STATEMENT);
        V_DDL_STATEMENT := TRIM(BOTH C_CARRIAGE_RETURN FROM V_DDL_STATEMENT);
        V_DDL_STATEMENT := TRIM(V_DDL_STATEMENT);

        PIPE ROW (TRIM(V_DDL_STATEMENT));
      end loop;
    end loop;
```

The EXPORT_SCHEMA function inserts a "ddl" object into the export file immediately after the "systemInformation" object. The "ddl" object is created based on the DDL statements returned by FETCH_DLL_STATEMENTS. The "ddl" object is an array, each item in the array contains a single ddl operation.

The IMPORT_JSON function passes the export file to APPLY_DDL_STATEMENTS before attempting to import data. This function uses JSON_TABLE to extract each of the ddl statements from the dump and executes them in order.Before replaying a DDL operation all references to the source schema are replaced with references to the target schema. This enables cloning from schema A to schema B.

Exceptions may be raised while replaying the DDL generated by DBMS_METADATA.  In most cases these can be safely ignored. Duplicate name/key/index/constraint exceptions occur because the DDL generated by DBMS_METADATA results in some objects being created more than once.  Exceptions also  arise when re-creating constraints if the required permissions have not been granted to the target schema . Exceptions are raised if the source schema uses Oracle Advanced Queuing (AQ). When AQ is present in the source DBMS_METADATA generates operations that cannot be  replayed in the target schemas, even by a  DBA.  Any exceptions that arise while replaying the DDL statements are caught, classified as IGNOREABLE, DUPLICATE, REFERENCE  WARNING, AQ_RELATED, or FATAL and logged.

The log of DDL operations can be queried by applying a table operator to the IMPORT_DDL_LOG function.

EXPORT_SCHEMA and IMPORT_JSON are now theoretically capable of cloning any database schema. The [next]({{ site.baseurl }}{% link _posts/2018-06-22-Cloning Sample Schemas.md%}) post will examine what occurs when attempting to clone each of the six Oracle supplied sample schemas.