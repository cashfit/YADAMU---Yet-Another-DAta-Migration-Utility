<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Object Tables and Wide Tables | JSON Exchange</title>
<meta name="generator" content="Jekyll v3.8.3" />
<meta property="og:title" content="Object Tables and Wide Tables" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Supporting Object Tables and Wide Tables The previous post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how support for object serialization can be applied to object tables. In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$. XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID. The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from SYS_NC_OID$. Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below from ALL_ALL_TABLES aat inner join ALL_TAB_COLS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME left outer join ALL_TYPES at on at.TYPE_NAME = atc.DATA_TYPE and at.OWNER = atc.DATA_TYPE_OWNER where aat.STATUS = &#39;VALID&#39; and aat.DROPPED = &#39;NO&#39; and aat.TEMPORARY = &#39;N&#39; and aat.EXTERNAL = &#39;NO&#39; and aat.NESTED = &#39;NO&#39; and aat.SECONDARY = &#39;N&#39; and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = &#39;IOT&#39;) and ( ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = &#39;NO&#39;)) or ( (TABLE_TYPE is not NULL) and (COLUMN_NAME in (&#39;SYS_NC_ROWINFO$&#39;,&#39;SYS_NC_OID$&#39;,&#39;ACLOID&#39;,&#39;OWNERID&#39;)) ) ) and aat.OWNER = P_SCHEMA With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file. Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in ORA-40478: output value too large (maximum: 32767) The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY. The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON. With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2 or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value. Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual JSON array. The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases. The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB) as C_SELECT_LIST_START CONSTANT VARCHAR2(32) := &#39;select JSON_ARRAY(&#39;; C_SELECT_LIST_END CONSTANT VARCHAR2(32) := &#39; NULL on NULL&#39;; V_SQL_STATEMENT CLOB; V_SELECT_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_SELECT_LIST_ITEM VARCHAR2(4000); V_SELECT_LIST_START PLS_INTEGER; V_SELECT_LIST_END PLS_INTEGER; V_FROM_CLAUSE_START PLS_INTEGER; V_CURRENT_OFFSET PLS_INTEGER; V_COLUMN_OFFSET PLS_INTEGER; V_FROM_WHERE_CLAUSE VARCHAR2(32767); V_COLUMN_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_COLUMN_NAME VARCHAR2(132); V_COLUMN_NAME_START PLS_INTEGER; V_COLUMN_NAME_END PLS_INTEGER; V_CURSOR SYS_REFCURSOR; V_CURSOR_ID NUMBER := DBMS_SQL.OPEN_CURSOR; V_COLUMN_DESCRIPTIONS DBMS_SQL.DESC_TAB2; V_COLUMN_COUNT NUMBER; V_COLUMN_VALUE VARCHAR2(32767); V_FIRST_ROW BOOLEAN := TRUE; V_FIRST_COLUMN BOOLEAN := TRUE; V_INDEX PLS_INTEGER; begin V_SQL_STATEMENT := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_SELECT_LIST_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START) + LENGTH(C_SELECT_LIST_START); V_SELECT_LIST_END := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1; V_CURRENT_OFFSET := V_SELECT_LIST_START; loop V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39;,&#39;,V_CURRENT_OFFSET); exit when ((V_COLUMN_OFFSET &lt; 1) or (V_COLUMN_OFFSET &gt; V_SELECT_LIST_END)); V_SELECT_LIST.extend; V_COLUMN_LIST.extend; V_INDEX := V_SELECT_LIST.count; V_SELECT_LIST_ITEM := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET - V_CURRENT_OFFSET,V_CURRENT_OFFSET); V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;); V_COLUMN_NAME_END := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;,V_COLUMN_NAME_START+1)+1; V_COLUMN_NAME := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END -V_COLUMN_NAME_START); V_SELECT_LIST_ITEM := &#39;JSON_ARRAY(&#39; || V_SELECT_LIST_ITEM || &#39; NULL ON NULL RETURNING VARCHAR2(&#39; || C_MAX_OUTPUT_SIZE || &#39;)) &#39; || V_COLUMN_NAME; V_SELECT_LIST(V_INDEX) := V_SELECT_LIST_ITEM; v_COLUMN_LIST(V_INDEX) := V_COLUMN_NAME; V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1; end loop; V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39; from &#39;,V_SELECT_LIST_END); V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START); DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12); DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST)); DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE)); EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT; V_FIRST_ROW := TRUE; open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR); DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS); V_COLUMN_NAME_START := 2; for i in 1..V_COLUMN_COUNT loop DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767); end loop; while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &gt; 0) loop if (not V_FIRST_ROW) then DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;,&#39;); end if; V_FIRST_ROW := FALSE; V_FIRST_COLUMN := TRUE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;[&#39;); for i in 1..V_COLUMN_COUNT loop DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE); V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2); if (not V_FIRST_COLUMN) then V_COLUMN_VALUE := &#39;,&#39; || V_COLUMN_VALUE; end if; V_FIRST_COLUMN := FALSE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE); end loop; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;]&#39;); end loop; DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID); end; With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas. The next post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data." />
<meta property="og:description" content="Supporting Object Tables and Wide Tables The previous post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how support for object serialization can be applied to object tables. In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$. XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID. The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from SYS_NC_OID$. Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below from ALL_ALL_TABLES aat inner join ALL_TAB_COLS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME left outer join ALL_TYPES at on at.TYPE_NAME = atc.DATA_TYPE and at.OWNER = atc.DATA_TYPE_OWNER where aat.STATUS = &#39;VALID&#39; and aat.DROPPED = &#39;NO&#39; and aat.TEMPORARY = &#39;N&#39; and aat.EXTERNAL = &#39;NO&#39; and aat.NESTED = &#39;NO&#39; and aat.SECONDARY = &#39;N&#39; and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = &#39;IOT&#39;) and ( ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = &#39;NO&#39;)) or ( (TABLE_TYPE is not NULL) and (COLUMN_NAME in (&#39;SYS_NC_ROWINFO$&#39;,&#39;SYS_NC_OID$&#39;,&#39;ACLOID&#39;,&#39;OWNERID&#39;)) ) ) and aat.OWNER = P_SCHEMA With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file. Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in ORA-40478: output value too large (maximum: 32767) The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY. The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON. With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2 or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value. Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual JSON array. The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases. The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB) as C_SELECT_LIST_START CONSTANT VARCHAR2(32) := &#39;select JSON_ARRAY(&#39;; C_SELECT_LIST_END CONSTANT VARCHAR2(32) := &#39; NULL on NULL&#39;; V_SQL_STATEMENT CLOB; V_SELECT_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_SELECT_LIST_ITEM VARCHAR2(4000); V_SELECT_LIST_START PLS_INTEGER; V_SELECT_LIST_END PLS_INTEGER; V_FROM_CLAUSE_START PLS_INTEGER; V_CURRENT_OFFSET PLS_INTEGER; V_COLUMN_OFFSET PLS_INTEGER; V_FROM_WHERE_CLAUSE VARCHAR2(32767); V_COLUMN_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_COLUMN_NAME VARCHAR2(132); V_COLUMN_NAME_START PLS_INTEGER; V_COLUMN_NAME_END PLS_INTEGER; V_CURSOR SYS_REFCURSOR; V_CURSOR_ID NUMBER := DBMS_SQL.OPEN_CURSOR; V_COLUMN_DESCRIPTIONS DBMS_SQL.DESC_TAB2; V_COLUMN_COUNT NUMBER; V_COLUMN_VALUE VARCHAR2(32767); V_FIRST_ROW BOOLEAN := TRUE; V_FIRST_COLUMN BOOLEAN := TRUE; V_INDEX PLS_INTEGER; begin V_SQL_STATEMENT := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_SELECT_LIST_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START) + LENGTH(C_SELECT_LIST_START); V_SELECT_LIST_END := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1; V_CURRENT_OFFSET := V_SELECT_LIST_START; loop V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39;,&#39;,V_CURRENT_OFFSET); exit when ((V_COLUMN_OFFSET &lt; 1) or (V_COLUMN_OFFSET &gt; V_SELECT_LIST_END)); V_SELECT_LIST.extend; V_COLUMN_LIST.extend; V_INDEX := V_SELECT_LIST.count; V_SELECT_LIST_ITEM := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET - V_CURRENT_OFFSET,V_CURRENT_OFFSET); V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;); V_COLUMN_NAME_END := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;,V_COLUMN_NAME_START+1)+1; V_COLUMN_NAME := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END -V_COLUMN_NAME_START); V_SELECT_LIST_ITEM := &#39;JSON_ARRAY(&#39; || V_SELECT_LIST_ITEM || &#39; NULL ON NULL RETURNING VARCHAR2(&#39; || C_MAX_OUTPUT_SIZE || &#39;)) &#39; || V_COLUMN_NAME; V_SELECT_LIST(V_INDEX) := V_SELECT_LIST_ITEM; v_COLUMN_LIST(V_INDEX) := V_COLUMN_NAME; V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1; end loop; V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39; from &#39;,V_SELECT_LIST_END); V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START); DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12); DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST)); DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE)); EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT; V_FIRST_ROW := TRUE; open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR); DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS); V_COLUMN_NAME_START := 2; for i in 1..V_COLUMN_COUNT loop DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767); end loop; while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &gt; 0) loop if (not V_FIRST_ROW) then DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;,&#39;); end if; V_FIRST_ROW := FALSE; V_FIRST_COLUMN := TRUE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;[&#39;); for i in 1..V_COLUMN_COUNT loop DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE); V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2); if (not V_FIRST_COLUMN) then V_COLUMN_VALUE := &#39;,&#39; || V_COLUMN_VALUE; end if; V_FIRST_COLUMN := FALSE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE); end loop; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;]&#39;); end loop; DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID); end; With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas. The next post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data." />
<link rel="canonical" href="http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html" />
<meta property="og:url" content="http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html" />
<meta property="og:site_name" content="JSON Exchange" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-06-19T18:00:00-07:00" />
<script type="application/ld+json">
{"description":"Supporting Object Tables and Wide Tables The previous post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how support for object serialization can be applied to object tables. In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$. XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID. The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from SYS_NC_OID$. Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below from ALL_ALL_TABLES aat inner join ALL_TAB_COLS atc on atc.OWNER = aat.OWNER and atc.TABLE_NAME = aat.TABLE_NAME left outer join ALL_TYPES at on at.TYPE_NAME = atc.DATA_TYPE and at.OWNER = atc.DATA_TYPE_OWNER where aat.STATUS = &#39;VALID&#39; and aat.DROPPED = &#39;NO&#39; and aat.TEMPORARY = &#39;N&#39; and aat.EXTERNAL = &#39;NO&#39; and aat.NESTED = &#39;NO&#39; and aat.SECONDARY = &#39;N&#39; and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = &#39;IOT&#39;) and ( ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = &#39;NO&#39;)) or ( (TABLE_TYPE is not NULL) and (COLUMN_NAME in (&#39;SYS_NC_ROWINFO$&#39;,&#39;SYS_NC_OID$&#39;,&#39;ACLOID&#39;,&#39;OWNERID&#39;)) ) ) and aat.OWNER = P_SCHEMA With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file. Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in ORA-40478: output value too large (maximum: 32767) The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY. The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON. With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2 or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value. Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual JSON array. The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases. The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB) as C_SELECT_LIST_START CONSTANT VARCHAR2(32) := &#39;select JSON_ARRAY(&#39;; C_SELECT_LIST_END CONSTANT VARCHAR2(32) := &#39; NULL on NULL&#39;; V_SQL_STATEMENT CLOB; V_SELECT_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_SELECT_LIST_ITEM VARCHAR2(4000); V_SELECT_LIST_START PLS_INTEGER; V_SELECT_LIST_END PLS_INTEGER; V_FROM_CLAUSE_START PLS_INTEGER; V_CURRENT_OFFSET PLS_INTEGER; V_COLUMN_OFFSET PLS_INTEGER; V_FROM_WHERE_CLAUSE VARCHAR2(32767); V_COLUMN_LIST T_VC4000_TABLE := T_VC4000_TABLE(); V_COLUMN_NAME VARCHAR2(132); V_COLUMN_NAME_START PLS_INTEGER; V_COLUMN_NAME_END PLS_INTEGER; V_CURSOR SYS_REFCURSOR; V_CURSOR_ID NUMBER := DBMS_SQL.OPEN_CURSOR; V_COLUMN_DESCRIPTIONS DBMS_SQL.DESC_TAB2; V_COLUMN_COUNT NUMBER; V_COLUMN_VALUE VARCHAR2(32767); V_FIRST_ROW BOOLEAN := TRUE; V_FIRST_COLUMN BOOLEAN := TRUE; V_INDEX PLS_INTEGER; begin V_SQL_STATEMENT := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_SELECT_LIST_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START) + LENGTH(C_SELECT_LIST_START); V_SELECT_LIST_END := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1; V_CURRENT_OFFSET := V_SELECT_LIST_START; loop V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39;,&#39;,V_CURRENT_OFFSET); exit when ((V_COLUMN_OFFSET &lt; 1) or (V_COLUMN_OFFSET &gt; V_SELECT_LIST_END)); V_SELECT_LIST.extend; V_COLUMN_LIST.extend; V_INDEX := V_SELECT_LIST.count; V_SELECT_LIST_ITEM := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET - V_CURRENT_OFFSET,V_CURRENT_OFFSET); V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;); V_COLUMN_NAME_END := instr(V_SELECT_LIST_ITEM,&#39;&quot;&#39;,V_COLUMN_NAME_START+1)+1; V_COLUMN_NAME := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END -V_COLUMN_NAME_START); V_SELECT_LIST_ITEM := &#39;JSON_ARRAY(&#39; || V_SELECT_LIST_ITEM || &#39; NULL ON NULL RETURNING VARCHAR2(&#39; || C_MAX_OUTPUT_SIZE || &#39;)) &#39; || V_COLUMN_NAME; V_SELECT_LIST(V_INDEX) := V_SELECT_LIST_ITEM; v_COLUMN_LIST(V_INDEX) := V_COLUMN_NAME; V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1; end loop; V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,&#39; from &#39;,V_SELECT_LIST_END); V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START); DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12); DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST)); DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE)); EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT; V_FIRST_ROW := TRUE; open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT; V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR); DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS); V_COLUMN_NAME_START := 2; for i in 1..V_COLUMN_COUNT loop DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767); end loop; while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &gt; 0) loop if (not V_FIRST_ROW) then DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;,&#39;); end if; V_FIRST_ROW := FALSE; V_FIRST_COLUMN := TRUE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;[&#39;); for i in 1..V_COLUMN_COUNT loop DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE); V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2); if (not V_FIRST_COLUMN) then V_COLUMN_VALUE := &#39;,&#39; || V_COLUMN_VALUE; end if; V_FIRST_COLUMN := FALSE; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE); end loop; DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,&#39;]&#39;); end loop; DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID); end; With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas. The next post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data.","@type":"BlogPosting","url":"http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html","headline":"Object Tables and Wide Tables","dateModified":"2018-06-19T18:00:00-07:00","datePublished":"2018-06-19T18:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="JSON Exchange" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">JSON Exchange</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Object Tables and Wide Tables</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-06-19T18:00:00-07:00" itemprop="datePublished">Jun 19, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h2 id="supporting-object-tables-and-wide-tables">Supporting Object Tables and Wide Tables</h2>

<p>The <a href="/json/export/import/oracle/2018/06/18/Objects-and-ANYDATA.html">previous</a> post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how  support for object serialization can be applied to object tables.</p>

<p>In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$.  XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID.</p>

<p>The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate “TABLE_TYPE = null” from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from  SYS_NC_OID$.</p>

<p>Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code> from ALL_ALL_TABLES aat
          inner join ALL_TAB_COLS atc
		         on atc.OWNER = aat.OWNER
		        and atc.TABLE_NAME = aat.TABLE_NAME
	 left outer join ALL_TYPES at
                  on at.TYPE_NAME = atc.DATA_TYPE
                 and at.OWNER = atc.DATA_TYPE_OWNER
   where aat.STATUS = 'VALID'
     and aat.DROPPED = 'NO'
	 and aat.TEMPORARY = 'N'
     and aat.EXTERNAL = 'NO'
	 and aat.NESTED = 'NO'
	 and aat.SECONDARY = 'N'
	 and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT')
	 and (
	       ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = 'NO'))
		   or 
		   (
		     (TABLE_TYPE is not NULL) 
		     and 
		     (COLUMN_NAME in ('SYS_NC_ROWINFO$','SYS_NC_OID$','ACLOID','OWNERID'))
		   )
         )		           							
	 and aat.OWNER = P_SCHEMA
</code></pre></div></div>

<p>With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file.</p>

<p>Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in</p>

<pre><code class="language-SQL">ORA-40478: output value too large (maximum: 32767)
</code></pre>

<p>The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY.</p>

<p>The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle’s JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON.  With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2  or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [””] characters that surround the column value.</p>

<p>Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual  JSON array.</p>

<p>The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a  column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases.</p>

<p>The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the ‘[’ and ‘]’ characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below</p>

<pre><code class="language-SQL">procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB)
as
   C_SELECT_LIST_START    CONSTANT VARCHAR2(32) := 'select JSON_ARRAY(';
   C_SELECT_LIST_END      CONSTANT VARCHAR2(32) := ' NULL on NULL';
   V_SQL_STATEMENT        CLOB;
   V_SELECT_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_SELECT_LIST_ITEM     VARCHAR2(4000);
   V_SELECT_LIST_START    PLS_INTEGER;
   V_SELECT_LIST_END      PLS_INTEGER;
   V_FROM_CLAUSE_START    PLS_INTEGER;
   V_CURRENT_OFFSET       PLS_INTEGER;
   V_COLUMN_OFFSET        PLS_INTEGER;
   
   V_FROM_WHERE_CLAUSE    VARCHAR2(32767);
   
   V_COLUMN_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_COLUMN_NAME          VARCHAR2(132);
   V_COLUMN_NAME_START    PLS_INTEGER;
   V_COLUMN_NAME_END      PLS_INTEGER;

   V_CURSOR               SYS_REFCURSOR;
   V_CURSOR_ID            NUMBER := DBMS_SQL.OPEN_CURSOR;
   V_COLUMN_DESCRIPTIONS  DBMS_SQL.DESC_TAB2;
   V_COLUMN_COUNT         NUMBER;
   V_COLUMN_VALUE         VARCHAR2(32767);
   V_FIRST_ROW            BOOLEAN := TRUE;
   V_FIRST_COLUMN         BOOLEAN := TRUE;

   V_INDEX                PLS_INTEGER;
begin
   V_SQL_STATEMENT      := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_SELECT_LIST_START  := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START)
                         + LENGTH(C_SELECT_LIST_START);
   V_SELECT_LIST_END    := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1;
   V_CURRENT_OFFSET := V_SELECT_LIST_START;
   loop
     V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,',',V_CURRENT_OFFSET); 
     exit when ((V_COLUMN_OFFSET &lt; 1) or (V_COLUMN_OFFSET &gt; V_SELECT_LIST_END));
	 V_SELECT_LIST.extend;
	 V_COLUMN_LIST.extend;
	 V_INDEX := V_SELECT_LIST.count;
	 V_SELECT_LIST_ITEM  := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET 
                          - V_CURRENT_OFFSET,V_CURRENT_OFFSET);

	 V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,'"');
     V_COLUMN_NAME_END   := instr(V_SELECT_LIST_ITEM,'"',V_COLUMN_NAME_START+1)+1;
     V_COLUMN_NAME       := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END
                           -V_COLUMN_NAME_START);

	 V_SELECT_LIST_ITEM  := 'JSON_ARRAY(' 
	                     || V_SELECT_LIST_ITEM 
	                     || ' NULL ON NULL RETURNING VARCHAR2(' 
	                     || C_MAX_OUTPUT_SIZE || ')) ' 
	                     || V_COLUMN_NAME;
						
	 V_SELECT_LIST(V_INDEX) :=  V_SELECT_LIST_ITEM;
	 v_COLUMN_LIST(V_INDEX) :=  V_COLUMN_NAME;
	 V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1;
   end loop;

   V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,' from ',V_SELECT_LIST_END);
   V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START);
   DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12);
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST));
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE));
   EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT;
   V_FIRST_ROW := TRUE;
   open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR);
   DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS);
   V_COLUMN_NAME_START := 2;
   for i in 1..V_COLUMN_COUNT loop
     DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767);
   end loop;
   while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) &gt; 0) loop
   	 if (not V_FIRST_ROW) then
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,',');
	 end if;
 	 V_FIRST_ROW := FALSE;
	 V_FIRST_COLUMN := TRUE;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,'[');
     for i in 1..V_COLUMN_COUNT loop
	   DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE);
	   V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2);
       if (not V_FIRST_COLUMN) then
         V_COLUMN_VALUE := ',' || V_COLUMN_VALUE;
  	   end if;
       V_FIRST_COLUMN := FALSE;
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE);
     end loop;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,']');
   end loop;
   DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID);
end;
</code></pre>

<p>With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas.</p>

<p>The <a href="/json/export/import/oracle/2018/06/20/JSON_IMPORT.html">next</a> post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data.</p>


  </div><a class="u-url" href="/json/export/import/oracle/2018/06/19/Object-and-Wide-Tables.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">JSON Exchange</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">JSON Exchange</li><li><a class="u-email" href="mailto:mdd@appdev4db.com">mdd@appdev4db.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/markddrake"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">markddrake</span></a></li><li><a href="https://www.twitter.com/markddrake"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">markddrake</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A simple utility for exporting and importing data using JSON</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
