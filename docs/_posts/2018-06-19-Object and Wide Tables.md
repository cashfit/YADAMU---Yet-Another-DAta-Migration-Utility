---
layout: post
title:  "Part VI: Further complications"
date:   2018-06-19 18:00:00-0700
categories: JSON Export Import Oracle
---

## Object Tables and Wide Tables

The [previous]({% link _posts/2018-06-18-Objects and ANYDATA.md%}) post explained how to export columns based on Oracle object types by including a serialized representation of the object in the JSON. This post will explain how  support for object serialization can be applied to object tables. 

In Oracle, object tables are based on an Object type or on the XMLType data type. Object tables have two columns, a unique identifier, SYS_NC_OID$ and a content column, SYS_NC_ROWINFO$.  XMLType tables that manage content associated with the XMLDB repository have 2 additional columns, ACLOID and OWNERID.

The OE schema contains two objects tables, an XMLType table, PURCHASEORDER and an object table, CATEGORIES_TAB. After removing the predicate "TABLE_TYPE = null" from the SQL in GENERATE_STATEMENT entries for these tables are added to the JSON file generated by EXPORT_SCHEMA. Closer examination of the entries for these tables shows the content of these entries is incorrect. The arrays generated from the rows in PURCHASEORDER table only includes content from the SYS_NC_ROWINFO$ column, content from the SYS_NC_OID$ column is omitted. The arrays generated from the rows in CATEGORIES_TAB contains 4 columns, one for each of the scalar attributes defined by the CATEGORY_TYP object. They do not contain the serialized representation of the object, or the content from  SYS_NC_OID$.

Further modifications to the SQL in GENERATE_STATEMENT are necessary to obtain the desired output. Columns SYS_NC_OID$ as well ACLOID and OWNER_ID are hidden columns and so are not included in view ALL_TAB_COLUMNS. Hidden columns are included in view ALL_TAB_COLS so the first step is to switch the view used by GENERATE_STATEMENT. Since ALL_TAB_COLS, show all hidden columns, filters are added so only the desired columns are selected when processing an object table. The reviews from and where clauses are shown below

```
 from ALL_ALL_TABLES aat
          inner join ALL_TAB_COLS atc
		         on atc.OWNER = aat.OWNER
		        and atc.TABLE_NAME = aat.TABLE_NAME
	 left outer join ALL_TYPES at
                  on at.TYPE_NAME = atc.DATA_TYPE
                 and at.OWNER = atc.DATA_TYPE_OWNER
   where aat.STATUS = 'VALID'
     and aat.DROPPED = 'NO'
	 and aat.TEMPORARY = 'N'
     and aat.EXTERNAL = 'NO'
	 and aat.NESTED = 'NO'
	 and aat.SECONDARY = 'N'
	 and (aat.IOT_TYPE is NULL or aat.IOT_TYPE = 'IOT')
	 and (
	       ((TABLE_TYPE is NULL) and (HIDDEN_COLUMN = 'NO'))
		   or 
		   (
		     (TABLE_TYPE is not NULL) 
		     and 
		     (COLUMN_NAME in ('SYS_NC_ROWINFO$','SYS_NC_OID$','ACLOID','OWNERID'))
		   )
         )		           							
	 and aat.OWNER = P_SCHEMA
```

With these modifications in place, invoking EXPORT_SCHEMA on the OE schema generates a valid JSON export file. 

Having succeeded with the OE schema the next target is the PM schema. Unfortunately in an unpatched 12.2 database, even with the PL/SQL workarounds for bypassing JSON_ARRAYAGG operations, invoking EXPORT_SCHEMA results in

```SQL
ORA-40478: output value too large (maximum: 32767)
```

The tables in the PM schema contain a large number of CLOB and BLOB columns. This presents a significant challenge when rendering rows as JSON in environments where CLOB is not a supported return type for JSON_ARRAY. 

The ORA-40478 exception is raised when the JSON representation of a row exceeds the maximum size for a VARCHAR2 data type. In many situations the problem can be avoided by generating a JSON_ARRAY for each column, rather than a JSON array for the entire row. PL/SQL code can then be used to construct a single JSON array from the query results. This approach uses Oracle's JSON operators to obtain the JSON representation of each column, guaranteeing that generated document is valid JSON.  With this workaround the ORA-40478 exception will only occur when when a CLOB, NCLOB VARCHAR2  or the encoded representation of a BLOB or RAW contains more than 32763 characters (4 characters are required for [""] characters that surround the column value. 

Wide column processing is triggered by catching the ORA-40478 exception. Once the exception has been caught, the CLOB containing the JSON export is truncated to the point at which the first array from the current table was appended to the output. A new procedure, PROCESS_WIDE_TABLE is invoked. The procedure regenerates the SQL statement for the table, so that it each column is returned as an individual  JSON array. 

The procedure picks the current select list apart, and reconstructs it as set of JSON_ARRAY operations. It adds a  column alias to each JSON_ARRAY operator to avoid issues that arise when DBMS_SQL processes the results of SQL containing JSON_ARRAY operators without column aliases. 

The new SQL is executed and processed using the DBMS_SQL package. A JSON array is output for each row returned by the query. The array contains one item for each JSON_ARRAY operation returned by the query. The value of each item is obtained by stripping the '[' and ']' characters from the value returned by the JSON_ARRAY operations. The code to manage the wide column problem is shown below

```SQL
procedure PROCESS_WIDE_TABLE(P_METADATA_INDEX NUMBER, P_JSON_DOCUMENT IN OUT CLOB)
as
   C_SELECT_LIST_START    CONSTANT VARCHAR2(32) := 'select JSON_ARRAY(';
   C_SELECT_LIST_END      CONSTANT VARCHAR2(32) := ' NULL on NULL';
   V_SQL_STATEMENT        CLOB;
   V_SELECT_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_SELECT_LIST_ITEM     VARCHAR2(4000);
   V_SELECT_LIST_START    PLS_INTEGER;
   V_SELECT_LIST_END      PLS_INTEGER;
   V_FROM_CLAUSE_START    PLS_INTEGER;
   V_CURRENT_OFFSET       PLS_INTEGER;
   V_COLUMN_OFFSET        PLS_INTEGER;
   
   V_FROM_WHERE_CLAUSE    VARCHAR2(32767);
   
   V_COLUMN_LIST          T_VC4000_TABLE := T_VC4000_TABLE();
   V_COLUMN_NAME          VARCHAR2(132);
   V_COLUMN_NAME_START    PLS_INTEGER;
   V_COLUMN_NAME_END      PLS_INTEGER;

   V_CURSOR               SYS_REFCURSOR;
   V_CURSOR_ID            NUMBER := DBMS_SQL.OPEN_CURSOR;
   V_COLUMN_DESCRIPTIONS  DBMS_SQL.DESC_TAB2;
   V_COLUMN_COUNT         NUMBER;
   V_COLUMN_VALUE         VARCHAR2(32767);
   V_FIRST_ROW            BOOLEAN := TRUE;
   V_FIRST_COLUMN         BOOLEAN := TRUE;

   V_INDEX                PLS_INTEGER;
begin
   V_SQL_STATEMENT      := EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_SELECT_LIST_START  := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_START)
                         + LENGTH(C_SELECT_LIST_START);
   V_SELECT_LIST_END    := DBMS_LOB.INSTR(V_SQL_STATEMENT,C_SELECT_LIST_END) - 1;
   V_CURRENT_OFFSET := V_SELECT_LIST_START;
   loop
     V_COLUMN_OFFSET := DBMS_LOB.INSTR(V_SQL_STATEMENT,',',V_CURRENT_OFFSET); 
     exit when ((V_COLUMN_OFFSET < 1) or (V_COLUMN_OFFSET > V_SELECT_LIST_END));
	 V_SELECT_LIST.extend;
	 V_COLUMN_LIST.extend;
	 V_INDEX := V_SELECT_LIST.count;
	 V_SELECT_LIST_ITEM  := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,V_COLUMN_OFFSET 
                          - V_CURRENT_OFFSET,V_CURRENT_OFFSET);

	 V_COLUMN_NAME_START := instr(V_SELECT_LIST_ITEM,'"');
     V_COLUMN_NAME_END   := instr(V_SELECT_LIST_ITEM,'"',V_COLUMN_NAME_START+1)+1;
     V_COLUMN_NAME       := substr(V_SELECT_LIST_ITEM,V_COLUMN_NAME_START,V_COLUMN_NAME_END
                           -V_COLUMN_NAME_START);

	 V_SELECT_LIST_ITEM  := 'JSON_ARRAY(' 
	                     || V_SELECT_LIST_ITEM 
	                     || ' NULL ON NULL RETURNING VARCHAR2(' 
	                     || C_MAX_OUTPUT_SIZE || ')) ' 
	                     || V_COLUMN_NAME;
						
	 V_SELECT_LIST(V_INDEX) :=  V_SELECT_LIST_ITEM;
	 v_COLUMN_LIST(V_INDEX) :=  V_COLUMN_NAME;
	 V_CURRENT_OFFSET := V_COLUMN_OFFSET + 1;
   end loop;

   V_FROM_CLAUSE_START := DBMS_LOB.INSTR(V_SQL_STATEMENT,' from ',V_SELECT_LIST_END);
   V_FROM_WHERE_CLAUSE := DBMS_LOB.SUBSTR(V_SQL_STATEMENT,32767,V_FROM_CLAUSE_START);
   DBMS_LOB.TRIM(V_SQL_STATEMENT,V_SELECT_LIST_START - 12);
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TABLE_TO_LIST(V_SELECT_LIST));
   DBMS_LOB.APPEND(V_SQL_STATEMENT,TO_CLOB(V_FROM_WHERE_CLAUSE));
   EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT := V_SQL_STATEMENT;
   V_FIRST_ROW := TRUE;
   open V_CURSOR for EXPORT_METADATA_CACHE(P_METADATA_INDEX).SQL_STATEMENT;
   V_CURSOR_ID := DBMS_SQL.TO_CURSOR_NUMBER(V_CURSOR);
   DBMS_SQL.DESCRIBE_COLUMNS2(V_CURSOR_ID, V_COLUMN_COUNT, V_COLUMN_DESCRIPTIONS);
   V_COLUMN_NAME_START := 2;
   for i in 1..V_COLUMN_COUNT loop
     DBMS_SQL.DEFINE_COLUMN(V_CURSOR_ID,i,V_COLUMN_LIST(i),32767);
   end loop;
   while (DBMS_SQL.FETCH_ROWS(V_CURSOR_ID) > 0) loop
   	 if (not V_FIRST_ROW) then
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,',');
	 end if;
 	 V_FIRST_ROW := FALSE;
	 V_FIRST_COLUMN := TRUE;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,'[');
     for i in 1..V_COLUMN_COUNT loop
	   DBMS_SQL.COLUMN_VALUE(V_CURSOR_ID,i,V_COLUMN_VALUE);
	   V_COLUMN_VALUE := substr(V_COLUMN_VALUE,2,length(V_COLUMN_VALUE)-2);
       if (not V_FIRST_COLUMN) then
         V_COLUMN_VALUE := ',' || V_COLUMN_VALUE;
  	   end if;
       V_FIRST_COLUMN := FALSE;
       DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,length(V_COLUMN_VALUE),V_COLUMN_VALUE);
     end loop;
     DBMS_LOB.WRITEAPPEND(P_JSON_DOCUMENT,1,']');
   end loop;
   DBMS_SQL.CLOSE_CURSOR(V_CURSOR_ID);
end;
```

With these modifications the EXPORT_SCHEMA function can successfully process the PM and IX schemas.

The [next]({% link _posts/2018-06-20-JSON_IMPORT.md%}) post will examine how the JSON_TABLE operator can be used to build an IMPORT_SCHEMA function that can convert the content of these export files back into relational and object-relational data.



