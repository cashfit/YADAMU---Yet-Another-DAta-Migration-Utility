---
layout: post
title:  "Unsupported Data Types (Part 1)"
date:   2018-06-16 18:00:00-0700
categories: JSON Export Import Oracle
---

## The Devil's in the Details: Unsupported scalar data types

In the [previous]({% link _posts/2018-06-15-JSON_EXPORT.md%}) post, the first attempt to run EXPORT_SCHEMA on the SH schema failed due to fact one of the tables contained a BLOB column. Fortunately, in that case of the SH schema, the BLOB column was in a table that needed to be excluded from the export operation, and all remaining tables in the SH schema use data types that are directly supported by the JSON_ARRAY operator However that incident was simply a precursor for the problems that are encountered when attempting to process the OE schema. 

```SQL
SQL> select JSON_EXPORT.EXPORT_SCHEMA('OE') from dual;
```

```SQL
ERROR:
ORA-40654: Input to JSON generation function has unsupported data type.
ORA-06512: at "SYSTEM.JSON_EXPORT", line 197
```

In order to determine the set of data types supported by JSON_ARRAY in each database release  a simple script was developed. The script creates a table containing one column for each possible scalar data type, insert 1 row of data into the tables and then attempts to perform a JSON_ARRAY operation on each column. The script, DATA_TYPE_TEST.sql can be found on GitHub in the project's test directory. The results of running this script on 12.2 and 18.1 are documented in the following table.

| Data Type                      | 12cR2 | 18S  | SQL Workaround |
| ------------------------------ | ----- | ---- | ---- |
| CHAR (BYTE semantics)``        | ✔️ | ✔️ |      |
| CHAR (CHAR semantics          | :heavy_check_mark: | ✔️ |      |
| VARCHAR2 (Byte Semantics)      | ✔️ | ✔️ |      |
| VARCHAR2 (Char Semantics)      | ✔️ | ✔️ |      |
| NCHAR                          | ❗️ | ✔️ |      |
| NVARCHAR                       |  ❗️  | ✔️ |      |
| NUMBER                         |  ✔️    | ✔️ |      |
| NUMBER (Precision)             |  ✔️     | ✔️ |      |
| NUMBER (Precision,Scale)       |  ✔️     | ✔️ |      |
| BINARY FLOAT                   | ❌ | ✔️ | TO_CHAR |
| BINARY DECIMAL                 | ❌ | ✔️ | TO_CHAR |
| DATE                           | ✔️ | ✔️ |      |
| TIMESTAMP                      | ✔️ | ✔️ |      |
| TIMESTAMP WITH TIME ZONE       | ✔️ | ✔️ |      |
| TIMESTAMP WITH LOCAL TIME ZONE | ❌ | ✔️ | SYS_EXTRACT_UTC |
| INTERVAL YEAR TO MONTH | ❌ | ✔️ | Custom SQL |
| INTERVAL DAY TO SECOND | ❌ | ✔️ | Custom SQL |
| RAW                            | ✔️ | ✔️ |      |
| ROWID                          | ❌ | ❌ | ROWIDTOCHAR |
| UROWID                         | ❌ | ❌ | ROWIDTOCHAR |
| BLOB                           | ❗️ | ✔️ |  |
| CLOB                           | ✔️ | ✔️ |      |
| NCLOB                          | ❗️ | ✔️ |      |
| BFILE                          | ❌ | ❌ |  |
| XMLTYPE                        | ❌ | ❌ | XMLSEQUENCE |
| LONG                           | ❌ | ❌ |  |
| LONG RAW                       | ❌ | ❌ |  |

The script also demonstrates potential solutions for some of the data types that are not supported natively.

❗️ The JSON_ARRAY documentation for Oracle 12.2 clearly states that BLOB is not a supported data type. However, as is shown by the DATA_TYPE_TESTS.sql script, if a BLOB is passed to the JSON_ARRAY operator it does not raise the expected "ORA-40654: Input to JSON generation function has unsupported data type" exception. Rather it mistakenly assumes that the BLOB column  contains textual data and attempts to process the content as text. If the BLOB contains binary data this will typically result in the exception "ORA-40474: invalid UTF-8 byte sequence in JSON data" being raised by JSON_ARRAY. In Oracle 18c the BLOB data type is supported by JSON_ARRAY. When a  BLOB is passed to JSON_ARRAY a HEXBINARY encoded representation of the BLOB's content is returned by the JSON_ARRAY operator.

❗️There is a similar issue with the way in which JSON_ARRAY handles of NCHAR, NVARCHAR2 and NCLOB in database 12.2 which can also result in exception  "ORA-40474: invalid UTF-8 byte sequence in JSON data"  being raised. A possible workaround for this, at least in databases configured to use 'AL32UTF8' as the database character set, is to apply the TO_CHAR or TO_CLOB operator to the column before passing it to the JSON_ARRAY operator. Note that it is likely that this workaround will fail in non AL32UTF8 environments.

Running the DATA_TYPE_TESTS.sql script in Oracle 18 conforms native support for Interval data types, and the value of an Interval column is output in a format compliant with the ISO 8601 standard. In order to support Interval data types in Oracle12.2 it is necessary to use the SQL extract function to decompose the interval value into discrete components and then construct a string that mimics the native support provided by Oracle 18.

Implementing the workarounds requires modifying the column list generated by GENERATE_STATEMENT to incorporate the appropriate workarounds. A case statement, based on the DATA_TYPE column,  is used to apply each workaround when generating the list of columns names. Conditional compilation is used to determine which workarounds are required in which database version. The result of implementing this logic is the query is now generates the "select list" required to export the table's content as JSON, rather than a list of the columns in the table.

The case statement required to generate the select list is shown below

```sql
case
   -- For some reason RAW columns have DATA_TYPE_OWNER set to the current schema and 
   -- the condition DATA_TYPE_OWNER is not NULL is requried to identify OBJECT types
   when DATA_TYPE = 'RAW'
     then '"' || COLUMN_NAME || '"'
  $IF NOT JSON_FEATURE_DETECTION.CLOB_SUPPORTED $THEN
  /*
  ** Pre 18.1 Some Scalar Data Types are not natively supported by JSON_ARRAY()
  */
  when DATA_TYPE in ('BINARY_DOUBLE','BINARY_FLOAT')
    then 'TO_CHAR("' || COLUMN_NAME || '")'
  when DATA_TYPE LIKE 'TIMESTAMP%WITH LOCAL TIME ZONE'
    then 'TO_CHAR(SYS_EXTRACT_UTC("' || COLUMN_NAME || '"),''IYYY-MM-DD"T"HH24:MI:SS.FF9"Z"'')'
  when DATA_TYPE LIKE 'INTERVAL DAY% TO SECOND%'
    then '''P'' || extract(DAY FROM "' || COLUMN_NAME || '") || ''D'' ||
          ''T'' || case 
                     when extract(HOUR FROM  "' || COLUMN_NAME || '") <> 0 
                       then extract(HOUR FROM  "' || COLUMN_NAME || '") || ''H'' 
                   end 
                || case 
                     when extract(MINUTE FROM  "' || COLUMN_NAME || '") <> 0 
                     then extract(MINUTE FROM  "' || COLUMN_NAME || '") || ''M'' 
                   end
	            || case 
                     when extract(SECOND FROM  "' || COLUMN_NAME || '") <> 0 
                     then extract(SECOND FROM  "' || COLUMN_NAME || '") ||  ''S''
                   end'
  when DATA_TYPE LIKE 'INTERVAL YEAR% TO MONTH%'
    then '''P'' || extract(YEAR FROM "' || COLUMN_NAME || '") || 
          ''Y'' || case 
                     when extract(MONTH FROM  "' || COLUMN_NAME || '") <> 0 
                       then extract(MONTH FROM  "' || COLUMN_NAME || '") || ''M''
                   end'
  when DATA_TYPE in ('NCHAR','NVARCHAR2')
    then 'TO_CHAR("' || COLUMN_NAME || '")'
  when DATA_TYPE = 'NCLOB'
    then 'TO_CLOB("' || COLUMN_NAME || '")'
  $END
  /*
  ** Quick Fixes for datatypes not natively supported
  */
  when DATA_TYPE = 'XMLTYPE'  -- Can be owned by SYS or PUBLIC
    then 'case 
            when "' ||  COLUMN_NAME || '" is NULL 
              then NULL 
              else XMLSERIALIZE(CONTENT "' ||  COLUMN_NAME || '" as CLOB) 
            end'
  when DATA_TYPE = 'ROWID' or DATA_TYPE = 'UROWID'
     then 'ROWIDTOCHAR("' || COLUMN_NAME || '")'
  /*
  ** Comment outunsupported scalar data types and Object types
  */
  when DATA_TYPE in ('LONG','LONG RAW','BFILE','BLOB')
    then '''"' || COLUMN_NAME || '". Unsupported data type ["' || DATA_TYPE || '"]'''
  when DATA_TYPE_OWNER is not NULL
    then '''"' || COLUMN_NAME  || '". Unsupported object type ["' 
               || DATA_TYPE_OWNER || '"."' || DATA_TYPE || '"]'''
  else
    '"' || COLUMN_NAME || '"'

```

The above implementation assumes that applying the patch to enable CLOB support to Database 12.2 will also extend the list of data types supported natively by JSON_ARRAY in 12.2. If this is not the case further 'DUCK TYPING' may be necessary to determine which data types are supported natively in which database release in order to generate a correctly optimized column list. 

After implementing these changes  an EXPORT_SCHEMA operation on the OE schema succeeds. However a closer examination of the JSON document generated shows that the export failed to export all of the data in the schema The following table shows the results of the export operation

| Table Name          | Results | Cause                          |
| ------------------- | ------- | ------------------------------ |
| INVENTORIES         | ✔️       |                                |
| ORDERS              | ✔️       |                                |
| ORDER_ITEMS         | ✔️       |                                |
| PROMOTIONS          | ✔️       |                                |
| PROCUCT_INFORMATION | ✔️       |                                |
| CUSTOMERS           | ❌       | Unsupported OBJECT Type Column |
| WAREHOUSES          | ❌       | Unsupported OBJECT Type Column |
| CATEOGORIES         | ❌       | Skipped OBJECT Table           |
| PURCHASEORDER       | ❌       | Skipped OBJECT (XMLTYPE) Table |

At this point EXPORT_SCHEMA is capable of handling most of the common scalar data types supported by the Oracle Database. The [next]({% link _posts/2018-06-17-BFILE and BLOB.md%}) post will explain how in-line PL/SQL procedures can be used to enable support for BIFLE and BLOB data types.